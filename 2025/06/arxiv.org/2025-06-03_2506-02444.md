<!-- metadata -->

- **title**: SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios
- **source**: https://arxiv.org/abs/2506.02444
- **author**: Dang, Lingwei, Shao, Ruizhi, Zhang, Hongwen, Min, Wei, Liu, Yebin, Wu, Qingyao
- **published**: 2025/06/03
- **fetched**: 2025-06-09T21:50:15.475378+00:00
- **tags**: codex, ai
- **image**: /static/browse/0.3.4/images/arxiv-logo-fb.png

## 要約

既存のHOI動作生成は3Dモデルと限定的なモーションデータに依存して汎化性が低く、映像生成では画質を優先するため物理的整合性を損ないがちであった。本研究は視覚的事前知識と動的制約を統合した**SViMo**フレームワークを提案し、同期拡散プロセスによって手物体相互作用の映像と動作を同時生成する。三モーダル適応モジュレーションと3Dフルアテンションで特徴を整合し、視覚指向の3Dインタラクション拡散モデルから得た動作列をフィードバックすることで映像と動作の一致を向上。事前定義のオブジェクトやポーズに頼らず、高精度で汎用的なHOIシーケンスを生成し、未見環境でも優れた性能を示した。

## 本文

[Submitted on 3 Jun 2025 (

[v1](https://arxiv.org/abs/2506.02444v1)

), last revised 5 Jun 2025 (this version, v3)]

# Title:SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios

View a PDF of the paper titled SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios, by Lingwei Dang and 5 other authors

[View PDF](/pdf/2506.02444)
[HTML (experimental)](https://arxiv.org/html/2506.02444v3)

> Abstract:Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at [this https URL](https://github.com/Droliven/SViMo_project).

## Submission history

From: Lingwei Dang [

[view email](/show-email/5a71a8c6/2506.02444)

]

**[[v1]](/abs/2506.02444v1)**

Tue, 3 Jun 2025 05:04:29 UTC (30,196 KB)

**[[v2]](/abs/2506.02444v2)**

Wed, 4 Jun 2025 10:53:28 UTC (30,186 KB)

**[v3]**

Thu, 5 Jun 2025 03:21:54 UTC (30,188 KB)
