<!-- metadata -->
- **title**: Comma v0.1 1T and 2T—7B LLMs trained on openly licensed text
- **source**: https://simonwillison.net/2025/Jun/7/comma/
- **author**: Simon Willison
- **published**: 2025-06-07T23:59:00
- **fetched**: 2025-06-08T16:34:39Z
- **tags**: codex, llms, open-data, eleutherai, machine-learning, apple-mlx
- **image**: 

## 要約
EleutherAI主導の **Common Pile v0.1** は 8TB に及ぶオープンライセンスのテキストコーパスです。これを使って学習した 7B パラメータの **Comma v0.1** モデルが 1T/2T 版として公開されました。著者は macOS 上で動かすために `mlx_lm` を用いてモデルを **MLX** 形式へ変換し、変換手順を示すスクリプトを紹介しています。変換後のモデルは Hugging Face にアップロードされ、`uv run` コマンドで実行可能です。初回起動では約 13GB のファイルがダウンロードされ、プロンプトへ "Facts about pelicans:" と入力すると例示の文章が生成されます。現状は生のベースモデルで、**インストラクションチューニング**が行われていないため、文頭に文章を与える必要があり評価が難しいと指摘。今後はオープンデータのみでの微調整が課題となるものの、チャット対応版の登場に期待しています。ペリカンの自転車画像生成テストでは無限ループのような結果となり、性能向上の余地も感じられました。

## 本文
## Comma v0.1 1T and 2T—7B LLMs trained on openly licensed text

7th June 2025

It’s been a long time coming, but we finally have some promising LLMs to try out which are trained entirely on openly licensed text!

EleutherAI released [the Pile](https://arxiv.org/abs/2101.00027) four and a half years ago: “an 800GB dataset of diverse text for language modeling”. It’s been used as the basis for many LLMs since then, but much of the data in it came from [Common Crawl](https://commoncrawl.org/)—a crawl of the public web which mostly ignored the licenses of the data it was collecting.

[The Common Pile v0.1](https://huggingface.co/blog/stellaathena/common-pile) is EleutherAI’s successor to the original Pile, in collaboration with a large group of other organizations with whom they have been “meticulously curating a 8 TB corpus of openly licensed and public domain text for training large language models”.

The dataset is exciting, but on top of that they’ve released two new LLMs that have been trained on it: Comma v0.1 1T and 2T, both with 7 billion parameters, the first trained on 1 trillion tokens and the second on 2 trillion tokens.

These are available on Hugging Face as [common-pile/comma-v0.1-1t](https://huggingface.co/common-pile/comma-v0.1-1t) and [common-pile/comma-v0.1-2t](https://huggingface.co/common-pile/comma-v0.1-2t).

EleutherAI claim that these new models perform “comparably to leading models trained in the same regime on unlicensed data”. I decided to try them out myself.

The models are currently only available as `.safetensors` files, which I’ve found difficult to run on macOS in the past. I decided to see if I could convert them to [MLX](https://github.com/ml-explore/mlx) format which I know how to run on my Mac.

MLX is still a very new format, but Claude 4 Sonnet has a training cutoff date of March 2025 so I crossed my fingers and hoped it would be able to help me out. [It did exactly that!](https://claude.ai/share/379951f0-4fb2-4b70-b6f9-f8a3afae1e33) I ran the following command to convert the 2T model to run using MLX:

```
uv run --python 3.12 \
  --with mlx-lm \
  python -m mlx_lm convert \
    --hf-path common-pile/comma-v0.1-2t \
    --mlx-path ./comma-v0.1-2t-mlx
```

I uploaded the converted model to Hugging Face as [simonw/comma-v0.1-2t-mlx](https://huggingface.co/simonw/comma-v0.1-2t-mlx).

Now that it’s on the Hub here’s how to try it out (using `uv run`):

```
uv run --python 3.12 \
  --with mlx-lm \
  mlx_lm.generate \
    --model simonw/comma-v0.1-2t-mlx \
    --prompt 'Facts about pelicans:'
```

The first time you run this it will download 13GB of files to `~/.cache/huggingface/hub/models--simonw--comma-v0.1-2t-mlx`.

Here’s what I got back:

> `1. They are the largest of the water birds. 2. They are found in all parts of the world. 3. They are very good swimmers. 4. They are very good divers. 5. They are very good flyers. 6. They are very good hunters. 7. They are very good eaters. 8. They are very good parents. 9. They are very good friends. 10.`

The big limitation of this model right now is that it’s a raw base model—it hasn’t been instruction-tuned or set up for chat.

This means you have to prefix-prompt it, like in the GPT-3 days. You need to give it a sentence for it to complete.

This makes it a lot harder to evaluate than the instruction-tuned models that I’ve become used to over the past few years!

I’m hoping someone releases a chat-tuned version of this model soon. The challenge there will be keeping to the openly licensed training data, since most of the fine-tuning datasets out there for this are themselves derived from models that were trained on unlicensed data.

Sadly it didn’t do too well on my [pelican on a bicycle](https://simonwillison.net/2025/Jun/6/six-months-in-llms/#ai-worlds-fair-2025-02) benchmark:

```
uv run --python 3.12 \
  --with mlx-lm \
  mlx_lm.generate \
    --model simonw/comma-v0.1-2t-mlx \
    --prompt 'An SVG of a pelican riding a bicycle: <svg' --max-tokens 2000
```

The output started like this and looped indefinitely:

> `xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 100 100"><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/><path d="M0 0h100v100H0z" fill="none"/>...`
