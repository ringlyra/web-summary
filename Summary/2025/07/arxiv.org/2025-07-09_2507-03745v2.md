---
title: 'StreamDiT: Real-Time Streaming Text-to-Video Generation'
source: https://arxiv.org/html/2507.03745v2
author:
  - Akio Kodaira
  - Tingbo Hou
  - Ji Hou
  - Masayoshi Tomizuka
  - Yue Zhao
published: '2025-07-08T00:00:00Z'
fetched: '2025-07-09T07:46:43.534086+00:00'
tags:
  - codex
  - arxiv
image: https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png
---

## è¦ç´„

è¿‘å¹´ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’å¤§è¦æ¨¡åŒ–ã™ã‚‹ã“ã¨ã§é«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãå‹•ç”»ç”ŸæˆãŒå¯èƒ½ã«ãªã£ãŸãŒã€å¾“æ¥æ‰‹æ³•ã¯çŸ­ã„ã‚¯ãƒªãƒƒãƒ—ã—ã‹ç”Ÿæˆã§ããšã€å¯¾è©±çš„ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨é€”ã«ã¯ä¸å‘ãã ã£ãŸã€‚æœ¬è«–æ–‡ã§ã¯ã€ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã«å‹•çš„ãƒãƒƒãƒ•ã‚¡ã‚’å°å…¥ã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ãŸå­¦ç¿’æ³•ã§æ™‚é–“çš„ä¸€è²«æ€§ã¨ç”»è³ªã‚’å‘ä¸Šã•ã›ã‚‹StreamDiTã‚’ææ¡ˆã™ã‚‹ã€‚4Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€åˆ†å‰²æ¯ã«è¡Œã†å¤šæ®µè’¸ç•™ã«ã‚ˆã‚Šæ¨è«–å›æ•°ã‚’ãƒãƒƒãƒ•ã‚¡ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã¾ã§å‰Šæ¸›ã—ãŸã€‚1GPUã§512på‹•ç”»ã‚’æ¯ç§’16ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆã§ãã€å®Ÿæ™‚é–“ã§ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç”Ÿæˆã‚’å®Ÿç¾ã™ã‚‹ã€‚å®šé‡æŒ‡æ¨™ã¨äººæ‰‹è©•ä¾¡ã§ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€Webä¸Šã§å‹•ç”»ä¾‹ã‚’å…¬é–‹ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã«ã‚ˆã‚Šé•·å°ºå‹•ç”»ç”Ÿæˆã®æ–°ãŸãªå¯èƒ½æ€§ãŒé–‹ã‹ã‚ŒãŸã€‚

## æœ¬æ–‡

### 3.1 Buffered Flow Matching

**Flow Matching:** Our method is based on the Flow Matching (FM)Â [[24](https://arxiv.org/html/2507.03745v2#bib.bib24)] framework for training.
FM produces a sample from the target data distribution by progressively transforming a sample from an initial prior distribution, such as a Gaussian.
During training, for a data sample denoted as ğ—1subscriptğ—1\mathbf{X}\_{1}bold\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT, we sample a time step tâˆˆ[0,1]ğ‘¡01t\in[0,1]italic\_t âˆˆ [ 0 , 1 ], and noise ğ—0âˆ¼ğ’©â¢(0,1)similar-tosubscriptğ—0ğ’©01\mathbf{X}\_{0}\sim\mathcal{N}(0,1)bold\_X start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT âˆ¼ caligraphic\_N ( 0 , 1 ). These are then used to create a training sample ğ—tsubscriptğ—ğ‘¡\mathbf{X}\_{t}bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT.
FM predicts the velocity ğ•tsubscriptğ•ğ‘¡\mathbf{V}\_{t}bold\_V start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT that moves the sample ğ—tsubscriptğ—ğ‘¡\mathbf{X}\_{t}bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT in the direction of data sample ğ—1subscriptğ—1\mathbf{X}\_{1}bold\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT.

A simple linear interpolation or the optimal transport (OT) pathÂ [[24](https://arxiv.org/html/2507.03745v2#bib.bib24)] is used to construct ğ±tsubscriptğ±ğ‘¡\mathbf{x}\_{t}bold\_x start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT, *i.e*.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ—t=tâ¢ğ—1+(1âˆ’(1âˆ’Ïƒmin)â¢t)â¢ğ—0,subscriptğ—ğ‘¡ğ‘¡subscriptğ—111subscriptğœminğ‘¡subscriptğ—0\mathbf{X}\_{t}=t~{}\mathbf{X}\_{1}+(1-(1-\sigma\_{\mathrm{min}})t)~{}\mathbf{X}\_% {0},bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = italic\_t bold\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT + ( 1 - ( 1 - italic\_Ïƒ start\_POSTSUBSCRIPT roman\_min end\_POSTSUBSCRIPT ) italic\_t ) bold\_X start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , |  | (1) |

where Ïƒminsubscriptğœmin\sigma\_{\mathrm{min}}italic\_Ïƒ start\_POSTSUBSCRIPT roman\_min end\_POSTSUBSCRIPT is the standard deviation of xğ‘¥xitalic\_x at t=1ğ‘¡1t=1italic\_t = 1.
Thus, the ground truth velocity can be derived as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ•t=dâ¢ğ—tdâ¢t=ğ—1âˆ’(1âˆ’Ïƒmin)â¢ğ—0.subscriptğ•ğ‘¡ğ‘‘subscriptğ—ğ‘¡ğ‘‘ğ‘¡subscriptğ—11subscriptğœminsubscriptğ—0\mathbf{V}\_{t}=\dfrac{d\mathbf{X}\_{t}}{dt}=\mathbf{X}\_{1}-(1-\sigma\_{\mathrm{% min}})\mathbf{X}\_{0}.bold\_V start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT = divide start\_ARG italic\_d bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_d italic\_t end\_ARG = bold\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT - ( 1 - italic\_Ïƒ start\_POSTSUBSCRIPT roman\_min end\_POSTSUBSCRIPT ) bold\_X start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT . |  | (2) |

It is worth noting that this target is irrelevant to time step tğ‘¡titalic\_t.
With parameters Î˜Î˜\Thetaroman\_Î˜ and text prompt ğğ\mathbf{P}bold\_P, the predicted velocity is written as uâ¢(ğ—t,ğ,t;Î˜)ğ‘¢subscriptğ—ğ‘¡ğğ‘¡Î˜u(\mathbf{X}\_{t},\mathbf{P},t;\Theta)italic\_u ( bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , bold\_P , italic\_t ; roman\_Î˜ ), and the training objective is represented as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ”¼t,ğ—tâ¢â€–uâ¢(ğ—t,ğ,t;Î˜)âˆ’ğ•tâ€–2.subscriptğ”¼  ğ‘¡subscriptğ—ğ‘¡superscriptnormğ‘¢subscriptğ—ğ‘¡ğğ‘¡Î˜subscriptğ•ğ‘¡2\mathbb{E}\_{t,\mathbf{X}\_{t}}\|u(\mathbf{X}\_{t},\mathbf{P},t;\Theta)-\mathbf{V% }\_{t}\|^{2}.blackboard\_E start\_POSTSUBSCRIPT italic\_t , bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT âˆ¥ italic\_u ( bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , bold\_P , italic\_t ; roman\_Î˜ ) - bold\_V start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âˆ¥ start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT . |  | (3) |

At inference, an FM model predicts the velocity to a clean sample on every denoising step. With the Euler solver, the inference can be formulated as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ—t+Î”â¢t=ğ—t+uâ¢(ğ—t,ğ,t;Î˜)â¢Î”â¢t,subscriptğ—ğ‘¡Î”ğ‘¡subscriptğ—ğ‘¡ğ‘¢subscriptğ—ğ‘¡ğğ‘¡Î˜Î”ğ‘¡\mathbf{X}\_{t+\Delta t}=\mathbf{X}\_{t}+u(\mathbf{X}\_{t},\mathbf{P},t;\Theta)% \Delta t,bold\_X start\_POSTSUBSCRIPT italic\_t + roman\_Î” italic\_t end\_POSTSUBSCRIPT = bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT + italic\_u ( bold\_X start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT , bold\_P , italic\_t ; roman\_Î˜ ) roman\_Î” italic\_t , |  | (4) |

where Î”â¢tÎ”ğ‘¡\Delta troman\_Î” italic\_t is the step size.

**Buffered Flow Matching:** We consider streaming video generation as a sequence of (possibly latent) frames [f1,f2,â€¦,fN]

subscriptğ‘“1subscriptğ‘“2â€¦subscriptğ‘“ğ‘[f\_{1},f\_{2},\dots,f\_{N}][ italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦ , italic\_f start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT ], and Nğ‘Nitalic\_N can be infinite. For a video diffusion model with frame buffer BğµBitalic\_B, the clean data sample starting with frame fisubscriptğ‘“ğ‘–f\_{i}italic\_f start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT is denoted as ğ—1i=[fi,â€¦,fi+B]superscriptsubscriptğ—1ğ‘–

subscriptğ‘“ğ‘–â€¦subscriptğ‘“ğ‘–ğµ\mathbf{X}\_{1}^{i}=[f\_{i},\dots,f\_{i+B}]bold\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT = [ italic\_f start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT , â€¦ , italic\_f start\_POSTSUBSCRIPT italic\_i + italic\_B end\_POSTSUBSCRIPT ]. We allow different noise levels to the frames: Ï„=[Ï„1,â€¦,Ï„B]ğœ

subscriptğœ1â€¦subscriptğœğµ\tau=[\tau\_{1},\dots,\tau\_{B}]italic\_Ï„ = [ italic\_Ï„ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦ , italic\_Ï„ start\_POSTSUBSCRIPT italic\_B end\_POSTSUBSCRIPT ] as a monotonically increasing sequence. Thus a training example can be constructed as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ—Ï„i=Ï„âˆ˜ğ—1i+(1âˆ’(1âˆ’Ïƒmin)â¢Ï„)âˆ˜ğ—0,superscriptsubscriptğ—ğœğ‘–ğœsuperscriptsubscriptğ—1ğ‘–11subscriptğœminğœsubscriptğ—0\mathbf{X}\_{\tau}^{i}=\tau\circ~{}\mathbf{X}\_{1}^{i}+(1-(1-\sigma\_{\mathrm{min% }})\tau)\circ~{}\mathbf{X}\_{0},bold\_X start\_POSTSUBSCRIPT italic\_Ï„ end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT = italic\_Ï„ âˆ˜ bold\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT + ( 1 - ( 1 - italic\_Ïƒ start\_POSTSUBSCRIPT roman\_min end\_POSTSUBSCRIPT ) italic\_Ï„ ) âˆ˜ bold\_X start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , |  | (5) |

where âˆ˜\circâˆ˜ denotes element-wise product. Please note that the noise sample X0subscriptğ‘‹0X\_{0}italic\_X start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT remains the same.

At inference, the buffer is updated by model predicted flow

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ—Ï„+Î”â¢Ï„i=ğ—Ï„i+uâ¢(ğ—Ï„i,ğ,Ï„;Î˜)âˆ˜Î”â¢Ï„,superscriptsubscriptğ—ğœÎ”ğœğ‘–superscriptsubscriptğ—ğœğ‘–ğ‘¢superscriptsubscriptğ—ğœğ‘–ğğœÎ˜Î”ğœ\mathbf{X}\_{\tau+\Delta\tau}^{i}=\mathbf{X}\_{\tau}^{i}+u(\mathbf{X}\_{\tau}^{i}% ,\mathbf{P},\tau;\Theta)\circ\Delta\tau,bold\_X start\_POSTSUBSCRIPT italic\_Ï„ + roman\_Î” italic\_Ï„ end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT = bold\_X start\_POSTSUBSCRIPT italic\_Ï„ end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT + italic\_u ( bold\_X start\_POSTSUBSCRIPT italic\_Ï„ end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT , bold\_P , italic\_Ï„ ; roman\_Î˜ ) âˆ˜ roman\_Î” italic\_Ï„ , |  | (6) |

where Î”â¢Ï„Î”ğœ\Delta\tauroman\_Î” italic\_Ï„ is a sequence of step sizes. If one or more frames achieve the final denoising step, they are popped out of the buffer, and new noise frames are pushed at the beginning of the buffer. Therefore, it can generate streaming video sequences.

### 3.2 Partitioning Scheme

![Refer to caption](https://arxiv.org/html/2507.03745v2/x2.png)


Figure 2: Illustration of StreamDiT partitioning. We partition the buffer to Kğ¾Kitalic\_K reference frames and Nğ‘Nitalic\_N chunks. Each chunk has cğ‘citalic\_c frames and sğ‘ sitalic\_s micro denoising steps.

To facilitate flexible training and inference of StreamDiT, we design a unified partitioning of buffered frames. As illustrated inÂ [Fig.Â 2](https://arxiv.org/html/2507.03745v2#S3.F2 "In 3.2 Partitioning Scheme â€£ 3 StreamDiT Training â€£ StreamDiT: Real-Time Streaming Text-to-Video Generation"), the buffer is partitioned to Kğ¾Kitalic\_K reference frames and Nğ‘Nitalic\_N chunks. Each chunk has cğ‘citalic\_c frames and sğ‘ sitalic\_s micro denoising steps.

**Clean Reference Frames:**
To enhance temporal consistency, we can optionally cache the last Kğ¾Kitalic\_K fully denoised frames at the beginning of the buffer. We refer to these clean frames of as reference frames. They participate in the denoising step as input but are no longer denoised. The reference frames are updated in the same way as other frames when the buffer moves. Allowing optional reference frames matches the design of FIFO-DiffusionÂ [[20](https://arxiv.org/html/2507.03745v2#bib.bib20)], which can be viewed as a special case. Since our method is trainable, we found that reference frames can be skipped for streaming video generation. Hence, we set K=0ğ¾0K=0italic\_K = 0 in the rest of our work.

**Chunked Denoising:**
Instead of denoising frame by frame, we group frames into stream chunks, with each chunk containing a specified number of frames indicated by chunk size. Noise levels are now applied at the chunk level, and each time a chunk of frames exits the pipeline.
Let Nğ‘Nitalic\_N denote the number of stream chunks and cğ‘citalic\_c the number of frames in each chunk. Then the total number of frames processed at any time is K+NÃ—cğ¾ğ‘ğ‘K+N\times citalic\_K + italic\_N Ã— italic\_c, and the number of denoising steps is constrained to Nğ‘Nitalic\_N.

**Micro Step:**
As the total number of denoising steps is bounded by Nğ‘Nitalic\_N, however, for better performance, additional denoising steps are usually necessary. A straightforward approach to address this limitation is to increase the size of the buffer, but this is limited by the maximum capacity of the model to denoise frames and would also increase latency for each frame in the buffer.

To overcome this, we introduce an additional dimension of the design called micro-denoising step, illustrated inÂ [Fig.Â 2](https://arxiv.org/html/2507.03745v2#S3.F2 "In 3.2 Partitioning Scheme â€£ 3 StreamDiT Training â€£ StreamDiT: Real-Time Streaming Text-to-Video Generation"). The core idea of micro step is to denoise stream chunks along the temporal axis while stagnating at a fixed spatial position for certain time. Let sğ‘ sitalic\_s denote the denoising steps per micro step; then each stream chunk undergoes sğ‘ sitalic\_s denoising steps before advancing to the next noise level and moving toward the output. This modification effectively extends the total number of denoising steps to sÃ—Nğ‘ ğ‘s\times Nitalic\_s Ã— italic\_N without increasing the buffer size.

With the incorporation of reference frames, chunked frames, and micro-step denoising, the following equations hold:

|  |  |  |  |
| --- | --- | --- | --- |
|  | B=K+NÃ—cT=sÃ—Nğµğ¾ğ‘ğ‘ğ‘‡ğ‘ ğ‘\begin{split}B&=K+N\times c\\ T&=s\times N\end{split}start\_ROW start\_CELL italic\_B end\_CELL start\_CELL = italic\_K + italic\_N Ã— italic\_c end\_CELL end\_ROW start\_ROW start\_CELL italic\_T end\_CELL start\_CELL = italic\_s Ã— italic\_N end\_CELL end\_ROW |  | (7) |

where BğµBitalic\_B is the total length of the frame fed into the model, Nğ‘Nitalic\_N is the number of stream chunks, cğ‘citalic\_c is the number of frames in each stream chunk, and Tğ‘‡Titalic\_T represents the total number of inference denoising steps.

|  |  |  |  |
| --- | --- | --- | --- |
| Method | Scheme | Consistency | Streaming |
| Uniform | c=B,s=1formulae-sequenceğ‘ğµğ‘ 1c=B,s=1italic\_c = italic\_B , italic\_s = 1 | High | No |
| Diagonal | c=1,s=1formulae-sequenceğ‘1ğ‘ 1c=1,s=1italic\_c = 1 , italic\_s = 1 | Low | Yes |
| StreamDiT | c=[1,â€¦,B],s=TNformulae-sequenceğ‘  1â€¦ğµğ‘ ğ‘‡ğ‘c=[1,\dots,B],s=\frac{T}{N}italic\_c = [ 1 , â€¦ , italic\_B ] , italic\_s = divide start\_ARG italic\_T end\_ARG start\_ARG italic\_N end\_ARG | High | Yes |

Table 1: StreamDiT unifies different partitioning schemes.

**Mixed Training:**
As shown inÂ [Tab.Â 1](https://arxiv.org/html/2507.03745v2#S3.T1 "In 3.2 Partitioning Scheme â€£ 3 StreamDiT Training â€£ StreamDiT: Real-Time Streaming Text-to-Video Generation"), StreamDiT unifies original diffusion or FM with uniform noise and diagonal noise used byÂ [[20](https://arxiv.org/html/2507.03745v2#bib.bib20), [42](https://arxiv.org/html/2507.03745v2#bib.bib42)]. The latter enables streaming generation but hurts consistency of generated content. To enhance consistency and avoid overfitting, we adopt mixed training with different schemes. This drives the model to learn generalized denoising with different noise levels, instead of memorizing fixed noise levels. It is worth noting that our mixed training covers diffusion and FM training. Therefore, our model can work as a standard T2V generation without streaming. This is also used for initializing our streaming generation.

According toÂ [Fig.Â 2](https://arxiv.org/html/2507.03745v2#S3.F2 "In 3.2 Partitioning Scheme â€£ 3 StreamDiT Training â€£ StreamDiT: Real-Time Streaming Text-to-Video Generation"), frames in each chunk correspond to a distinct segment of the overall time step range. Thus, during training, we sample a random time step for the iğ‘–iitalic\_i-th chunk as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï„iâˆ¼Uniformâ¢([TNâ‹…(iâˆ’1),TNâ‹…i])similar-tosubscriptğœğ‘–Uniformâ‹…ğ‘‡ğ‘ğ‘–1â‹…ğ‘‡ğ‘ğ‘–\tau\_{i}\sim\text{Uniform}\left(\left[\frac{T}{N}\cdot(i-1),\frac{T}{N}\cdot i% \right]\right)italic\_Ï„ start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT âˆ¼ Uniform ( [ divide start\_ARG italic\_T end\_ARG start\_ARG italic\_N end\_ARG â‹… ( italic\_i - 1 ) , divide start\_ARG italic\_T end\_ARG start\_ARG italic\_N end\_ARG â‹… italic\_i ] ) |  | (8) |

Interestingly, the StreamDiT training can be viewed as parallel training of the full range of denoising.