---
title: 'CriticTool: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios'
source: https://arxiv.org/html/2506.13977
author:
  - arxiv.org
published: ''
fetched: '2025-06-18T10:51:57.295763+00:00'
tags:
  - codex
  - arxiv
image: 
---

## 要約

LLMが外部ツールを用いる際に発生する各種の誤りを体系的に扱い、自己批評能力を評価するためのベンチマークCriticToolを提案する。既存のツール利用評価はタスク達成結果のみを重視し、誤り検出・回復の過程を十分に観察していない。本研究ではツール選択ミスや幻想ツールの利用、パラメータ指定の誤り、外部環境による失敗などを網羅した多様なデータを進化的手法で生成し、自己反省・訂正・再試行・スキップの指標でLLMを詳細評価する。データ収集には既存の高品質ベンチマークから複数の実APIとタスクを取り込み、エラー多発シナリオを人手で確認。ベンチマークはLLMの自己批評行動を多角的に観察できるよう設計され、生成モデルの性能差が浮き彫りになった。実験の結果、GPT-4oが比較的高性能を示す一方、全体的に誤り回復能力はまだ不十分で、継続的な改善が必要と結論付けられた。

## 本文

### 3.2 Benchmarking Results on CriticTool

The detailed experimental results are shown in Tab. [2](https://arxiv.org/html/2506.13977v1#S2.T2 "Table 2 ‣ 2.3.1 Self-Critique Task Decomposition ‣ 2.3 Fine-Grained Evaluation ‣ 2 CriticTool ‣ CriticTool: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios").
Experiments using the chain-of-thought strategy Wei et al. ([2022](https://arxiv.org/html/2506.13977v1#bib.bib41)) are also conducted, leading to improvements in LLMs’ self-critique performance, with the results provided in the Appendix [D.2](https://arxiv.org/html/2506.13977v1#A4.SS2 "D.2 Full Results on CriticTool-CoT ‣ Appendix D Additional Results ‣ CriticTool: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios").
We analyze the benchmarking results by exploring the following four questions.

**Q1: Which Model is Better at Tool Self-Critique?**GPT-4o leads in self-critique performance for tool-use error scenarios, achieving an impressive overall score of 69.01.
Close behind, large-scale open-source models LLaMA3.1-70B and Qwen2.5-72B, deliver comparable scores, showcasing strong self-critique capabilities.

For internal model-driven errors, the closed-source models GPT-4o and Claude3.5 deliver comparable top performance, though Claude3.5 slightly underperforms in error categorization.
In contrast, open-source models exhibit substantial variability in self-critique performance.
While most open-source models significantly lag behind the closed-source models, highlighting a clear gap in their capabilities, LLaMA3.1 and Qwen2.5 stand out as notable exceptions.
Their performance not only approaches but occasionally surpasses that of closed-source models.
However, tool-use-fineturned models show disappointing results in handling internal errors.
Except for AgentLM-8B, the other models exhibit almost no instruction-following or self-critique capabilities, which can be attributed to the damage to their generalization ability caused by fine-tuning on specific data.

For external environment errors, most models can recognize errors and avoid endless repetition, though Claude3.5 and Ministral-8B shows weaker performance in this regard, and some tool-use-finetuned models entirely lack this ability.
When it comes to handling errors by either proceeding with subsequent tasks or finish tool call action, GPT-4o outperforms other models, with some large-scale open-source models achieving comparably strong performance.

**Q2: What is the self-critique performance of LLMs across various scenarios?**In the internal critique task, models should proceed with subsequent tool-calling tasks within error-injected data.
However, poor performance models tend to exhibit over-reflection, mistakenly classifying a correct step as an errors.
For error-injected cases, models are expected to accurately reflect and correct the mistake it made in the previous step, but many models with limited critique capabilities fail in such task.
In the tool selection error scenario, LLMs may select the wrong tool while still providing valid parameters, leading to silent errors without explicit signals from the environment Sun et al. ([2024](https://arxiv.org/html/2506.13977v1#bib.bib35)), hindering models’ error reflection.
In such cases, the most frequently observed poor self-critique behaviors are correction without reflection or error Ignorance.
In contrast, the other three internal error scenarios often trigger explicit error signals due to invalid tool inputs or parameters, aiding models in reflecting and achieving higher self-critique success rates.
Nonetheless, weaker models may still display failure to detect, failure to correct, or even experience unexpected tool call interruptions.

In the external critique task, the model should retry the failed operation retry within limits, exit the loop appropriately, and either complete the remaining subtasks or ask user for guidance.
However, when models fail to recognize errors, they tend to repeat the same call more than three times, resulting in a significant resource drain.
Some models go further by hallucinating, offering false answers to user questions rather than asking for guidance.

**Q3: How does Data Evolution Effects?**As illustrated in Fig. [3](https://arxiv.org/html/2506.13977v1#S3.F3 "Figure 3 ‣ 3.2 Benchmarking Results on CriticTool ‣ 3 Experiment ‣ CriticTool: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios"), the data evolution leads to a decline in the scores of all LLMs.
GPT-4o retains its SOTA results, while Qwen2.5-7B also demonstrates impressive capabilities.
In contrast, LLaMA3-70B experiences significant performance degradation, falling below the performance of most small scale models.

![Refer to caption](https://arxiv.org/html/2506.13977v1/x4.png)


Figure 3:  Comparison of the performance of five models across various evolution strategies. The red cross indicates the score corresponding to the base dataset.

This is consistent with CriticBench Lin et al. ([2024](https://arxiv.org/html/2506.13977v1#bib.bib23)) experimental observation.
We attribute this to the unstable generalizability of the offline data, a limitation that becomes increasingly pronounced as the number of model parameters grows.
We independently test the four sub-strategies to investigate their impact on models’ self-critic performance.
The negative impact on the model decreases in the following order: Long Context, Noisy Query, Extra Tools and Harder Tools.
Long Context and Extra Tools increase the difficulty of retrieval and challenge the model’s ability to follow instructions and Extra Tools introduce relatively little extra data.
Noisy Query presents a significant challenge to the model’s capacity for comprehension and parameter transfer, reminiscent of the disruptive influence encapsulated by the adage ‘A loose cannon’.
However, as the API documents become more verbose and longer, some models demonstrate improved comprehension of the APIs, leading to slight performance enhancements, such as GLM4-9B-chat.

Overall, for the model, the three key components—the context, query, and tool list—are not merely superimposed.
The interplay between scalable and robust levels results in a compounding effect, causing the model’s performance to degrade more rapidly under the hybrid strategy compared to individual strategies.
The detailed results can be found in Appendix [C.2.3](https://arxiv.org/html/2506.13977v1#A3.SS2.SSS3 "C.2.3 Data Evolution ‣ C.2 Prompts Demonstration ‣ Appendix C Implementation Details ‣ CriticTool: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios").

![Refer to caption](https://arxiv.org/html/2506.13977v1/x5.png)


Figure 4:  Comparison between BFCL Overall Accuracy and CriticTool Overall Scores across several models. LLMs show similar trends in tool-use and self-critique capabilities.

**Q4: What is the Relationship Between Tool-Use and Self-Critique Capabilities?**We compare the fine-grained evaluations on CriticTool with the results of the benchmark designed to explore tool-use capabilities, investigating the relationship between models’ self-critique capabilities in tool-calling tasks and their tool-use capabilities.
We analyze the overall accuracy metric from tool-use benchmarks to examine the relationship between the tool-use performances of selected models and their Overall performance on CriticTool.
As results shown in Fig. [4](https://arxiv.org/html/2506.13977v1#S3.F4 "Figure 4 ‣ 3.2 Benchmarking Results on CriticTool ‣ 3 Experiment ‣ CriticTool: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios"), we observe a general alignment between the trends in models’ tool-use and self-critique capabilities.
This observation not only indicates a strong connection between models’ ability to accurately use tools and their self-critique capabilities, suggesting that strengthening self-critique mechanisms could provide a promising avenue for enhancing overall tool-use performance, but also validates the rationale behind our benchmark.
