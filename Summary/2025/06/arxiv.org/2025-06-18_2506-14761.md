---
title: 'From Bytes to Ideas: Language Modeling with Autoregressive U-Nets'
source: https://arxiv.org/html/2506.14761
author:
  - arxiv.org
published: ''
fetched: '2025-06-18T11:20:05.814998+00:00'
tags:
  - codex
  - arxiv
image: 
---

## è¦ç´„

å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å›ºå®šé•·ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã™ã‚‹å¾“æ¥æ‰‹æ³•ã®åˆ¶ç´„ã‚’ç·©å’Œã™ã‚‹ãŸã‚ã€ãƒã‚¤ãƒˆåˆ—ã‹ã‚‰ç›´æ¥å­¦ç¿’ã—è‡ªåˆ†ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹éšå±¤å‹U-Netã‚’å°å…¥ã€‚ãƒ¢ãƒ‡ãƒ«ã¯æ–‡å­—ã‹ã‚‰å˜èªã€å¥ã¸ã¨æ®µéšçš„ã«ãƒ—ãƒ¼ãƒ«ã—ã€ä¸Šä½æ®µã»ã©å…ˆã®å˜èªã‚’è¦‹é€šã—ã¦äºˆæ¸¬ã™ã‚‹ã€‚è¨ˆç®—é‡ã‚’æƒãˆãŸå­¦ç¿’æ¯”è¼ƒã§ã¯æµ…ã„éšå±¤ã§BPEã¨åŒç­‰ã€æ·±ã„éšå±¤ã§ã¯ã‚ˆã‚Šå¤§è¦æ¨¡ãªäºˆæ¸¬èƒ½åŠ›ãŒæœŸå¾…ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚å†…éƒ¨ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’æŠ±ãˆã‚‹ãŸã‚æ–‡å­—å˜ä½ã‚¿ã‚¹ã‚¯ã‚„ä½ãƒªã‚½ãƒ¼ã‚¹è¨€èªã¸ã®é©ç”¨ãŒå®¹æ˜“ã«ãªã‚‹ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯U-Netæ§‹é€ ã®å„æ®µãŒåç¸®ã¨æ‹¡å¼µã‚’é€šã˜ã¦å¤šæ§˜ãªç²’åº¦ã®è¡¨ç¾ã‚’å­¦ç¿’ã—ã€ã‚¹ã‚­ãƒƒãƒ—æ¥ç¶šã§æƒ…å ±ã‚’ä¿æŒã€‚å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®æ¢ç´¢ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã‚‚ã‚¹ãƒ ãƒ¼ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯èƒ½ã§ã‚ã‚‹ã€‚å®Ÿé¨“ã¯ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«è¨ˆç®—é‡ã®æ¯”ç‡ã‚’ç¶­æŒã™ã‚‹é‡è¦æ€§ã‚’ç¤ºã—ã€å°†æ¥çš„ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æŸ”è»Ÿãªæ§‹ç¯‰ã«ç¹‹ãŒã‚‹ã¨æœŸå¾…ã•ã‚Œã‚‹ã€‚

## æœ¬æ–‡

### 2.1 Autoregressive U-Net

Table 1: 1B equivalent on 370B tokens

Inspired by U-Net-like architecturesÂ (Ronneberger etÂ al., [2015](https://arxiv.org/html/2506.14761v1#bib.bib3); Nawrot etÂ al., [2022](https://arxiv.org/html/2506.14761v1#bib.bib4)), we propose an autoregressive hierarchical model for language modeling, illustrated inÂ [figureÂ 1](https://arxiv.org/html/2506.14761v1#S0.F1 "In From Bytes to Ideas: Language Modeling with Autoregressive U-Nets").
This architecture features a *contracting path*, which compresses the input sequence, and an *expanding path*, which reconstructs it. Both paths are fully *adaptive*: they do not require fixed pooling or upsampling sizes.
Pooling and upsampling operations can be designed independently, even if we choose to make them symmetrical in this paper.
The only requirement is a *splitting function*, which specifies the positions in the sequence where pooling should occur. This function is detailed inÂ [sectionÂ 2.2](https://arxiv.org/html/2506.14761v1#S2.SS2 "2.2 Splitting Function â€£ 2 Method â€£ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets").

Our architecture is *monolithic*: unlike recent approachesÂ (Pagnoni etÂ al., [2024](https://arxiv.org/html/2506.14761v1#bib.bib5); Neitemeier etÂ al., [2025](https://arxiv.org/html/2506.14761v1#bib.bib6)) that use local models, we apply attention globally at each stage (or within a sliding window), allowing every input to attend to previous inputs.
This ensures that words or word groups are not processed in isolation.
To preserve fine-grained information that might be lost during contraction, we introduce skip connections between stages, following the approach inÂ Ronneberger etÂ al. ([2015](https://arxiv.org/html/2506.14761v1#bib.bib3)) andÂ Nawrot etÂ al. ([2022](https://arxiv.org/html/2506.14761v1#bib.bib4)).
We also increase the hidden dimension at each stage in proportion to its contraction factor, enabling richer representations as the sequence is contracted.
To keep computation tractable at the byte-level stage (Stage 1), where sequences are longest, we restrict attention to a window.

#### 2.1.1 Pooling and Upsampling

Since our pooling and upsampling are adaptive, we cannot rely on fixed window sizes.
To address this, we explored several pooling and upsampling strategies.
In this section, we describe the method used in all experiments reported in the main text.
A complete description of the alternatives and ablation results can be found in the appendixÂ [8](https://arxiv.org/html/2506.14761v1#S8 "8 Ablation â€£ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets").

![Refer to caption](https://arxiv.org/html/2506.14761v1/x2.png)


Figure 2: Pooling simply selects the vectors at the positions specified by the splitting function.
Upsampling then expands each pooled vector to fill the next segment, applying a separate linear layer for each position.
For instance, the pooled vector representing the word â€˜SATâ£â€™ is used to help predict â€˜ONâ£â€™.
This offset lets deeper stages predict further ahead in the sequence.
When using 4 stages, for example, this results in the deepest stage helping for the prediction of the next four words.

**Pooling.**
We adopt the simplest pooling strategy: selecting the indices identified by the splitting function and projecting them to the next stageâ€™s dimensionality using a linear layer.
Since the preceding layers already include attention mechanisms, we rely on these to do the pooling implicitly instead of relying on explicit cross attention as used in Nawrot etÂ al. ([2022](https://arxiv.org/html/2506.14761v1#bib.bib4)); Pagnoni etÂ al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib5)).

**Upsampling.**
The upsampling step maps coarse representations to finer ones for the next stage. As illustrated inÂ [FigureÂ 2](https://arxiv.org/html/2506.14761v1#S2.F2 "In 2.1.1 Pooling and Upsampling â€£ 2.1 Autoregressive U-Net â€£ 2 Method â€£ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets"), we duplicate each coarse vector to match the length of the **following** segment, applying distinct, position-specific linear transformations to these duplicates.
Since these transformations are shared across segments but vary by position within a segment, we term this *Multi-Linear Upsampling*.
In our experiments, models with multiple stages are more sensitive to the specific choice of upsampling strategy, whereas for pooling, many strategies work equally well.

#### 2.1.2 Generation

During training, we process the entire input sequence in parallel, activating all stages simultaneously.
At inference, generation is autoregressive: the byte-level stage is active at every step, while deeper stages activate less frequently according to the pooling pattern.
Skip connections transmit information upward at each stage, so deeper stages can integrate fine-grained details.
This cascading, conditional activation enables efficient inference: computationally intensive high-level stages activate rarely, but still effectively guide detailed lower-level predictions.
In practice, this means that we need to cache the latest vector at the output of each stage to correctly propagate deeper stagesâ€™ outputs.

### 2.3 Evaluating on different scales

Large language models scale very predictablyÂ Kaplan etÂ al. ([2020](https://arxiv.org/html/2506.14761v1#bib.bib9)); Hoffmann etÂ al. ([2022](https://arxiv.org/html/2506.14761v1#bib.bib10)); Bi etÂ al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)).
This allows us to estimate the performance of a model for a large compute budget.
But more surprisingly, it allows us to predict the optimal hyperparameters for models way beyond our ablation budget.
Bi etÂ al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)) described a method for sweeping learning rates and batch sizes across a range of small models, and they demonstrated that these results can be used to predict optimal hyperparameters for larger models.
Following their methodology, we show a different evolution of hyperparameters, both due to the data in our setup and to the hierarchical model. These hyperparameters are then used to do scaling laws for a bigger range of compute budgets to compare the baseline architecture and AU-Net.
Throughout this paper, the *scale* of a run is its total pre-training compute Cğ¶Citalic\_C measured in Floating Point Operation (FLOP):

|  |  |  |
| --- | --- | --- |
|  | C=Fmodel / input-unitâŸFLOPs per (forward+backward) pass per input unitÃ—Ninput-unitâŸnumber of units of training input.ğ¶subscriptâŸsubscriptğ¹model / input-unitFLOPs per (forward+backward) pass per input unitsubscriptâŸsubscriptğ‘input-unitnumber of units of training inputC=\underbrace{F\_{\text{model / input-unit}}}\_{\text{FLOPs per (forward+% backward) pass per input unit}}\times\underbrace{N\_{\text{input-unit}}}\_{\text% {number of units of training input}}.italic\_C = underâŸ start\_ARG italic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT end\_ARG start\_POSTSUBSCRIPT FLOPs per (forward+backward) pass per input unit end\_POSTSUBSCRIPT Ã— underâŸ start\_ARG italic\_N start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT end\_ARG start\_POSTSUBSCRIPT number of units of training input end\_POSTSUBSCRIPT . |  |

Following Bi etÂ al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)), we define model size as the number of FLOPs per input unit instead of relying on the number of parameters.
This allows us to compare models with different architectures fairly.
The formula for the number of FLOP per input-unit for a decoder-only transformer is given by:

|  |  |  |
| --- | --- | --- |
|  | Fmodel / input-unit=6â¢Nparamsno-embedâŸlinear term+6â¢dâ¢Lâ¢SâŸattention term.subscriptğ¹model / input-unitsubscriptâŸ6superscriptsubscriptğ‘paramsno-embedlinear termsubscriptâŸ6ğ‘‘ğ¿ğ‘†attention termF\_{\text{model / input-unit}}=\underbrace{6N\_{\text{params}}^{\text{no-embed}}% }\_{\text{linear term}}+\underbrace{6d\,L\,S}\_{\text{attention term}}.italic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT = underâŸ start\_ARG 6 italic\_N start\_POSTSUBSCRIPT params end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT no-embed end\_POSTSUPERSCRIPT end\_ARG start\_POSTSUBSCRIPT linear term end\_POSTSUBSCRIPT + underâŸ start\_ARG 6 italic\_d italic\_L italic\_S end\_ARG start\_POSTSUBSCRIPT attention term end\_POSTSUBSCRIPT . |  |

where, Nparamsno-embedsuperscriptsubscriptğ‘paramsno-embedN\_{\text{params}}^{\text{no-embed}}italic\_N start\_POSTSUBSCRIPT params end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT no-embed end\_POSTSUPERSCRIPT is the number of parameters, excluding the embeddings. dğ‘‘ditalic\_d is the dimension, Sğ‘†Sitalic\_S the sequence length and Lğ¿Litalic\_L the number of layers.
To scale up, one can either make the model bigger (Fmodel / input-unitâ†‘â†‘subscriptğ¹model / input-unitabsentF\_{\text{model / input-unit}}\uparrowitalic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT â†‘), give it more data (Ninput-unitâ†‘â†‘subscriptğ‘input-unitabsentN\_{\text{input-unit}}\uparrowitalic\_N start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT â†‘), or do both.
Gadre etÂ al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib12)) showed that keeping the *data-to-model ratio* Î³input-unitsubscriptğ›¾input-unit\gamma\_{\text{input-unit}}italic\_Î³ start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT constant is key to getting smooth scaling laws and predictable performance, where:

|  |  |  |
| --- | --- | --- |
|  | Î³input-unit=Ninput-unitFmodel / input-unit.subscriptğ›¾input-unitsubscriptğ‘input-unitsubscriptğ¹model / input-unit\gamma\_{\text{input-unit}}=\frac{N\_{\text{input-unit}}}{F\_{\text{model / input% -unit}}}.italic\_Î³ start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT = divide start\_ARG italic\_N start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT end\_ARG . |  |

We adopt this convention in all experiments and report the data-to-model ratio Î³input-unitsubscriptğ›¾input-unit\gamma\_{\text{input-unit}}italic\_Î³ start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT used in the experiments.

**Bytes versus tokens.**
On DCLM, a token sequence is on average kâ‰ˆ4.56ğ‘˜4.56k\approx 4.56italic\_k â‰ˆ 4.56 times shorter than its byte sequence when using the LLaMa 3 tokenizer.

Given some compression factor kğ‘˜kitalic\_k between bytes and tokens, we want to express the equivalent Î³bytessubscriptğ›¾bytes\gamma\_{\text{bytes}}italic\_Î³ start\_POSTSUBSCRIPT bytes end\_POSTSUBSCRIPT.
To do this, we note that Nbyte=kÃ—Ntokensubscriptğ‘byteğ‘˜subscriptğ‘tokenN\_{\text{byte}}=k\times N\_{\text{token}}italic\_N start\_POSTSUBSCRIPT byte end\_POSTSUBSCRIPT = italic\_k Ã— italic\_N start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT and
Fmodel/byte=Fmodel/token/ksubscriptğ¹model/bytesubscriptğ¹model/tokenğ‘˜F\_{\text{model/byte}}=F\_{\text{model/token}}/kitalic\_F start\_POSTSUBSCRIPT model/byte end\_POSTSUBSCRIPT = italic\_F start\_POSTSUBSCRIPT model/token end\_POSTSUBSCRIPT / italic\_k. Therefore,

|  |  |  |
| --- | --- | --- |
|  | Î³byte=k2â¢NtokenFmodel/token=k2â¢Î³token.subscriptğ›¾bytesuperscriptğ‘˜2subscriptğ‘tokensubscriptğ¹model/tokensuperscriptğ‘˜2subscriptğ›¾token\gamma\_{\text{byte}}=k^{2}\frac{N\_{\text{token}}}{F\_{\text{model/token}}}=k^{2% }\gamma\_{\text{token}}.italic\_Î³ start\_POSTSUBSCRIPT byte end\_POSTSUBSCRIPT = italic\_k start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT divide start\_ARG italic\_N start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_F start\_POSTSUBSCRIPT model/token end\_POSTSUBSCRIPT end\_ARG = italic\_k start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT italic\_Î³ start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT . |  |

This factor allows us to compare the performance of our model with the baseline on the same scale, as they will have seen the same amount of data and spent the same amount of FLOPs per token.
Throughout the paper, we always express the data-to-model ratio in LLaMa 3 tokens (Î³tokensubscriptğ›¾token\gamma\_{\text{token}}italic\_Î³ start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT).

**FLOPS per byte for AU-Net.**
In the case of AU-Net, we cannot use the same formula as the baseline because of the contraction and expansion happening in the model.
However, we can still use the same formulas as long as we account for the contraction at each stage.
So the total FLOPs per byte for AU-Net is simply the sum of each stage divided by the contraction factor.

|  |  |  |
| --- | --- | --- |
|  | Fmodel/byte=âˆ‘i=1LFmodel/byteiki,subscriptğ¹model/bytesuperscriptsubscriptğ‘–1ğ¿subscriptsuperscriptğ¹ğ‘–model/bytesubscriptğ‘˜ğ‘–F\_{\text{model/byte}}=\sum\_{i=1}^{L}\frac{F^{i}\_{\text{model/byte}}}{k\_{i}},italic\_F start\_POSTSUBSCRIPT model/byte end\_POSTSUBSCRIPT = âˆ‘ start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_L end\_POSTSUPERSCRIPT divide start\_ARG italic\_F start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT model/byte end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_k start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_ARG , |  |

where kisubscriptğ‘˜ğ‘–k\_{i}italic\_k start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT is the contraction factor at stage iğ‘–iitalic\_i.

This property allows us to have models with a higher number of parameters for the same compute budget and data-to-model ratio.

**Hyperparameter scaling laws**
Bi etÂ al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)) showed that the regularity of scaling laws can be exploited to tune very large models from a sweep over much smaller ones.
We replicate their protocol on six miniature versions of each architecture (baseline Transformer and AU-Net):
we perform a quasi-random search over batch size and learning rate, keep the configurations within 1% of the best validation loss, and fit BSZâ¢(C)=Aâ¢CÎ±BSZğ¶ğ´superscriptğ¶ğ›¼\text{BSZ}(C)=A\,C^{\alpha}BSZ ( italic\_C ) = italic\_A italic\_C start\_POSTSUPERSCRIPT italic\_Î± end\_POSTSUPERSCRIPT and LRâ¢(C)=Bâ¢CÎ²LRğ¶ğµsuperscriptğ¶ğ›½\text{LR}(C)=B\,C^{\beta}LR ( italic\_C ) = italic\_B italic\_C start\_POSTSUPERSCRIPT italic\_Î² end\_POSTSUPERSCRIPT to those points, with parameters A,Î±,Bâ¢Â andÂ â¢Î²

ğ´ğ›¼ğµÂ andÂ ğ›½A,\alpha,B\text{ and }\betaitalic\_A , italic\_Î± , italic\_B and italic\_Î².
We find the following formulas at the byte level for AU-Net:

|  |  |  |
| --- | --- | --- |
|  | BSZAU-Netâ¢(C)=0.66â¢C0.321LRAU-Netâ¢(C)=6.6Ã—Câˆ’0.176.formulae-sequencesubscriptBSZAU-Netğ¶0.66superscriptğ¶0.321subscriptLRAU-Netğ¶6.6superscriptğ¶0.176\text{BSZ}\_{\text{AU-Net}}(C)=0.66C^{0.321}\qquad\text{LR}\_{\text{AU-Net}}(C)=% 6.6\times C^{-0.176}.BSZ start\_POSTSUBSCRIPT AU-Net end\_POSTSUBSCRIPT ( italic\_C ) = 0.66 italic\_C start\_POSTSUPERSCRIPT 0.321 end\_POSTSUPERSCRIPT LR start\_POSTSUBSCRIPT AU-Net end\_POSTSUBSCRIPT ( italic\_C ) = 6.6 Ã— italic\_C start\_POSTSUPERSCRIPT - 0.176 end\_POSTSUPERSCRIPT . |  |

And we run the same tuning for the BPE baseline, for which we find:

|  |  |  |
| --- | --- | --- |
|  | BSZBPEâ¢(C)=29.9â¢C0.231LRBPEâ¢(C)=19.3Ã—Câˆ’0.177.formulae-sequencesubscriptBSZBPEğ¶29.9superscriptğ¶0.231subscriptLRBPEğ¶19.3superscriptğ¶0.177\text{BSZ}\_{\text{BPE}}(C)=29.9C^{0.231}\qquad\text{LR}\_{\text{BPE}}(C)=19.3% \times C^{-0.177}.BSZ start\_POSTSUBSCRIPT BPE end\_POSTSUBSCRIPT ( italic\_C ) = 29.9 italic\_C start\_POSTSUPERSCRIPT 0.231 end\_POSTSUPERSCRIPT LR start\_POSTSUBSCRIPT BPE end\_POSTSUBSCRIPT ( italic\_C ) = 19.3 Ã— italic\_C start\_POSTSUPERSCRIPT - 0.177 end\_POSTSUPERSCRIPT . |  |
