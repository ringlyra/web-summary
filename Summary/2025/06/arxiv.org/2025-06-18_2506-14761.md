---
title: 'From Bytes to Ideas: Language Modeling with Autoregressive U-Nets'
source: https://arxiv.org/html/2506.14761
author:
  - arxiv.org
published: ''
fetched: '2025-06-18T11:20:05.814998+00:00'
tags:
  - codex
  - arxiv
image: 
---

## 要約

入力テキストを固定長トークンに分割する従来手法の制約を緩和するため、バイト列から直接学習し自分でトークンを生成する階層型U-Netを導入。モデルは文字から単語、句へと段階的にプールし、上位段ほど先の単語を見通して予測する。計算量を揃えた学習比較では浅い階層でBPEと同等、深い階層ではより大規模な予測能力が期待できることを確認。内部にトークナイザを抱えるため文字単位タスクや低リソース言語への適用が容易になる。このフレームワークではU-Net構造の各段が収縮と拡張を通じて多様な粒度の表現を学習し、スキップ接続で情報を保持。小規模モデルの探索から得られたハイパーパラメータを利用することで、大規模モデルでもスムーズにスケーリング可能である。実験はデータとモデル計算量の比率を維持する重要性を示し、将来的に大規模言語モデルの柔軟な構築に繋がると期待される。

## 本文

### 2.1 Autoregressive U-Net

Table 1: 1B equivalent on 370B tokens

Inspired by U-Net-like architectures (Ronneberger et al., [2015](https://arxiv.org/html/2506.14761v1#bib.bib3); Nawrot et al., [2022](https://arxiv.org/html/2506.14761v1#bib.bib4)), we propose an autoregressive hierarchical model for language modeling, illustrated in [figure 1](https://arxiv.org/html/2506.14761v1#S0.F1 "In From Bytes to Ideas: Language Modeling with Autoregressive U-Nets").
This architecture features a *contracting path*, which compresses the input sequence, and an *expanding path*, which reconstructs it. Both paths are fully *adaptive*: they do not require fixed pooling or upsampling sizes.
Pooling and upsampling operations can be designed independently, even if we choose to make them symmetrical in this paper.
The only requirement is a *splitting function*, which specifies the positions in the sequence where pooling should occur. This function is detailed in [section 2.2](https://arxiv.org/html/2506.14761v1#S2.SS2 "2.2 Splitting Function ‣ 2 Method ‣ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets").

Our architecture is *monolithic*: unlike recent approaches (Pagnoni et al., [2024](https://arxiv.org/html/2506.14761v1#bib.bib5); Neitemeier et al., [2025](https://arxiv.org/html/2506.14761v1#bib.bib6)) that use local models, we apply attention globally at each stage (or within a sliding window), allowing every input to attend to previous inputs.
This ensures that words or word groups are not processed in isolation.
To preserve fine-grained information that might be lost during contraction, we introduce skip connections between stages, following the approach in Ronneberger et al. ([2015](https://arxiv.org/html/2506.14761v1#bib.bib3)) and Nawrot et al. ([2022](https://arxiv.org/html/2506.14761v1#bib.bib4)).
We also increase the hidden dimension at each stage in proportion to its contraction factor, enabling richer representations as the sequence is contracted.
To keep computation tractable at the byte-level stage (Stage 1), where sequences are longest, we restrict attention to a window.

#### 2.1.1 Pooling and Upsampling

Since our pooling and upsampling are adaptive, we cannot rely on fixed window sizes.
To address this, we explored several pooling and upsampling strategies.
In this section, we describe the method used in all experiments reported in the main text.
A complete description of the alternatives and ablation results can be found in the appendix [8](https://arxiv.org/html/2506.14761v1#S8 "8 Ablation ‣ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets").

![Refer to caption](https://arxiv.org/html/2506.14761v1/x2.png)


Figure 2: Pooling simply selects the vectors at the positions specified by the splitting function.
Upsampling then expands each pooled vector to fill the next segment, applying a separate linear layer for each position.
For instance, the pooled vector representing the word ‘SAT␣’ is used to help predict ‘ON␣’.
This offset lets deeper stages predict further ahead in the sequence.
When using 4 stages, for example, this results in the deepest stage helping for the prediction of the next four words.

**Pooling.**
We adopt the simplest pooling strategy: selecting the indices identified by the splitting function and projecting them to the next stage’s dimensionality using a linear layer.
Since the preceding layers already include attention mechanisms, we rely on these to do the pooling implicitly instead of relying on explicit cross attention as used in Nawrot et al. ([2022](https://arxiv.org/html/2506.14761v1#bib.bib4)); Pagnoni et al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib5)).

**Upsampling.**
The upsampling step maps coarse representations to finer ones for the next stage. As illustrated in [Figure 2](https://arxiv.org/html/2506.14761v1#S2.F2 "In 2.1.1 Pooling and Upsampling ‣ 2.1 Autoregressive U-Net ‣ 2 Method ‣ From Bytes to Ideas: Language Modeling with Autoregressive U-Nets"), we duplicate each coarse vector to match the length of the **following** segment, applying distinct, position-specific linear transformations to these duplicates.
Since these transformations are shared across segments but vary by position within a segment, we term this *Multi-Linear Upsampling*.
In our experiments, models with multiple stages are more sensitive to the specific choice of upsampling strategy, whereas for pooling, many strategies work equally well.

#### 2.1.2 Generation

During training, we process the entire input sequence in parallel, activating all stages simultaneously.
At inference, generation is autoregressive: the byte-level stage is active at every step, while deeper stages activate less frequently according to the pooling pattern.
Skip connections transmit information upward at each stage, so deeper stages can integrate fine-grained details.
This cascading, conditional activation enables efficient inference: computationally intensive high-level stages activate rarely, but still effectively guide detailed lower-level predictions.
In practice, this means that we need to cache the latest vector at the output of each stage to correctly propagate deeper stages’ outputs.

### 2.3 Evaluating on different scales

Large language models scale very predictably Kaplan et al. ([2020](https://arxiv.org/html/2506.14761v1#bib.bib9)); Hoffmann et al. ([2022](https://arxiv.org/html/2506.14761v1#bib.bib10)); Bi et al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)).
This allows us to estimate the performance of a model for a large compute budget.
But more surprisingly, it allows us to predict the optimal hyperparameters for models way beyond our ablation budget.
Bi et al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)) described a method for sweeping learning rates and batch sizes across a range of small models, and they demonstrated that these results can be used to predict optimal hyperparameters for larger models.
Following their methodology, we show a different evolution of hyperparameters, both due to the data in our setup and to the hierarchical model. These hyperparameters are then used to do scaling laws for a bigger range of compute budgets to compare the baseline architecture and AU-Net.
Throughout this paper, the *scale* of a run is its total pre-training compute C𝐶Citalic\_C measured in Floating Point Operation (FLOP):

|  |  |  |
| --- | --- | --- |
|  | C=Fmodel / input-unit⏟FLOPs per (forward+backward) pass per input unit×Ninput-unit⏟number of units of training input.𝐶subscript⏟subscript𝐹model / input-unitFLOPs per (forward+backward) pass per input unitsubscript⏟subscript𝑁input-unitnumber of units of training inputC=\underbrace{F\_{\text{model / input-unit}}}\_{\text{FLOPs per (forward+% backward) pass per input unit}}\times\underbrace{N\_{\text{input-unit}}}\_{\text% {number of units of training input}}.italic\_C = under⏟ start\_ARG italic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT end\_ARG start\_POSTSUBSCRIPT FLOPs per (forward+backward) pass per input unit end\_POSTSUBSCRIPT × under⏟ start\_ARG italic\_N start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT end\_ARG start\_POSTSUBSCRIPT number of units of training input end\_POSTSUBSCRIPT . |  |

Following Bi et al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)), we define model size as the number of FLOPs per input unit instead of relying on the number of parameters.
This allows us to compare models with different architectures fairly.
The formula for the number of FLOP per input-unit for a decoder-only transformer is given by:

|  |  |  |
| --- | --- | --- |
|  | Fmodel / input-unit=6⁢Nparamsno-embed⏟linear term+6⁢d⁢L⁢S⏟attention term.subscript𝐹model / input-unitsubscript⏟6superscriptsubscript𝑁paramsno-embedlinear termsubscript⏟6𝑑𝐿𝑆attention termF\_{\text{model / input-unit}}=\underbrace{6N\_{\text{params}}^{\text{no-embed}}% }\_{\text{linear term}}+\underbrace{6d\,L\,S}\_{\text{attention term}}.italic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT = under⏟ start\_ARG 6 italic\_N start\_POSTSUBSCRIPT params end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT no-embed end\_POSTSUPERSCRIPT end\_ARG start\_POSTSUBSCRIPT linear term end\_POSTSUBSCRIPT + under⏟ start\_ARG 6 italic\_d italic\_L italic\_S end\_ARG start\_POSTSUBSCRIPT attention term end\_POSTSUBSCRIPT . |  |

where, Nparamsno-embedsuperscriptsubscript𝑁paramsno-embedN\_{\text{params}}^{\text{no-embed}}italic\_N start\_POSTSUBSCRIPT params end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT no-embed end\_POSTSUPERSCRIPT is the number of parameters, excluding the embeddings. d𝑑ditalic\_d is the dimension, S𝑆Sitalic\_S the sequence length and L𝐿Litalic\_L the number of layers.
To scale up, one can either make the model bigger (Fmodel / input-unit↑↑subscript𝐹model / input-unitabsentF\_{\text{model / input-unit}}\uparrowitalic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT ↑), give it more data (Ninput-unit↑↑subscript𝑁input-unitabsentN\_{\text{input-unit}}\uparrowitalic\_N start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT ↑), or do both.
Gadre et al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib12)) showed that keeping the *data-to-model ratio* γinput-unitsubscript𝛾input-unit\gamma\_{\text{input-unit}}italic\_γ start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT constant is key to getting smooth scaling laws and predictable performance, where:

|  |  |  |
| --- | --- | --- |
|  | γinput-unit=Ninput-unitFmodel / input-unit.subscript𝛾input-unitsubscript𝑁input-unitsubscript𝐹model / input-unit\gamma\_{\text{input-unit}}=\frac{N\_{\text{input-unit}}}{F\_{\text{model / input% -unit}}}.italic\_γ start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT = divide start\_ARG italic\_N start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_F start\_POSTSUBSCRIPT model / input-unit end\_POSTSUBSCRIPT end\_ARG . |  |

We adopt this convention in all experiments and report the data-to-model ratio γinput-unitsubscript𝛾input-unit\gamma\_{\text{input-unit}}italic\_γ start\_POSTSUBSCRIPT input-unit end\_POSTSUBSCRIPT used in the experiments.

**Bytes versus tokens.**
On DCLM, a token sequence is on average k≈4.56𝑘4.56k\approx 4.56italic\_k ≈ 4.56 times shorter than its byte sequence when using the LLaMa 3 tokenizer.

Given some compression factor k𝑘kitalic\_k between bytes and tokens, we want to express the equivalent γbytessubscript𝛾bytes\gamma\_{\text{bytes}}italic\_γ start\_POSTSUBSCRIPT bytes end\_POSTSUBSCRIPT.
To do this, we note that Nbyte=k×Ntokensubscript𝑁byte𝑘subscript𝑁tokenN\_{\text{byte}}=k\times N\_{\text{token}}italic\_N start\_POSTSUBSCRIPT byte end\_POSTSUBSCRIPT = italic\_k × italic\_N start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT and
Fmodel/byte=Fmodel/token/ksubscript𝐹model/bytesubscript𝐹model/token𝑘F\_{\text{model/byte}}=F\_{\text{model/token}}/kitalic\_F start\_POSTSUBSCRIPT model/byte end\_POSTSUBSCRIPT = italic\_F start\_POSTSUBSCRIPT model/token end\_POSTSUBSCRIPT / italic\_k. Therefore,

|  |  |  |
| --- | --- | --- |
|  | γbyte=k2⁢NtokenFmodel/token=k2⁢γtoken.subscript𝛾bytesuperscript𝑘2subscript𝑁tokensubscript𝐹model/tokensuperscript𝑘2subscript𝛾token\gamma\_{\text{byte}}=k^{2}\frac{N\_{\text{token}}}{F\_{\text{model/token}}}=k^{2% }\gamma\_{\text{token}}.italic\_γ start\_POSTSUBSCRIPT byte end\_POSTSUBSCRIPT = italic\_k start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT divide start\_ARG italic\_N start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_F start\_POSTSUBSCRIPT model/token end\_POSTSUBSCRIPT end\_ARG = italic\_k start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT italic\_γ start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT . |  |

This factor allows us to compare the performance of our model with the baseline on the same scale, as they will have seen the same amount of data and spent the same amount of FLOPs per token.
Throughout the paper, we always express the data-to-model ratio in LLaMa 3 tokens (γtokensubscript𝛾token\gamma\_{\text{token}}italic\_γ start\_POSTSUBSCRIPT token end\_POSTSUBSCRIPT).

**FLOPS per byte for AU-Net.**
In the case of AU-Net, we cannot use the same formula as the baseline because of the contraction and expansion happening in the model.
However, we can still use the same formulas as long as we account for the contraction at each stage.
So the total FLOPs per byte for AU-Net is simply the sum of each stage divided by the contraction factor.

|  |  |  |
| --- | --- | --- |
|  | Fmodel/byte=∑i=1LFmodel/byteiki,subscript𝐹model/bytesuperscriptsubscript𝑖1𝐿subscriptsuperscript𝐹𝑖model/bytesubscript𝑘𝑖F\_{\text{model/byte}}=\sum\_{i=1}^{L}\frac{F^{i}\_{\text{model/byte}}}{k\_{i}},italic\_F start\_POSTSUBSCRIPT model/byte end\_POSTSUBSCRIPT = ∑ start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_L end\_POSTSUPERSCRIPT divide start\_ARG italic\_F start\_POSTSUPERSCRIPT italic\_i end\_POSTSUPERSCRIPT start\_POSTSUBSCRIPT model/byte end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_k start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_ARG , |  |

where kisubscript𝑘𝑖k\_{i}italic\_k start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT is the contraction factor at stage i𝑖iitalic\_i.

This property allows us to have models with a higher number of parameters for the same compute budget and data-to-model ratio.

**Hyperparameter scaling laws**
Bi et al. ([2024](https://arxiv.org/html/2506.14761v1#bib.bib11)) showed that the regularity of scaling laws can be exploited to tune very large models from a sweep over much smaller ones.
We replicate their protocol on six miniature versions of each architecture (baseline Transformer and AU-Net):
we perform a quasi-random search over batch size and learning rate, keep the configurations within 1% of the best validation loss, and fit BSZ⁢(C)=A⁢CαBSZ𝐶𝐴superscript𝐶𝛼\text{BSZ}(C)=A\,C^{\alpha}BSZ ( italic\_C ) = italic\_A italic\_C start\_POSTSUPERSCRIPT italic\_α end\_POSTSUPERSCRIPT and LR⁢(C)=B⁢CβLR𝐶𝐵superscript𝐶𝛽\text{LR}(C)=B\,C^{\beta}LR ( italic\_C ) = italic\_B italic\_C start\_POSTSUPERSCRIPT italic\_β end\_POSTSUPERSCRIPT to those points, with parameters A,α,B⁢ and ⁢β

𝐴𝛼𝐵 and 𝛽A,\alpha,B\text{ and }\betaitalic\_A , italic\_α , italic\_B and italic\_β.
We find the following formulas at the byte level for AU-Net:

|  |  |  |
| --- | --- | --- |
|  | BSZAU-Net⁢(C)=0.66⁢C0.321LRAU-Net⁢(C)=6.6×C−0.176.formulae-sequencesubscriptBSZAU-Net𝐶0.66superscript𝐶0.321subscriptLRAU-Net𝐶6.6superscript𝐶0.176\text{BSZ}\_{\text{AU-Net}}(C)=0.66C^{0.321}\qquad\text{LR}\_{\text{AU-Net}}(C)=% 6.6\times C^{-0.176}.BSZ start\_POSTSUBSCRIPT AU-Net end\_POSTSUBSCRIPT ( italic\_C ) = 0.66 italic\_C start\_POSTSUPERSCRIPT 0.321 end\_POSTSUPERSCRIPT LR start\_POSTSUBSCRIPT AU-Net end\_POSTSUBSCRIPT ( italic\_C ) = 6.6 × italic\_C start\_POSTSUPERSCRIPT - 0.176 end\_POSTSUPERSCRIPT . |  |

And we run the same tuning for the BPE baseline, for which we find:

|  |  |  |
| --- | --- | --- |
|  | BSZBPE⁢(C)=29.9⁢C0.231LRBPE⁢(C)=19.3×C−0.177.formulae-sequencesubscriptBSZBPE𝐶29.9superscript𝐶0.231subscriptLRBPE𝐶19.3superscript𝐶0.177\text{BSZ}\_{\text{BPE}}(C)=29.9C^{0.231}\qquad\text{LR}\_{\text{BPE}}(C)=19.3% \times C^{-0.177}.BSZ start\_POSTSUBSCRIPT BPE end\_POSTSUBSCRIPT ( italic\_C ) = 29.9 italic\_C start\_POSTSUPERSCRIPT 0.231 end\_POSTSUPERSCRIPT LR start\_POSTSUBSCRIPT BPE end\_POSTSUBSCRIPT ( italic\_C ) = 19.3 × italic\_C start\_POSTSUPERSCRIPT - 0.177 end\_POSTSUPERSCRIPT . |  |
