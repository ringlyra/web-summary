---
title: 'Human-like object concept representations emerge naturally in multimodal large language models'
source: https://arxiv.org/html/2407.01067v3
author:
  - arxiv.org
published: ''
fetched: '2025-06-16T10:37:37.536145+00:00'
tags:
  - codex
  - arxiv
image: 
---

## Ë¶ÅÁ¥Ñ
Êú¨Á†îÁ©∂„ÅØ„ÄÅË®ÄË™û„Åä„Çà„Å≥ÁîªÂÉè„ÇíÊâ±„ÅÜÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´(LLM/MLLM)„Åå„ÄÅ‰∫∫Èñì„ÅÆ„Çà„ÅÜ„Å™Áâ©‰ΩìÊ¶ÇÂøµË°®Áèæ„ÇíÁç≤Âæó„Åô„Çã„Åã„ÇíË™ø„Åπ„Åü„ÄÇLLM„Å®MLLM„Åã„ÇâÂèéÈõÜ„Åó„Åü470‰∏á‰ª∂„ÅÆ„Éà„É™„Éó„É¨„ÉÉ„ÉàÂà§Êñ≠„Çí„ÇÇ„Å®„Å´1,854Á®ÆÈ°û„ÅÆËá™ÁÑ∂Áâ©„ÅÆÈ°û‰ººÂ∫¶„ÇíË°®„Åô66Ê¨°ÂÖÉÂüã„ÇÅËæº„Åø„ÇíÁîüÊàê„Åó„Åü„ÄÇÂæó„Çâ„Çå„ÅüÂüã„ÇÅËæº„Åø„ÅØÂÆâÂÆö„Åó„Å¶„Åä„Çä„ÄÅÊÑèÂë≥ÁöÑ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÇíÁ§∫„Åó„ÄÅÂêÑÊ¨°ÂÖÉ„ÅØËß£ÈáàÂèØËÉΩ„Å†„Å£„Åü„ÄÇ„Åï„Çâ„Å´„ÄÅ„Åì„ÅÆÂüã„ÇÅËæº„Åø„ÅØË°åÂãï„Éá„Éº„Çø„ÅÆÂÜçÁèæÊÄß„ÅåÈ´ò„Åè„ÄÅËÑ≥Ê¥ªÂãï„Éë„Çø„Éº„É≥(Áâπ„Å´EBA„ÄÅPPA„ÄÅRSC„ÄÅFFA„Å™„Å©)„Å®„ÅÆÂº∑„ÅÑÂØæÂøú„ÇÇÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ„Åì„Çå„Çâ„ÅÆÁµêÊûú„ÅØ„ÄÅLLM/MLLM„ÅÆÁâ©‰ΩìË°®Ë±°„Åå‰∫∫Èñì„ÅÆÊ¶ÇÂøµÁü•Ë≠ò„Å®Êú¨Ë≥™ÁöÑ„Å™ÂÖ±ÈÄöÊÄß„ÇíÊúâ„Åó„ÄÅ„Çà„Çä‰∫∫Èñì„Çâ„Åó„ÅÑ‰∫∫Â∑•Áü•ËÉΩ„ÅÆÊßãÁØâ„Å´ÂØÑ‰∏é„Åô„ÇãÂèØËÉΩÊÄß„ÇíÁ§∫„Åó„Å¶„ÅÑ„Çã„ÄÇ„Åì„ÅÆÁü•Ë¶ã„ÅØÊ©üÊ¢∞Áü•ÊÄß„ÅÆÁêÜËß£„ÇíÊ∑±„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„ÅÇ„Çã„ÄÇ


## Êú¨Êñá

Introduction
------------

The ability to categorize and conceptualize objects forms the bedrock of human cognition, influencing everything from perception to decision-making. When confronted with diverse objects, humans can often differentiate their categories and concepts by making structured comparisons between them. This process is an essential part of human cognition in tasks ranging from everyday communication to problem-solving. In this cognitive process, our mental representations serve as a substrate, aiding in the recognition of objects [[1](https://arxiv.org/html/2407.01067v3#bib.bib1), [2](https://arxiv.org/html/2407.01067v3#bib.bib2)], formation of categories [[3](https://arxiv.org/html/2407.01067v3#bib.bib3), [4](https://arxiv.org/html/2407.01067v3#bib.bib4), [5](https://arxiv.org/html/2407.01067v3#bib.bib5)], organization of conceptual knowledge [[6](https://arxiv.org/html/2407.01067v3#bib.bib6), [7](https://arxiv.org/html/2407.01067v3#bib.bib7)], and the prediction of behaviors based on experiences. Therefore, understanding the structure of these representations is a fundamental pursuit in cognitive neuroscience and psychology [[8](https://arxiv.org/html/2407.01067v3#bib.bib8), [9](https://arxiv.org/html/2407.01067v3#bib.bib9), [10](https://arxiv.org/html/2407.01067v3#bib.bib10), [11](https://arxiv.org/html/2407.01067v3#bib.bib11)], underpinning significant research advancements in the field. For instance, various studies have identified potential dimensions that organize these representations, such as animals versus non-animals [[12](https://arxiv.org/html/2407.01067v3#bib.bib12), [13](https://arxiv.org/html/2407.01067v3#bib.bib13), [14](https://arxiv.org/html/2407.01067v3#bib.bib14), [15](https://arxiv.org/html/2407.01067v3#bib.bib15)], natural versus human-made [[16](https://arxiv.org/html/2407.01067v3#bib.bib16), [17](https://arxiv.org/html/2407.01067v3#bib.bib17)], and large versus small [[18](https://arxiv.org/html/2407.01067v3#bib.bib18), [19](https://arxiv.org/html/2407.01067v3#bib.bib19)].

The cognitive plausibility of deep learning systems has sparked significant debate [[20](https://arxiv.org/html/2407.01067v3#bib.bib20), [21](https://arxiv.org/html/2407.01067v3#bib.bib21)], with recent works often focusing on diverse neural networks pretrained on limited datasets for specific computer vision tasks like image classification [[22](https://arxiv.org/html/2407.01067v3#bib.bib22), [23](https://arxiv.org/html/2407.01067v3#bib.bib23), [24](https://arxiv.org/html/2407.01067v3#bib.bib24), [25](https://arxiv.org/html/2407.01067v3#bib.bib25), [26](https://arxiv.org/html/2407.01067v3#bib.bib26), [27](https://arxiv.org/html/2407.01067v3#bib.bib27)].
While these endeavors have led to notable advancements [[27](https://arxiv.org/html/2407.01067v3#bib.bib27), [28](https://arxiv.org/html/2407.01067v3#bib.bib28), [29](https://arxiv.org/html/2407.01067v3#bib.bib29), [30](https://arxiv.org/html/2407.01067v3#bib.bib30)], including some evidence of human-like representations emerging from self-supervised learning [[31](https://arxiv.org/html/2407.01067v3#bib.bib31), [32](https://arxiv.org/html/2407.01067v3#bib.bib32), [33](https://arxiv.org/html/2407.01067v3#bib.bib33), [34](https://arxiv.org/html/2407.01067v3#bib.bib34)], a critical question remains: to what extent can complex, task-general psychological representations emerge without explicit task-specific training, and how do these compare to human cognitive processes across a broad range of tasks and domains?
LLMs, such as OpenAI‚Äôs ChatGPT and Google‚Äôs Gemini, have emerged as potent tools in text and image understanding, generation, and reasoning. These models exhibit impressive capabilities in tasks like object identification, information categorization, concept communication, and inference. Unlike task-specific small-scale neural network models, LLMs utilize generic neural network architectures with billions of parameters, trained through next token prediction on massive text corpora (and images for MLLMs) comprising trillions of tokens. Despite ongoing debates about their capacities [[35](https://arxiv.org/html/2407.01067v3#bib.bib35), [36](https://arxiv.org/html/2407.01067v3#bib.bib36), [37](https://arxiv.org/html/2407.01067v3#bib.bib37)], one potential strength lies in their adeptness at problem-solving with minimal task-specific training, often requiring only straightforward task instructions without parameter updates. These features raised the question of whether LLMs have developed human-like conceptual representations about natural objects.

In this study, we used a data-driven approach to explore the core dimensions of mental representations in LLM (ChatGPT-3.5) and MLLM (Gemini Pro Vision 1.0). Inspired by previous work conducted on human similarity judgments using visual object images, we adopted a similar methodology to both the LLM and MLLM. Unlike presenting visual stimuli to human participants and MLLMs, we presented corresponding textual descriptions of visual images to the LLMs. Harnessing the models‚Äô ability to perform a triplet odd-one-out task, a well-established paradigm in cognitive psychology [[16](https://arxiv.org/html/2407.01067v3#bib.bib16), [38](https://arxiv.org/html/2407.01067v3#bib.bib38), [17](https://arxiv.org/html/2407.01067v3#bib.bib17), [10](https://arxiv.org/html/2407.01067v3#bib.bib10)], we collected extensive datasets comprising 4.7 million triplet similarity judgments for both the LLM and MLLM. Each dataset is rich in triple similarity judgment entries, drawn from a pool of 1,854 unique objects. This diverse collection enables the examination and capture of visual and conceptual mental representations spanning a wide array of natural objects.

Using a representation learning method previously designed for human participants [[39](https://arxiv.org/html/2407.01067v3#bib.bib39), [16](https://arxiv.org/html/2407.01067v3#bib.bib16)], we identified 66 sparse, non-negative dimensions underlying LLMs‚Äô similarity judgments that lead to excellent predictions of both single-trial behavior and similarity scores between pairs of objects. We demonstrated that these dimensions are interpretable, exhibited spontaneous semantic clustering, and characterized the large-scale structure of LLMs‚Äô mental representations of natural objects. Furthermore, by comparing the identified dimensions with the core dimensions observed in human cognition, we found close alignment between model and human embeddings. Finally, we found strong correspondence between the model embeddings and neural activity patterns in category-selective brain Region of Interests (ROIs, e.g., EBA, PPA, RSC, FFA), underscoring the generalization of these learned mental representations and offering a compelling evidence that the object representations in LLMs, while not identical to those in the human, share fundamental commonalities that reflect key schemas of human conceptual knowledge. These results enrich the growing body of work characterizing the emergent characteristics of LLMs [[40](https://arxiv.org/html/2407.01067v3#bib.bib40), [41](https://arxiv.org/html/2407.01067v3#bib.bib41), [42](https://arxiv.org/html/2407.01067v3#bib.bib42), [43](https://arxiv.org/html/2407.01067v3#bib.bib43), [44](https://arxiv.org/html/2407.01067v3#bib.bib44), [45](https://arxiv.org/html/2407.01067v3#bib.bib45), [46](https://arxiv.org/html/2407.01067v3#bib.bib46), [47](https://arxiv.org/html/2407.01067v3#bib.bib47), [48](https://arxiv.org/html/2407.01067v3#bib.bib48), [49](https://arxiv.org/html/2407.01067v3#bib.bib49)], showcasing their potential to capture and reflect human-like conceptualizations of real-world objects.



Figure 1: Schematic diagrams of the experiment and analysis methods. a, THINGS database and examples of object image with their language descriptions at the bottom. b-d, Pipelines of mental embedding learning under the triplet odd-one-out paradigm for LLM, MLLM, and humans, respectively. Odd-one-out judgments were collected for approximately 4.7 million triplets, and modeled using the SPoSE approach to derive the corresponding low-dimensional embedding. e, Examples of prompts and responses for LLM and MLLM. f, Illustration of the SPoSE modeling approach. g, Illustration of the NSD dataset with dimension ratings for stimulus images. The schematic structure incorporates elements adapted from Figure 1A of Horikawa et al. (2020)[[54](https://arxiv.org/html/2407.01067v3#bib.bib54)] (https://doi.org/10.1016/j.isci.2020.101060), published under a CC BY 4.0 license. h, Overview of the comparisons between space of LLMs, human behavior and brain activity. For this figure, all images were replaced by images with similar appearance from the public domain. Images used under a CC0 license, from Pixabay and Pexels.

Results
-------

We initiated our study by selecting a diverse set of objects from the THINGS database [[50](https://arxiv.org/html/2407.01067v3#bib.bib50)], encompassing 1,854 common objects (Fig. [1](https://arxiv.org/html/2407.01067v3#Sx1.F1 "Figure 1 ‚Ä£ Introduction ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a). To compare LLMs‚Äô mental representations with humans, we adopted the triplet odd-one-out task, effective for modeling human mental dimensions [[16](https://arxiv.org/html/2407.01067v3#bib.bib16), [38](https://arxiv.org/html/2407.01067v3#bib.bib38), [17](https://arxiv.org/html/2407.01067v3#bib.bib17), [10](https://arxiv.org/html/2407.01067v3#bib.bib10), [51](https://arxiv.org/html/2407.01067v3#bib.bib51)] (Figs. [1](https://arxiv.org/html/2407.01067v3#Sx1.F1 "Figure 1 ‚Ä£ Introduction ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")b-d).
Given the impracticality of conducting 1.06 billion triplet judgments, we approximated the similarity matrix using approximately 0.44%percent0.440.44\%0.44 % of the total judgments, following established methods [[16](https://arxiv.org/html/2407.01067v3#bib.bib16), [17](https://arxiv.org/html/2407.01067v3#bib.bib17)]. Human similarity judgments were collected from 4.7 million trials via Amazon Mechanical Turk [[17](https://arxiv.org/html/2407.01067v3#bib.bib17)], and LLMs‚Äô behavioral data mirrored these trials. Fig. [1](https://arxiv.org/html/2407.01067v3#Sx1.F1 "Figure 1 ‚Ä£ Introduction ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")e displays examples of prompts and responses from GPT-3.5-Turbo and Gemini Pro Vision, detailing choice derivation.
We utilized the Sparse Positive Similarity Embedding (SPoSE) method [[39](https://arxiv.org/html/2407.01067v3#bib.bib39), [16](https://arxiv.org/html/2407.01067v3#bib.bib16)] (Fig. [1](https://arxiv.org/html/2407.01067v3#Sx1.F1 "Figure 1 ‚Ä£ Introduction ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")f) to infer LLMs‚Äô low-dimensional representations, optimizing object weights to predict behavioral judgments. We validated the generalization of LLM embeddings on the Natural Scenes Dataset (NSD) [[52](https://arxiv.org/html/2407.01067v3#bib.bib52)] and applied Representational Similarity Analysis (RSA) [[53](https://arxiv.org/html/2407.01067v3#bib.bib53)] to assess correlations with neural activity (Figs. [1](https://arxiv.org/html/2407.01067v3#Sx1.F1 "Figure 1 ‚Ä£ Introduction ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")g-h).

Low-dimensional embeddings identified from LLMs are stable and predictive

Given the stochastic nature of SPoSE modeling (see Methods), we conducted multiple reruns with different random initializations, yielding slightly varied embeddings. Dimensions were sorted by their total object weights, and redundant dimensions (correlation > 0.4) were pruned, retaining only one. This reduced redundancy, as most dimensions appeared consistently across runs. To evaluate retained dimensions, we gathered triplet judgments for 48 typical objects (these triplet judgments are not included in the SPoSE model‚Äôs training data), comparing choice probabilities with predictions from the SPoSE embedding. Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a shows that predictive performance stabilizes as dimensions increase, saturating at 60 dimensions for LLM, MLLM, and human. We chose the top 66 dimensions for LLM and MLLM to align with the 66 core dimensions from human similarity judgments [[17](https://arxiv.org/html/2407.01067v3#bib.bib17)], as dimensions beyond the 66th contribute minimally to object similarity prediction.

Figs. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")b-d illustrate strong correlations between the model-predicted and behaviorally-measured Representational Similarity Matrices (RSMs) for LLM (0.71), MLLM (0.85), and human (0.9), validating the close reflection of behavioral similarity space. This result shows that, despite the complex object pool, a low-dimensional embedding can capture a large portion of the representational structure derived from similarity judgments.



Figure 2: Validation of the embeddings derived from similarity judgments over 4.7 million trials. a, Prediction performance of the measured similarity matrix with varying dimensions of the SPoSE embedding. b-d, RSMs for a subset consisting of 48 objects, created by estimating similarity based on the model embedding (left) and by fully sampling all possible triplets in a validation behavioral experiment (middle). Here, the similarity between two objects is operationalized as the proportion of times they are judged to be similar, across all trials. Correlation between the predicted and measured similarity on all object pairs were shown in right. e-g, Reproducibility of dimensions in the chosen 66-dimensional embedding. The dimensions were sorted in descending order by the sum of their weights across objects. The scores are presented as mean ¬±plus-or-minus\pm¬± 95% confidence intervals (CIs), and shaded areas reflect the 95% CIs (n=20 runs, and each dot represents the highest correlation of each selected dimension with all dimensions of a single run). h, Odd-one-out prediction performance on the model‚Äôs own held-out behavioral choice test set. Results and chance-levels are presented as mean ¬±plus-or-minus\pm¬± 95% CIs, and the error bars reflect 95% CIs (n=1000 bootstraps). The noise ceilings were estimated from the additional behavioral datasets for each model separately, and were presented as mean ¬±plus-or-minus\pm¬± 95% CIs (shaded bands). i, How closely SPoSE embeddings mimic model‚Äôs original features in odd-one-out predictions. The vertical axis represents the ratio of the SPoSE embedding accuracy to the original feature accuracy on the held-out test set constructed using cosine distances. j, How correlated are the model probing methods based on behavioral choices with those based on cosine distance. The numbers on the gray arrows represent the Pearson correlation between different RSMs (of the 48 objects).

Next, we calculated reproducibility scores for each retained dimension (see Methods). In Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")e, all LLM embedding dimensions scored above 0.51, with 37 dimensions exceeding 0.90. Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")f shows that MLLM dimensions had reproducibility scores above 0.36, except one at 0.22, with 31 dimensions exceeding 0.80. Human dimensions in Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")g showed comparable reproducibility. These findings confirm that the embeddings are stable across reruns.

We also evaluated the ability of these embeddings to predict choices in the odd-one-out task using model‚Äôs own held-out behavioral choice test set. As shown in Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")h, accuracies were 56.7% (¬±plus-or-minus\pm¬±0.22%), 63.4% (¬±plus-or-minus\pm¬±0.25%), and 64.1% (¬±plus-or-minus\pm¬±0.18%) for LLM, MLLM, and human, respectively (chance = 33.3%, 95% CI = [33.19%, 33.47%], 1,000 permutation tests). Noise ceilings for fitting individual-trial behavior were 65.1% (¬±plus-or-minus\pm¬±0.96%), 73.8% (¬±plus-or-minus\pm¬±1.12%), and 67.2% (¬±plus-or-minus\pm¬±1.04%), indicating that the low-dimensional embeddings achieve up to 87.1%, 85.9%, and 95.4% of the optimal predictive accuracy for LLM, MLLM, and human, respectively.

Furthermore, we compared SPoSE embedding‚Äôs predictive performance to that of the original model features using open-source models. As shown in Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")i, the accuracy ratios demonstrate that SPoSE embeddings closely approximate the original features (with ratios around 90%), highlighting their effectiveness as compressed representations (see Extended Data Fig. 1a for the number of retained dimensions for these models and their predictive performance curves). Additionally, in Fig. [2](https://arxiv.org/html/2407.01067v3#Sx2.F2 "Figure 2 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")j, we compared two model probing methods: the behavioral judgment method and the cosine distance method. For the pure language model Llama3.1, the correlation between the two methods was relatively strong (r=0.55ùëü0.55r=0.55italic\_r = 0.55), while for the vision-language model Qwen2\_VL [[55](https://arxiv.org/html/2407.01067v3#bib.bib55)](7B version), it was lower (r=0.38ùëü0.38r=0.38italic\_r = 0.38). Importantly, the behavioral judgment method aligned better with human-derived RSM than the cosine distance method (0.70 vs. 0.42 for Qwen2\_VL, and 0.51 vs. 0.49 for Llama3.1). These results suggest the feasibility of using SPoSE embeddings derived from behavioral judgments to probe the closed-source LLMs/MLLMs where direct feature extraction is infeasible.

Overall, SPoSE modeling generated a low-dimensional, stable, and predictive mental embedding, excelling in predicting triplet similarity judgments and reconstructing their representational space. This indicates that LLM (particularly MLLM) judgments of natural objects are structured and principled. In the following sections, we explore key schemas in this embedding and their connections to human mental representations.

Emergent object category information



Figure 3: Emerging object category information in the derived embeddings. a, Categorization performance of different embeddings, tested on 18 categories in the THINGS database. Chance-levels are presented as mean ¬±plus-or-minus\pm¬± 95% CIs, and the error bars reflect 95% CIs (n=1000 bootstraps). b, Categorization performance comparisons between the SPoSE embedding and original model feature. c-d, t-SNE visualization of 1,854 objects, showing emergent category clusters in the learned embedding space of human and models. Dots correspond to objects, and were colored according to their labels.

Natural object categories emerge from mental embeddings derived from human similarity judgments [[16](https://arxiv.org/html/2407.01067v3#bib.bib16), [38](https://arxiv.org/html/2407.01067v3#bib.bib38)]. To assess whether embeddings from LLM and MLLM also show emergent category structures, we used 18 high-level categories from the THINGS database [[50](https://arxiv.org/html/2407.01067v3#bib.bib50)] and applied a cross-validated nearest-centroid classifier to predict the category membership for each of the 1,112 objects of these categories (see Methods).

As seen in Fig. [3](https://arxiv.org/html/2407.01067v3#Sx2.F3 "Figure 3 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a, LLM embeddings achieved 83.4% top-1 accuracy (chance = 9.8%, 95% CI = [8.2%, 11.4%]), while MLLM reached 78.3% (chance = 9.9%, 95% CI = [8.2%, 11.5%]). Human embeddings performed best with 87.1% top-1 accuracy (chance = 10.3%, 95% CI = [8.6%, 12.0%]). Fig. [3](https://arxiv.org/html/2407.01067v3#Sx2.F3 "Figure 3 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")b shows similar categorization performance between SPoSE embeddings and original features across models, confirming SPoSE‚Äôs effectiveness in capturing object categories if the model itself is powerful in object representation [[24](https://arxiv.org/html/2407.01067v3#bib.bib24)].
Figs. [3](https://arxiv.org/html/2407.01067v3#Sx2.F3 "Figure 3 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")c-d visualizes the global structure of embeddings via a t-SNE plot (dual perplexity: 5 and 30; 1,000 iterations) initialized with multidimensional scaling (MDS). Objects with similar values cluster together, showing that items from the same category group across LLM, MLLM, and human data. Thus, LLMs inherently capture object category structures without explicit representational constraints. Compared to traditional supervised models (like VGG16 [[56](https://arxiv.org/html/2407.01067v3#bib.bib56)]) or self-supervised models (like SimCLR [[57](https://arxiv.org/html/2407.01067v3#bib.bib57)]), LLMs and humans exhibit superior object category information.
Overall, LLM and MLLM results support known distinctions between animate/inanimate and man-made/natural objects, consistent with previous human studies [[16](https://arxiv.org/html/2407.01067v3#bib.bib16)].

The embedding dimensions of the LLMs are interpretable and informative

While past research has explored multidimensional mental representations in humans [[16](https://arxiv.org/html/2407.01067v3#bib.bib16), [17](https://arxiv.org/html/2407.01067v3#bib.bib17)], this study is the first to examine LLMs. We focused on analyzing these dimensions to identify properties prioritized by LLM and MLLM when assessing object similarity.
Figs. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a-d visually represent selected dimensions in LLM and MLLM by showing object images weighted most heavily in those dimensions. These dimensions are interpretable, reflecting conceptual and perceptual traits. We assigned intuitive labels (e.g., "animal-related" and "food-related"; see Methods) to dimensions from LLM and MLLM. Some dimensions appear to represent semantic categories (e.g., food, animals, vehicles) (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a), while others capture perceptual features like hardness, value, temperature, or texture (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")b). Certain MLLM dimensions seem to reflect global spatial properties (e.g., crowded) (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")c), while some convey shape (flatness, elongation) and color (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")d). Dimensions also distinguish user specificity (children vs. adults, everyday consumers vs. experts) (Extended Data Fig. 1b), physical composition (wood, ceramic, metal) (Extended Data Fig. 1c), and environment-related traits (land vs. sea, indoor vs. outdoor) (Extended Data Fig. 1d). See Extended Data Figs. 2-6 for a visual display of all 66 dimensions. Each dimension in LLM or MLLM embodies multiple attributes, but we offer a single interpretation per dimension to showcase the concepts they represent.

We categorized the dimensions into three groups: shared across all three (LLM, MLLM, human), unique to human, and missing from human but present in LLM/MLLM. Shared dimensions include "animal-related" (2, 3), "food-related" (2, 3, 6, 18, 41, 58), "electronics/technology" (5, 11), "transportation/movement" (8, 19, 52, 58), and more.
Unique human dimensions include "white" (22), "red" (24), "black" (27), "tubular" (31), "grid/grating-related" (33), "spherical/voluminous" (36), "elliptical/curved" (41), and more.
Dimensions missing in humans but present in LLM/MLLM include "vegetable-related" (13, 28),
"frozen treats/drink" (22),
"presentation/display-related" (23),
"headwear-related" (25),
"livestock-related" (26),
and more.
In general, categories such as animals, food, and technology are universally recognized across humans, LLMs, and MLLMs, indicating a common conceptual basis. Humans excel at distinguishing object differences through perceptual features like color, shape, and texture, which are less pronounced in LLM and MLLM. Moreover, LLM and MLLM tend to form more specific categories (e.g., fruits, vegetables, headwear) than humans‚Äô broader categorizations. The absence of certain dimensions in human representations does not imply an inability to perceive them; rather, these dimensions may emerge at a higher level, such as humans consolidating "vegetable-related" and "nut-related" dimensions under a "food-related" dimension.



Figure 4: Object dimensions illustrating their interpretability. a-d, For each dimension, visualization includes the top 6 images carrying the greatest weights, accompanied by a word cloud reflecting human‚Äôs annotations for what is captured by the dimension. For LLM, we replaced linguistic descriptions with images of the related objects to aid visualization. e, Proportions of visual, semantic, and mixed visual-semantic dimensions. f, Proportions of easy and hard to interpret. g, Illustration of example objects with their dominant dimensions. h, To explain 95 to 99% of the predictive performance in behavior, how many dimensions are required. For subfigures a-d, g, all images were replaced by images with similar appearance from the public domain. Images used under a CC0 license, from Pixabay and Pexels.

The dimensions derived from LLM and MLLM appear to exhibit a degree of interpretability, as evidenced by the ability to assign intuitive labels to them. These labels were listed in Extended Data Table 1. We also annotated these dimensions using MLLM, comparing human-generated vs. MLLM-generated labels in Extended Data Table 2.
In addition, we divided all dimensions into visual, semantic, and mixed visual-semantic groups (based on examination by human experts) and calculated the proportion for each group (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")e). LLM and MLLM have more semantic dimensions, while humans are better at using visual information. In contrast, the purely vision model SimCLR (a self-supervised learning model) shows minimal ability to learn semantic dimensions (Extended Data Fig. 7), whereas the dimensions derived from random representations lack any interpretability (Supplementary Fig. 1). We also categorized dimensions by ease of interpretation (based on whether they can be clearly explained by a single label), finding that most dimensions are easy to interpret (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")f). Specifically, 60/66 dimensions for LLM, 57/66 for MLLM, and 62/66 for humans are easy to interpret, with humans having the fewest hard-to-interpret dimensions.

We examined the composition of dimensions for specific objects. Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")g uses circular bar plots to represent objects, where petal angle and color denote dimensions, and length indicates the dimension‚Äôs importance. For example, "almond" is primarily food-related, while "satellite" is associated with electronics and flying. These plots also demonstrate that objects are indeed characterized by a rather small number of dimensions, indicating that not all 66 dimensions are necessary for particular similarity judgment.
To quantify this, we progressively eliminated less significant dimensions for each object and assessed model performance. We found that retaining 3 to 8 dimensions for LLM, 2 to 10 for MLLM, and 7 to 13 for humans suffices to achieve 95-99% of the full model‚Äôs performance in explaining behavioral judgments within the odd-one-out context (Fig. [4](https://arxiv.org/html/2407.01067v3#Sx2.F4 "Figure 4 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")h). LLM exhibits lower dimensionality than humans, likely due to its lack of visual input. Although MLLM can access visual data, its multimodal integration remains inferior to human capabilities, limiting dimensions related to shape or color, inherently tied to human visual experience.

Comparison between models and humans

We employed two approaches to assess model-human alignment: one measuring consistency in similarity judgments [[58](https://arxiv.org/html/2407.01067v3#bib.bib58)] and the other analyzing core dimension relationships.

Using comprehensive triplet sampling on 48 objects, we estimated similarity via choice probabilities and correlated model and human similarity matrices with Pearson correlation. Fig. [5](https://arxiv.org/html/2407.01067v3#Sx2.F5 "Figure 5 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a compares various models, including visual-only, visual-language, LLMs, MLLMs, and a Gabor baseline, revealing higher human-consistency for LLM and MLLM.
A preliminary comparison between ChatGPT-3.5 and GPT-4 in Fig. [5](https://arxiv.org/html/2407.01067v3#Sx2.F5 "Figure 5 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")b, directly based on their choice consistency with human on 2,171 triplets, shows that notable differences remain between LLMs and human. To delve deeper into the reasons behind these differences, we show in Fig. [5](https://arxiv.org/html/2407.01067v3#Sx2.F5 "Figure 5 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")c the most relevant dimensions that humans and models rely on to make choices (see Methods). We see that human and models make different choices because of the differently key dimensions they rely on. For example, human can make choice based on color (like "red"), while LLM only makes choice based on semantics (like "protective"). More examples are in Extended Data Fig. 1f.

Next, we explored the relationship between the core dimensions of LLMs and humans, as shown in Fig. [5](https://arxiv.org/html/2407.01067v3#Sx2.F5 "Figure 5 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")d. The matrices are generally sparse, indicating that a dimension in one system strongly correlates with only a few dimensions in the other. Many dimensions even show a strong one-to-one mapping. Quantitatively, 31 out of the 66 LLM dimensions and 42 out of the 66 MLLM dimensions strongly correlate with human dimensions (r>0.4ùëü0.4r>0.4italic\_r > 0.4), indicating substantial alignment. In MLLM, several human dimensions are subdivided (e.g., human dim. 18 "fluid-related" splitting into MLLM dims. 18 "container" and 22 "fluid-related") or amalgamated (e.g., human dims. 3 "animal-related" and 40 "disgusting" merging into MLLM dim. 34 "insect-related"). Similarly, LLM shows adaptations, particularly in semantics, though it lacks sensory dimensions like color or shape. For example, LLM distinguishes between dim. 22 "frozen treats" and dim. 57 "hot drinks" (or dim. 2 "wild animals" vs. dim. 26 "livestock," dim. 13 "vegetables" vs. dim. 18 "fruits," etc.). While MLLM still lacks specific color-related dimensions (e.g., "red," "black"), it aligns more closely with humans, especially in dimensions like shape (e.g., dim. 35 "grainy," dim. 64 "round/curvature") and spatial features (e.g., dim. 8 "serried/stacked," dim. 44 "dense/many small things"). This shows that MLLM, like humans, can perceive a large amount of visual information. Quantitatively, Fig. [5](https://arxiv.org/html/2407.01067v3#Sx2.F5 "Figure 5 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")e shows the number of shared and unique dimensions (r>0.2ùëü0.2r>0.2italic\_r > 0.2) between models and humans, where 38 of 66 dimensions being shared across the three systems.



Figure 5: Comparison between models and humans. a, Human-model consistency (Pearson‚Äôs rùëüritalic\_r) between human and model object similarity matrices. Left blue bar shows baseline between-human consistency. Data are presented as mean ¬±plus-or-minus\pm¬± 95% CIs, and the error bars reflect 95% CIs (n=1000 bootstraps). b, Preliminary comparison between ChatGPT-3.5 and GPT-4. The error bars reflect standard deviation (SD), and data are presented as mean ¬±plus-or-minus\pm¬± SD (n=5 samplings, and dots represent the result of each time). c, Key dimensions that underpin specific behavioral choices made by human and models. d, Cross-correlation matrix between each pair of model systems (human-LLM, human-MLLM, and LLM-MLLM (in Extended Data Fig. 1e)). e, Quantification of shared (r>0.2ùëü0.2r>0.2italic\_r > 0.2) and non-shared dimensions between different systems. For subfigure c, all images were replaced by images with similar appearance from the public domain. Images used under a CC0 license, from Pixabay and Pexels.



Figure 6: Relationship to the cerebral representational geometries. a, Searchlight brain RSM and the varied model RSMs on the NSD shared\_1k dataset. b, RSA between model RSM and brain ROI RSM constructed from the SPoSE embedding of that brain ROI (see Methods). The error bars reflect SD, and data are presented as mean ¬±plus-or-minus\pm¬± SD (n=4 subjects, and dots represent the scores of different individuals). c-d, Cortical maps of searchlight RSA and voxel-wise encoding (evaluated by using R2superscriptùëÖ2R^{2}italic\_R start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT with noise ceiling normalization). For visualization purpose, we only conducted noise ceiling normalization for voxels that have the predicted R2>0.2superscriptùëÖ20.2R^{2}>0.2italic\_R start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT > 0.2. e, 2-D histograms of human, LLM and MLLM performance in R2superscriptùëÖ2R^{2}italic\_R start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT against noise ceiling across all voxels in the whole brain. f, 2-D histograms of LLM, MLLM against human performance.

Relationship to the cerebral representational geometries

To link LLMs‚Äô embeddings with brain responses, we applied searchlight RSA [[53](https://arxiv.org/html/2407.01067v3#bib.bib53)] (see Fig. [6](https://arxiv.org/html/2407.01067v3#Sx2.F6 "Figure 6 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")a) using fMRI data from the NSD dataset [[52](https://arxiv.org/html/2407.01067v3#bib.bib52)]. Independent dimension rating models were fitted for each dimension, and these models predicted multi-dimensional embeddings for objects, creating a representational geometry. We then compared this predicted RSM to SPoSE embedding RSMs of brain ROIs and searchlight RSMs of brain sectors to gauge how well the LLM‚Äôs embedding aligns with brain regions.

The representational similarity scores for each model and brain ROI are depicted in Fig. [6](https://arxiv.org/html/2407.01067v3#Sx2.F6 "Figure 6 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")b. It should be noted that we adopted the SPoSE method to infer low-dimensional embeddings for CLIP [[59](https://arxiv.org/html/2407.01067v3#bib.bib59)] (here used as a strong baseline [[60](https://arxiv.org/html/2407.01067v3#bib.bib60)]) and brain ROIs, using cosine distance as a metric to construct the desired odd-one-out records.
Human and MLLM embeddings outperform LLM and CLIP, particularly in functionally defined, category-selective ROIs (e.g., EBA, PPA, RSC, FFA). However, ROI-based analysis may miss fine-grained spatial patterns, as similar scores can conceal spatial differences.

Figs. [6](https://arxiv.org/html/2407.01067v3#Sx2.F6 "Figure 6 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")c&d display fine-grained cortical maps of human, LLM, and MLLM embeddings using searchlight RSA and voxel-wise encoding (see Methods) for subject S1, highlighting only significant voxels (P<0.05ùëÉ0.05P<0.05italic\_P < 0.05, FDR-corrected). Additional models and subjects are shown in Extended Data Fig. 8a. Visual inspection shows MLLM and human embeddings align more closely with most of the brain regions than LLM and CLIP, and the contrast of local details can also be clearly viewed. This performance difference is most obvious under searchlight RSA, and relatively moderate in voxel-wise encoding.
Beyond overall performance metric, peaks in the cortical maps align with scene-selective [[61](https://arxiv.org/html/2407.01067v3#bib.bib61)] (PPA, RSC, OPA), body-selective [[62](https://arxiv.org/html/2407.01067v3#bib.bib62)] (EBA) and face-selective [[63](https://arxiv.org/html/2407.01067v3#bib.bib63), [64](https://arxiv.org/html/2407.01067v3#bib.bib64)] (FFA, OFA) ROIs, suggesting MLLM captures semantic relationships similar to human cognition. Furthermore, both the overall performance levels and the pattern consistency remain stable across multiple subjects (Extended Data Fig. 8a). Voxel-wise encoding results based on the original CLIP embedding and its low-dimensional SPoSE embedding (Extended Data Fig. 8b) also provide strong evidence that SPoSE is an effective intrinsic dimension learning method. Fig. [6](https://arxiv.org/html/2407.01067v3#Sx2.F6 "Figure 6 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")e presents 2-D histograms of human, LLM and MLLM performance in R2superscriptùëÖ2R^{2}italic\_R start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT against noise ceiling across all voxels. For human and MLLM, most voxels in the category-selective ROIs (e.g., EBA, PPA, RSC, FFA) are predicted close to their 85% noise ceiling, while LLM is slightly worse.
Fig. [6](https://arxiv.org/html/2407.01067v3#Sx2.F6 "Figure 6 ‚Ä£ Results ‚Ä£ Human-like object concept representations emerge naturally in multimodal large language models")f presents 2D histograms comparing LLM and MLLM to human performance across whole brain voxels. LLM and MLLM achieve about 60% and 85% of human performance under searchlight RSA, respectively. In voxel-wise encoding, LLM reaches 90% of human performance, while MLLM nearly matches human levels.

Discussion
----------

The present study comprehensively investigates object concept representations in LLMs and MLLMs, and their relationship to human cognition and brain representations. We collected 4.7 million behavioral judgments to derive 66 stable dimensions predicting object similarity, uncovering semantic clustering in both LLM and MLLM embeddings, resembling human mental structures. Despite differing architectures, these models developed conceptual representations similar to humans, supported by interpretable dimensions reflecting core aspects of object understanding. MLLM, which integrates visual and linguistic data, predicted individual choices at 85.9% of the noise ceiling, consistent with findings that multimodal learning enhances representation robustness and generalizability [[65](https://arxiv.org/html/2407.01067v3#bib.bib65), [66](https://arxiv.org/html/2407.01067v3#bib.bib66), [67](https://arxiv.org/html/2407.01067v3#bib.bib67)]. Moreover, the strong alignment between MLLM embeddings and neural activity in regions like EBA, PPA, RSC, and FFA suggests that MLLM representations share similarities with human conceptual knowledge [[68](https://arxiv.org/html/2407.01067v3#bib.bib68)].

Broad applications of the derived embeddings

The low-dimensional mental embeddings identified in this study can be used in human-machine representation alignment and fusion, potentially enhancing human-machine interfaces and collaborative systems by revealing shared object representation schemas. Practically, these interpretable dimensions could inform the development of more human-like artificial cognitive systems, improving their natural interaction with humans [[69](https://arxiv.org/html/2407.01067v3#bib.bib69)]. To better align LLM and MLLM with human reasoning in the odd-one-out task, we can explore the method of guiding model attention to human-preferred dimensions. By tailoring prompts to emphasize specific attributes (e.g., "red" or "artificial"), we believe that models could make choices more consistent with human judgments (i.e., explicit guidance can help bridge the gap between model and human reasoning; Supplementary Figs. 2-4).
Moreover, the collected extensive machine behavioral datasets offer a valuable benchmark for evaluating AI model representations.

Relationship to the other related studies

Both the human brain and large-scale AI models are complex systems, typically analyzed through dimensionality reduction. Recent hypotheses like the "low-rank" [[70](https://arxiv.org/html/2407.01067v3#bib.bib70)] and "distributed information bottleneck" [[71](https://arxiv.org/html/2407.01067v3#bib.bib71)] propose solutions to identifying optimal latent dimensions. Our findings align with these concepts, demonstrating that LLMs can develop human-like object representations using fundamental dimensions, akin to the brain‚Äôs capacity to derive rich conceptual knowledge from simple neural mechanisms. Exploring these low-dimensional structures could deepen our understanding of cognition in both biological and artificial systems.

The similarity between LLMs and human representations, despite differing input modalities, suggests a convergence beyond data covariance. This is consistent with findings on innate semantic transformations in the visual system [[72](https://arxiv.org/html/2407.01067v3#bib.bib72)], and is further supported by the interpretability of LLMs‚Äô embeddings, reflecting fundamental semantic structures. Prior studies [[73](https://arxiv.org/html/2407.01067v3#bib.bib73), [74](https://arxiv.org/html/2407.01067v3#bib.bib74), [75](https://arxiv.org/html/2407.01067v3#bib.bib75)] demonstrate that artificial models can predict visual brain activity, which aligns with our results showing model-neural correlations in higher cortical regions. These findings suggest LLMs develop representations that capture key aspects of human conceptual knowledge [[76](https://arxiv.org/html/2407.01067v3#bib.bib76), [77](https://arxiv.org/html/2407.01067v3#bib.bib77)], further highlighting the natural alignment between language and vision [[78](https://arxiv.org/html/2407.01067v3#bib.bib78), [79](https://arxiv.org/html/2407.01067v3#bib.bib79)].
Previous fMRI studies have revealed diverse organizational principles in the brain for processing external stimuli. The primary visual cortex exhibits retinotopy through eccentricity and angle selectivity [[80](https://arxiv.org/html/2407.01067v3#bib.bib80), [81](https://arxiv.org/html/2407.01067v3#bib.bib81)]. These principles of dimensional organization extend to higher-order information [[82](https://arxiv.org/html/2407.01067v3#bib.bib82), [83](https://arxiv.org/html/2407.01067v3#bib.bib83), [84](https://arxiv.org/html/2407.01067v3#bib.bib84), [85](https://arxiv.org/html/2407.01067v3#bib.bib85), [86](https://arxiv.org/html/2407.01067v3#bib.bib86), [87](https://arxiv.org/html/2407.01067v3#bib.bib87), [88](https://arxiv.org/html/2407.01067v3#bib.bib88)]. Our study expands this research to the conceptual representations of natural objects.

Traditionally, neural network representations are analyzed by examining neuron activation patterns [[89](https://arxiv.org/html/2407.01067v3#bib.bib89), [90](https://arxiv.org/html/2407.01067v3#bib.bib90), [91](https://arxiv.org/html/2407.01067v3#bib.bib91), [92](https://arxiv.org/html/2407.01067v3#bib.bib92)]. However, as AI systems grow in complexity, neuron-level approaches become less effective. Instead, inspired by cognitive psychology, behavioral methods can infer AI system representations through actions. Decades of research have developed techniques to elucidate mental representations from human behavior [[93](https://arxiv.org/html/2407.01067v3#bib.bib93), [16](https://arxiv.org/html/2407.01067v3#bib.bib16)]. Our study adopts this behavioral approach for LLMs, complementing existing neuron-level methods.
Probing LLMs from a cognitive perspective has gained attention [[94](https://arxiv.org/html/2407.01067v3#bib.bib94), [95](https://arxiv.org/html/2407.01067v3#bib.bib95), [35](https://arxiv.org/html/2407.01067v3#bib.bib35), [96](https://arxiv.org/html/2407.01067v3#bib.bib96), [97](https://arxiv.org/html/2407.01067v3#bib.bib97), [98](https://arxiv.org/html/2407.01067v3#bib.bib98)], revealing insights into areas like color processing [[99](https://arxiv.org/html/2407.01067v3#bib.bib99)], emotion analysis [[100](https://arxiv.org/html/2407.01067v3#bib.bib100), [101](https://arxiv.org/html/2407.01067v3#bib.bib101)], memory [[102](https://arxiv.org/html/2407.01067v3#bib.bib102), [103](https://arxiv.org/html/2407.01067v3#bib.bib103)], morality [[104](https://arxiv.org/html/2407.01067v3#bib.bib104)], and decision-making [[40](https://arxiv.org/html/2407.01067v3#bib.bib40), [105](https://arxiv.org/html/2407.01067v3#bib.bib105), [106](https://arxiv.org/html/2407.01067v3#bib.bib106)]. Understanding the parallels between human cognition and LLMs offers exciting opportunities to explore the intersections of AI and cognitive science [[37](https://arxiv.org/html/2407.01067v3#bib.bib37), [69](https://arxiv.org/html/2407.01067v3#bib.bib69)].

Limitations and future directions

One potential limitation of this study is its focus on ChatGPT-3.5 and Gemini Pro Vision (v1.0), which may not encompass the full spectrum of models. However, the methodology is extendable to other state-of-the-art LLMs such as GPT-4V [[107](https://arxiv.org/html/2407.01067v3#bib.bib107)]. This extension could reveal the generalization of identified dimensions and highlight the unique aspects of different AI architectures. Another potential limitation is that the impact of varying language prompts on LLMs‚Äô responses. In this study, the language prompts we used were carefully designed to ensure that the LLMs understand the task instructions correctly. We think that these considerations have a negligible impact on the study‚Äôs overall conclusions. Moreover, we only employed object-level annotations in the language prompts of LLM. Object-level annotations focus on abstract categories, while image-level annotations (generated by a vision-language model or human annotators) can capture more image-specific visual attributes like color and texture (Supplementary Fig. 5). Using the image-level annotations will make LLM more consistent with human judgments (this can be confirmed in the MLLM probing experiments, which is equivalent to using image-level annotation in essence), highlighting the importance of visual information in similarity judgments (Supplementary Figs. 6-8).

Future work could leverage instruction fine-tuning for LLM/MLLM on large-scale triplet odd-one-out question-answer pairs, where answers include both human choices and the underlying reasoning dimensions, to improve model-human alignment.

Methods
-------

Stimuli and triplet odd-one-out task.
In selecting stimulus objects, our preference was for the THINGS database [[50](https://arxiv.org/html/2407.01067v3#bib.bib50)], a resource designed to encompass 1,854 living and non-living objects based on their practical usage in daily life. During the triplet odd-one-out task, participants (humans or LLMs) encountered three objects drawn from the THINGS database, either through images or textual descriptions. Their objective was to identify the object with the highest dissimilarity among the three. This task evaluates the relationship between two objects considering the context set by a third object. Featuring a diverse range of objects, this method provides a systematic means to assess perceived similarity unaffected by context, thus minimizing response bias. Moreover, it enables the measurement of context-dependent similarity, such as by restricting similarity evaluations to specific higher-level categories like animals or vehicles.

Behavioral responses from humans.
The human behavioral dataset utilized in our research originated from a recent study [[17](https://arxiv.org/html/2407.01067v3#bib.bib17)], where 5,517,400 human similarity judgments were collected via Amazon Mechanical Turk. After quality control‚Äìwhich excluded 818,240 trials (14.83%) based on overly fast responses (>25% trials <800ms and >50% <1,100ms), repetitive patterns (outside central 95% distribution in ‚â•\geq‚â•200 trials), and inconsistent demographic reporting (>3 ages provided)‚Äìthe final dataset comprised 4,699,160 valid trials from 12,340 participants. Participants (6,619 female; 4,400 male; 56 other/unspecified; mean age = 36.71 years, SD = 11.87; 41.9% unreported age) were right-handed with normal/corrected vision, compensated at $0.10 per 20 trials. The protocol, approved by the NIH Institutional Review Board (93-M-0170) and NIH Office of Human Research Subject Protection, obtained informed consent. While self-selection bias (tech-savvy English-speakers) and handedness exclusion may limit generalizability, the focus on relative similarity judgments‚Äìdemonstrated robust across demographics [[16](https://arxiv.org/html/2407.01067v3#bib.bib16)]‚Äìreduces population-specific effects.

Collecting behavioral responses from LLM.
For our study, we gathered all human-used similarity judgments, totaling 4.7 million trials. To solicit responses from ChatGPT-3.5 (gpt-3.5-turbo), Llama3.1 (Meta-Llama-3.1-8B-Instruct), and GPT-4 (gpt-4-0314), we employed a prompt where each image was represented by its object name and descriptions, as image input processing was not supported by these models. These text descriptions are sourced from definitions of object names in WordNet, Google, or Wikipedia, and have been compiled and made publicly available at <https://osf.io/jum2f/>. For model comparison, Llama3.1 was used to collect the full sampling of triplets (91,568 trials) of the 48 typical objects. Due to cost constraints, GPT-4 only amassed a total of 2,171 trials, primarily for initial comparisons with ChatGPT-3.5.

The prompt structure used was standardized:
"*Given a triplet of objects {[Object\_A], [Object\_B], [Object\_C]}, which one in the triplet is the odd-one-out? Please give the answer first and then explain in detail.*" In practice, [Object\_A], [Object\_B], and [Object\_C] were replaced with the respective object descriptions for each trial. The temperature parameter, dictating response randomness in LLMs, was set to 0.01.
Because of the well-structured nature of the model‚Äôs responses, we parsed the model choice from the first sentence of their response using string matching.
To assess the upper limit of predictability under dataset randomness (the noise ceiling), we randomly selected 1,000 triplets and conducted a minimum of 14 trials and a maximum of 25 trials for each using the same prompt, evaluating consistency in choices across trials.

Collecting behavioral responses from MLLM. Regarding collecting behavioral responses from Gemini Pro Vision (v1.0), we adopted a similar strategy. The prompt we used is as follows:
"*You are shown three object images side by side and are asked to report the image that was the least similar to the other two. You should focus your judgment on the object, but you are not given additional constraints as to the strategy you should use. If you did not recognize the object, you should base your judgment on your best guess of what the object could be. 1. Tell me your answer. 2. Tell me the location of the object you have chosen. 3. Explain the reasons.*" In some trials, the Gemini Pro Vision model refused to respond because it believed that the given images contained some unknown sensitive information. In this case, we applied a method akin to image replacement to address the issue.

The temperature parameter for determining response randomness in Gemini Pro Vision was also configured to 0.01, with images displayed at 512 x 512 pixels. Since the model‚Äôs responses are well structured, we extracted the keyword about the position of the object in its answers (e.g., "left," "middle," or "right") to determine the model‚Äôs choice. Similarly, to gauge the noise ceiling and potential predictability, we additionally sampled 1,000 randomly chosen triplets and ran a minimum of 14 trials and a maximum of 25 trials for each of them using the same prompt for each trial and estimated the consistency of choices for each triplet across trials.

As for the model of Qwen2\_VL-7B, we used a similar strategy to collect the full sampling of triplets for the 48 typical objects.

Constructing behavioral responses for the other models. For models do not have visual or language-based question-answer capabilities (such as CLIP, SimCLR, VGG16, etc.), we first used the pre-trained model to extract the features of the object images (or their language descriptions), and then constructed the required odd-one-out data based on the cosine distance of the features.

Feature extractors. For the pre-trained models originally used for classification tasks (such as VGG16, ResNet18, etc.), we extracted the penultimate layer features, rather than the head. For CLIP, we extract features in the final embedding layer.
For GPT2 and Llama3.1, we extracted features by averaging the last hidden state activations across all tokens to obtain sentence embeddings. For Qwen2\_VL, we extracted image features from the last layer of its visual branch, which is based on a 600M-parameter ViT.
Some of the pretrained models sourced from the following repositories: the Torchvision model zoo, the Pytorch-Image-Models (timm) library, the VISSL (self-supervised) model zoo, the OpenAI CLIP collection, and the Transformer python library.
In particular, the Gabor model feature extractor consists of a single fixed set of convolutions: 12 Gabor wavelets with spatial frequency log-spaced between 3 and 72 cyc/stimulus at 6 evenly-spaced orientations between 0 and œÄùúã\piitalic\_œÄ, following previous work [[108](https://arxiv.org/html/2407.01067v3#bib.bib108)].

Natural Scene Dataset (NSD).
NSD [[52](https://arxiv.org/html/2407.01067v3#bib.bib52)], recognized as the largest neuroimaging dataset linking brain insights with artificial intelligence, involves richly sampled fMRI data from 8 subjects. Across 30-40 MRI sessions, each subject observed between 9,000-10,000 distinct natural scenes using whole-brain gradient-echo EPI at 1.8 mm isotropic resolution and 1.6 s TR during 7T scanning. Image stimuli were drawn from the COCO dataset [[109](https://arxiv.org/html/2407.01067v3#bib.bib109)], with corresponding captions retrievable using COCO ID. To assess the generalization ability of the low-dimensional embeddings learned from humans and LLMs across datasets, the shared\_1k subset from the NSD were chosen as the test set (because the stimuli in this subset were shared by all 8 subjects). Additionally, fMRI responses linked to the shared\_1k stimuli across subjects S1, S2, S5, and S7 were earmarked for subsequent analysis (because subjects S3, S4, S6, and S8 did not complete the full fMRI data acquisition).

Sparse Positive Similarity Embedding (SPoSE).
Utilizing the SPoSE approach [[39](https://arxiv.org/html/2407.01067v3#bib.bib39), [16](https://arxiv.org/html/2407.01067v3#bib.bib16)], we derived embedding representations for 1,854 objects based on similarity judgment data from LLM and MLLM, respectively. The PyTorch implementation for this process can be accessed at <https://github.com/ViCCo-Group/SPoSE>. Initially, an embedding matrix ùêóùêó\mathbf{X}bold\_X was created with random weights in the range of 0 to 1 across 100 latent dimensions for each object, resulting in a 1854-by-100 matrix. Stochastic gradient descent was subsequently applied to fine-tune this embedding matrix using odd-one-out responses. The optimization objective function aimed to minimize a combination of cross-entropy loss concerning triplet choice probabilities for all options and an L1-norm on the weights to promote sparsity:

|  |  |  |  |
| --- | --- | --- | --- |
|  | min‚Å°‚Ñí‚Å¢(ùê±)=‚àënlog‚Å°(exp‚Å°(ùê±i‚Å¢ùê±j)exp‚Å°(ùê±i‚Å¢ùê±j)+exp‚Å°(ùê±i‚Å¢ùê±k)+exp‚Å°(ùê±j‚Å¢ùê±k))+Œª‚Å¢‚àëm‚Äñùê±‚Äñ1,‚Ñíùê±superscriptùëõsubscriptùê±ùëñsubscriptùê±ùëósubscriptùê±ùëñsubscriptùê±ùëósubscriptùê±ùëñsubscriptùê±ùëòsubscriptùê±ùëósubscriptùê±ùëòùúÜsuperscriptùëösubscriptnormùê±1\displaystyle\hskip 65.44142pt\min\mathcal{L}(\mathbf{x})=\sum^{n}\log\left(% \frac{\exp\left(\mathbf{x}\_{i}\mathbf{x}\_{j}\right)}{\exp\left(\mathbf{x}\_{i}% \mathbf{x}\_{j}\right)+\exp\left(\mathbf{x}\_{i}\mathbf{x}\_{k}\right)+\exp\left(% \mathbf{x}\_{j}\mathbf{x}\_{k}\right)}\right)+\lambda\sum^{m}\|\mathbf{x}\|\_{1},roman\_min caligraphic\_L ( bold\_x ) = ‚àë start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT roman\_log ( divide start\_ARG roman\_exp ( bold\_x start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT bold\_x start\_POSTSUBSCRIPT italic\_j end\_POSTSUBSCRIPT ) end\_ARG start\_ARG roman\_exp ( bold\_x start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT bold\_x start\_POSTSUBSCRIPT italic\_j end\_POSTSUBSCRIPT ) + roman\_exp ( bold\_x start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT bold\_x start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT ) + roman\_exp ( bold\_x start\_POSTSUBSCRIPT italic\_j end\_POSTSUBSCRIPT bold\_x start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT ) end\_ARG ) + italic\_Œª ‚àë start\_POSTSUPERSCRIPT italic\_m end\_POSTSUPERSCRIPT ‚à• bold\_x ‚à• start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , |  | (1) |

where ùê±ùê±\mathbf{x}bold\_x corresponds to an object vector; iùëñiitalic\_i, jùëójitalic\_j and kùëòkitalic\_k to the indices of the current triplet; nùëõnitalic\_n to the number of triplets; and mùëömitalic\_m to the number of objects. The regularization parameter ŒªùúÜ\lambdaitalic\_Œª, which controls the trade-off between sparsity and model performance, was determined using cross-validation on the training set (Œª=0.004ùúÜ0.004\lambda=0.004italic\_Œª = 0.004 for LLM, 0.00350.00350.00350.0035 for MLLM, 0.003850.003850.003850.00385 for humans, and 0.007 for the other models and brain ROIs). In addition to sparsity, the optimization was constrained by strictly enforcing weights in the embedding ùêóùêó\mathbf{X}bold\_X to be positive. The minimization of this objective was carried out using stochastic gradient descent with an Adam optimizer [[110](https://arxiv.org/html/2407.01067v3#bib.bib110)] (with default parameters) and a batch size of 100 on triplet odd-one-out judgments. After the optimization was complete, dimensions with weights below 0.1 for all objects were eliminated. Finally, the dimensions underwent sorting based on the sum of their weights across objects in descending order.

This model operates under two key theoretical assumptions. Firstly, it postulates sparsity within the embedding space dimensions, indicating that each object primarily influences certain dimensions rather than all. Secondly, it assumes positivity in these dimensions. Consequently, an object‚Äôs weight on a specific dimension signifies the extent of the related property within the object. These assumptions diverge from typical dimensionality reduction approaches like Principal Component Analysis (PCA), which assume dense dimensions across the real number spectrum. Furthermore, SPoSE facilitates cross-correlations among dimensions while PCA assumes independence. Consequently, SPoSE often uncovers a greater number of dimensions, reflecting finer details or attributes, which are more easily interpretable compared to PCA dimensions. Notably, the weight an object holds on a dimension directly corresponds to the presence of the associated property within the object.

We opted for the behavioral odd-one-out task and the SPoSE method to learn the low-dimensional embeddings of LLMs rather than attempting to directly access their internal features, primarily due to the challenges associated with extracting features from modern, large-scale LLMs that are often proprietary or too vast to navigate directly. This approach allows us to circumvent the limitations imposed by the closed nature or sheer scale of contemporary LLMs, providing us with a more feasible avenue to explore their mental representations.

Reproducibility of embedding dimensions.
Considering the stochastic nature of the optimization process, the SPoSE method yields varying sets of dimensions upon each reiteration. To assess the stability of the 66-dimensional embedding, we conducted 20 model runs with distinct random initializations. Evaluating each original dimension against all dimensions in the 20 reference embeddings, we identified the best-matching dimension based on the highest correlation. Consistent with previous research [[16](https://arxiv.org/html/2407.01067v3#bib.bib16)], a Fisher z-transform was applied to these correlations, averaged across the 20 reference embeddings, and then reversed to obtain a mean reliability value for each dimension across all 20 embeddings.

Category prediction.
Evaluating the representational embeddings‚Äô categorization performance involved testing them across 18 out of the 27 THINGS database categories. Objects falling into multiple categories were excluded from the analysis, resulting in the removal of 9 categories. Among these excluded categories, 7 were subcategories or had less than ten unique objects post-filtering. The remaining 18 categories included clothing, toy, vehicle, container, electronic device, animal, furniture, body part, food, musical instrument, plant, home decor, sports equipment, office supply, part of car, medical equipment, tool, and weapon, totaling 1,112 objects. Classification was conducted through leave-one-object-out cross-validation. Training involved computing category centroids by averaging the 66-dimensional vectors of all objects within each category, excluding the left-out object. The category membership of the excluded object was predicted based on the smallest Euclidean distance to the respective centroid. This process was iterated for all 1,112 objects, with prediction accuracy averaged across the dataset. The chance level is determined by 1000 permutation tests.

Evaluating consistency between humans and models by comparing behaviors.
With the exception of GPT-4, all other models (and human) have completed behavioral data acquisition on the full sample triples of the 48 typical objects described above. For each model, we constructed its RSM for the 48 objects by calculating the choice probability of each object pair. To estimate human consistency, following previous work [[58](https://arxiv.org/html/2407.01067v3#bib.bib58)], we computed the Pearson correlation on the behavioral RSMs from the model (mùëömitalic\_m) and the human (h‚Ñéhitalic\_h) and we then divide that raw Pearson correlation by the geometric mean of the split-half internal reliability measured for each system as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | œÅ~‚Å¢(m,h)=œÅ‚Å¢(R‚Å¢S‚Å¢Mm,R‚Å¢S‚Å¢Mh)œÅ‚Å¢(R‚Å¢S‚Å¢Mmh‚Å¢a‚Å¢l‚Å¢f1,R‚Å¢S‚Å¢Mmh‚Å¢a‚Å¢l‚Å¢f2)‚Å¢œÅ‚Å¢(R‚Å¢S‚Å¢Mhh‚Å¢a‚Å¢l‚Å¢f1,R‚Å¢S‚Å¢Mhh‚Å¢a‚Å¢l‚Å¢f2),~ùúåùëö‚ÑéùúåùëÖùëÜsubscriptùëÄùëöùëÖùëÜsubscriptùëÄ‚ÑéùúåùëÖùëÜsuperscriptsubscriptùëÄùëö‚Ñéùëéùëôsubscriptùëì1ùëÖùëÜsuperscriptsubscriptùëÄùëö‚Ñéùëéùëôsubscriptùëì2ùúåùëÖùëÜsuperscriptsubscriptùëÄ‚Ñé‚Ñéùëéùëôsubscriptùëì1ùëÖùëÜsuperscriptsubscriptùëÄ‚Ñé‚Ñéùëéùëôsubscriptùëì2\displaystyle\hskip 85.35826pt\tilde{\rho}(m,h)=\frac{\rho(RSM\_{m},RSM\_{h})}{% \sqrt{\rho(RSM\_{m}^{half\_{1}},RSM\_{m}^{half\_{2}})\rho(RSM\_{h}^{half\_{1}},RSM\_{% h}^{half\_{2}})}},over~ start\_ARG italic\_œÅ end\_ARG ( italic\_m , italic\_h ) = divide start\_ARG italic\_œÅ ( italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT , italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_h end\_POSTSUBSCRIPT ) end\_ARG start\_ARG square-root start\_ARG italic\_œÅ ( italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT , italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT ) italic\_œÅ ( italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_h end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT , italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_h end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT ) end\_ARG end\_ARG , |  | (2) |

where R‚Å¢S‚Å¢Mmh‚Å¢a‚Å¢l‚Å¢f1ùëÖùëÜsuperscriptsubscriptùëÄùëö‚Ñéùëéùëôsubscriptùëì1RSM\_{m}^{half\_{1}}italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT and R‚Å¢S‚Å¢Mmh‚Å¢a‚Å¢l‚Å¢f2ùëÖùëÜsuperscriptsubscriptùëÄùëö‚Ñéùëéùëôsubscriptùëì2RSM\_{m}^{half\_{2}}italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT were computed by using the split-half behavioral data of triples of the 48 typical objects, and similar for R‚Å¢S‚Å¢Mhh‚Å¢a‚Å¢l‚Å¢f1ùëÖùëÜsuperscriptsubscriptùëÄ‚Ñé‚Ñéùëéùëôsubscriptùëì1RSM\_{h}^{half\_{1}}italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_h end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT and R‚Å¢S‚Å¢Mhh‚Å¢a‚Å¢l‚Å¢f2ùëÖùëÜsuperscriptsubscriptùëÄ‚Ñé‚Ñéùëéùëôsubscriptùëì2RSM\_{h}^{half\_{2}}italic\_R italic\_S italic\_M start\_POSTSUBSCRIPT italic\_h end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_h italic\_a italic\_l italic\_f start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT end\_POSTSUPERSCRIPT.

Comparison between ChatGPT-3.5 and GPT-4 was conducted directly based on their choice consistency with human on a specific set of 2,171 triplets. We conducted a total of 5 comparisons, each based on randomly selecting 1,000 samples from these 2,171 samples, and finally reported the average result.

Dimensional relevance score for odd-one-out choice.
For a given triplet, we compute the original predicted softmax probabilities based on the entire low-dimensional embeddings of each image within the triplet. Then, we iteratively remove a certain dimension from the low-dimensional embeddings, calculate the softmax probabilities predicted by the pruned embeddings, and then compute the difference between the softmax probabilities obtained before and after pruning. This difference is taken as the relevance score for that dimension. This approach has been used in a previous study [[26](https://arxiv.org/html/2407.01067v3#bib.bib26)].

Dimension naming.
In defining the human mental embedding, the dimension names from a previous investigation were employed as references [[17](https://arxiv.org/html/2407.01067v3#bib.bib17)]. However, for LLM and MLLM, each of the 66 dimensions within the embedding was associated with common-sense labels through a straightforward naming procedure. Specifically, we analyze a set of 1-by-12 images of objects and identify shared properties described in the images. Each array consisted of images selected from the top of one dimension from the embedding. Ten of the authors provided concise labels, limited to 1‚Äì2 words, describing the arrayed images. Subsequently, word clouds were generated to visualize dimension names, showcasing the distribution of labels based on frequency, utilizing the wordcloud function in MATLAB (Mathworks) with default settings. Finally, the lead authors of this study gave intuitive labels for each dimension. Dimension labels were also summed up by the MLLM (here gemini-pro-1.5-exp) with the prompt as follows: "*There are 9 subfigures in the picture. Please use 1-2 English words or phrases to describe the common theme represented by these 9 subfigures.*"

Dimension rating for NSD images.
We predicted the 66 object dimensions for each image within the NSD dataset. Specifically, we leveraged the OpenAI-trained CLIP model [[59](https://arxiv.org/html/2407.01067v3#bib.bib59)] (with "ViT-L/14" as the backbone), which is a multimodal model trained on image-text pairs and which was recently demonstrated to yield excellent prediction of human similarity judgments [[111](https://arxiv.org/html/2407.01067v3#bib.bib111), [112](https://arxiv.org/html/2407.01067v3#bib.bib112)]. For each of the 1,854 object images in the THINGS dataset, we extracted the image and text features from the final layer of the CLIP image and text encoders, respectively. Subsequently, for each of the 66 dimensions of LLM (or MLLM, or Human), we fitted a ridge regression model to predict dimension values, using a concatenation of the extracted image and text features from CLIP as input. The optimal regularization hyperparameters were determined by using 5-fold cross-validation across the training set (100 candidate parameters spaced evenly on a log scale from 10‚àí3superscript10310^{-3}10 start\_POSTSUPERSCRIPT - 3 end\_POSTSUPERSCRIPT to 103superscript10310^{3}10 start\_POSTSUPERSCRIPT 3 end\_POSTSUPERSCRIPT, that is np.logspace(-3, 3, 100)) . These trained regression models were then applied to the extracted features across all images in the NSD dataset.

Searchlight RSA.
For fMRI, local cerebral RSMs were computed in subject space within a grey-matter spherical region (6 mm diameter) centered at each voxel location. RSA analyses assessed the Pearson correlation rùëüritalic\_r between the local cerebral RSM and each kind of the model RSMs.

SPoSE RSA.
For each brain ROI, we extracted the fMRI signal in that region on the shared\_1k dataset and constructed a large number of odd-one-out data based on the cosine distance. After that, SPoSE learning was used to obtain the corresponding low-dimensional embeddings of each brain ROI, and the RSMs of each ROI were calculated using the learned low-dimensional embeddings. Finally, Pearson correlations between the brain ROI RSM and the model RSM were calculated.

Voxel-wise encoding.
For each subject in the NSD, we built a ridge regression model to predict the fMRI response to each test image per voxel. The images of the training set are subject-specific, but the images of the test set are shared (that is, shared\_1k). For all training and testing images, we first used the dimension rating model to predict the low-dimensional embeddings, and then conducted voxel-wise fitting based on the predicted embeddings. The regularization parameter for each voxel was selected autonomously through a 5-fold cross-validation process on the training dataset. We explored 100 evenly spaced regularization parameters on a logarithmic scale ranging from 10‚àí3superscript10310^{-3}10 start\_POSTSUPERSCRIPT - 3 end\_POSTSUPERSCRIPT to 103superscript10310^{3}10 start\_POSTSUPERSCRIPT 3 end\_POSTSUPERSCRIPT, which corresponds to the np.logspace(-3, 3, 100) function in Python. The model‚Äôs accuracy was assessed on the test dataset utilizing both Pearson‚Äôs correlation coefficient (rùëüritalic\_r) and the noise ceiling normalized coefficient of determination (R2superscriptùëÖ2R^{2}italic\_R start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT). Following the NSD work [[52](https://arxiv.org/html/2407.01067v3#bib.bib52)], the noise ceiling was calculated by:

|  |  |  |  |
| --- | --- | --- | --- |
|  | N‚Å¢C=100√ón‚Å¢c‚Å¢s‚Å¢n‚Å¢r2n‚Å¢c‚Å¢s‚Å¢n‚Å¢r2+1n,ùëÅùê∂100ùëõùëêùë†ùëõsuperscriptùëü2ùëõùëêùë†ùëõsuperscriptùëü21ùëõ\displaystyle\hskip 170.71652ptNC=100\times\frac{ncsnr^{2}}{ncsnr^{2}+\frac{1}% {n}},italic\_N italic\_C = 100 √ó divide start\_ARG italic\_n italic\_c italic\_s italic\_n italic\_r start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT end\_ARG start\_ARG italic\_n italic\_c italic\_s italic\_n italic\_r start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT + divide start\_ARG 1 end\_ARG start\_ARG italic\_n end\_ARG end\_ARG , |  | (3) |

where nùëõnitalic\_n indicates the number of trials that are averaged together (n=3ùëõ3n=3italic\_n = 3 for subjects S1, S2, S5, and S7), and n‚Å¢c‚Å¢s‚Å¢n‚Å¢rùëõùëêùë†ùëõùëüncsnritalic\_n italic\_c italic\_s italic\_n italic\_r indicates the noise ceiling signal-to-noise ratio which has been provided in NSD.
To ascertain the statistical significance of our predictions, we conducted a bootstrapping procedure, resampling the test dataset with replacement 2,000 times, and subsequently calculated the False Discovery Rate (FDR) adjusted PùëÉPitalic\_P-values.

Abbreviation of Brain ROIs.
EarlyVis: early visual cortex; Scene, PPA: parahippocampal place area, OPA: occipital place area, RSC: retrosplenial cortex; Body, EBA: extrastriate body area; Face, FFA-1: fusiform face area 1, FFA-2: fusiform face area 2; Mind and Language, TPOJ-1: temporoparietal junction 1, AG: angular gyrus, Broca, MTL: medial temporal lobe.

Visualization of cerebral cortex.
To visualize the analytical outcomes across the entire cortical region, we employed flattened cortical surfaces derived from individual subjects‚Äô anatomical images. FreeSurfer [[113](https://arxiv.org/html/2407.01067v3#bib.bib113)] facilitated the generation of cortical surface meshes from T1-weighted anatomical images. This process involved applying five relaxation cuts on each hemisphere‚Äôs surface and excluding the corpus callosum. Subsequently, functional images were registered to the anatomical images and mapped onto the surfaces for visualization purposes using Pycortex [[114](https://arxiv.org/html/2407.01067v3#bib.bib114)].
