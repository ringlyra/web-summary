---
title: 'LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs'
source: https://arxiv.org/pdf/2506.14429
author:
  - Xiaoran Liu
  - Zhigeng Liu
  - Zengfeng Huang
  - Qipeng Guo
  - Ziwei He
  - Xipeng Qiu
published: '2025-06-17T11:45:37Z'
fetched: '2025-06-21T06:00:00.284623+00:00'
tags:
  - codex
  - arxiv
image: 
---

## 要約

本論文では、拡散型言語モデル（diffusion LLM）の長文脈性能を系統的に分析し、従来の自己回帰型モデルと比較した。diffusion LLMは訓練長を超える入力でも困難なく処理でき、パープレキシティが安定し、最新4kトークンからの局所的な情報取得が可能であることを確認した。この特性をRotary Position Embeddingの理論で説明し、NTKベースのスケーリングとLLaDAを組み合わせたLongLLaDAを提案。学習なしで24kトークンまで文脈を拡張し、既存のスケーリング則が有効であることを実証した。さらに、取得や質問応答など長文脈タスクで高い性能を示す一方、情報統合では自己回帰型に劣る側面も明らかにした。

## 本文

LONG LLADA: U NLOCKING LONG CONTEXT CAPABILITIES IN DIFFUSION LLM S Xiaoran Liu1,2, Zhigeng Liu1, Zengfeng Huang1,2, Qipeng Guo2,3, Ziwei He2∗, Xipeng Qiu1,2∗ 1School of Computer Science, Fudan University,2Shanghai Innovation Institute,3Shanghai AI Labxrliu24@m.fudan.edu.cn ,ziwei.he@sjtu.edu.cn ,xpqiu@fudan.edu.cn ABSTRACT Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understandingtheir scalability and downstream task performance. However, their long-contextcapabilities remain unexplored, lacking systematic analysis or methods for contextextension. In this work, we present the first systematic investigation comparingthe long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike autoregressive LLMs, they maintain remarkably stable perplexity during direct contextextrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct “ local perception ” phenomenon, enablingsuccessful retrieval from recent context segments. We explain both phenomenathrough the lens of Rotary Position Embedding (RoPE) scaling theory. Buildingon these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate thatestablished extrapolation scaling laws remain effective for extending the contextwindows of diffusion LLMs. Furthermore, we identify long-context tasks wherediffusion LLMs outperform auto-regressive LLMs and others where they fall short.
Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarkscritical for advancing future research on long-context diffusion LLMs.
1 I NTRODUCTION Recently, diffusion LLMs have become a widely discussed topic in Natural Language Processingresearch (Nie et al., 2025; Ye et al., 2025). They are regarded as a potential solution to key limitations 0 4000 8000 12000 16000 Context Length51015202530PerplexityLLaMA3-8B-Base LLaMA3-8B-Instruct LLaDA-8B-Base LLaDA-8B-Instruct (a) LLaDA-8B-Base 2000 4000 8000 16000 24000 32000 Context Length020406080100Retrieval Accuracy LLaMA3-8B-Base LLaMA3-8B-Instruct LLaDA-8B-Base LLaDA-8B-Instruct (b) LLaMA3-8B-Base Figure 1: Comparison of perplexity and retrieval accuracy between the diffusion LLM, LLaDA-8B, and the auto-regressive LLM, LLaMA3-8B, both within and beyond pre-training context length.
∗Corresponding Author.
SII-OpenMOSSof traditional auto-regressive LLMs, including the reversal curse (Berglund et al., 2023), complexreasoning (Dziri et al., 2023), long-term planning, and maintaining coherence across extended contexts (Bachmann & Nagarajan, 2024; Ye et al., 2024; 2025). Significant research efforts have focusedon validating their scalability (Nie et al., 2025; Ye et al., 2025), adapting them for multimodality (Yanget al., 2025; You et al., 2025; Yu et al., 2025), applying them to reasoning tasks (Zhao et al., 2025; Huang et al., 2025; Zhu et al., 2025), and optimizing their efficiency (Ma et al., 2025; Hu et al., 2025; Wu et al., 2025). However, the long-context capabilities of diffusion LLMs, specifically theirperformance and potential for length extrapolation, remain unexplored.
We begin by systematically evaluating diffusion LLM LLaDA (Nie et al., 2025) against auto-regressive LLM LLaMA3 (Meta, 2024a) on perplexity and retrieval tasks, both within and beyond theirpretrained context lengths (Figure 1). Notably, diffusion LLMs maintain stable perplexity and exhibitlocalized perception during direct length extrapolation. In stark contrast, auto-regressive LLMs suffercatastrophic perplexity surges and performance collapse when input length exceeds their maximumsupported context window, 8k tokens. This divergence reveals fundamental architectural differencesin long-context handling, raising critical questions: (1) What mechanisms enable diffusion LLMs’ extrapolation stability? (2) Can established length-extension techniques for auto-regressive LLMsbe transferred to diffusion architectures? (3) How do diffusion LLMs perform on long-contextbenchmarks relative to auto-regressive baselines, and what unique capabilities or limitations emerge?
In this work, we address these questions through comprehensive experiments and analysis. Besides theperplexity and retrieval experiment, we also benchmark Needle-In-A-Haystack (NIAH) performancefor diffusion LLMs (LLaDA (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025), Dream-v0 (Yeet al., 2025)), quantitatively confirming their local perception bias during length extrapolation. Wethen analyze this phenomenon through Rotary Position Embedding (RoPE) theory, validating ourinterpretation with t-SNE visualizations. Building on these insights, we propose LongLLaDA, atraining-free method which successfully extends LLaDA’s context window using NTK-based RoPEextrapolation (bloc97, 2023b), and verify preserved scaling laws (Liu et al., 2023b). Finally, weidentify task-dependent capabilities where diffusion LLMs surpass or lag behind auto-regressivecounterparts on long-context benchmarks. Our contributions are summarized as follows:
•First systematic analysis of diffusion LLMs’ long-context behavior, revealing their uniquecharacteristics for stable perplexity and local perception during context extrapolation, withmechanistic explanation via RoPE dynamics.
•Effective context extension demonstrating NTK-based RoPE extrapolation and scalinglaws transfer seamlessly to diffusion LLMs, achieving 6 ×context expansion (24k tokens) without further training.
•Capability benchmarking revealing diffusion LLMs match auto-regressive models onretrieval tasks, lag in aggregation, but excel at QA. We provide foundational insights forfuture long-context diffusion research.
2 L ONG -CONTEXT PHENOMENOLOGY OF DIFFUSION LLM S We first evaluate the length extrapolation capabilities of diffusion LLMs, including LLaDA (Nieet al., 2025), LLaDA-1.5 (Zhu et al., 2025), and Dream-v0 (Ye et al., 2025), compared with autoregressive LLMs such as LLaMA3 (Meta, 2024a), via Needle-In-A-Haystack (Gkamradt, 2023; Liet al., 2024), based on the experimental setup in Appendix B.1. All LLMs are required to generate atmost 32 tokens, with diffusion LLMs using a block size and sampling steps of 32. The results areshown in Figure 2. LLaMA3-8B-Base and LLaMA3-8B-Instruct maintain perfect retrieval accuracywithin their pretrained 8k length, but suffer catastrophic performance degradation beyond this limit, failing to retrieve information at any depth. In contrast, LLaDA-8B-Base and LLaDA-8B-Instructachieve 100% retrieval accuracy within a 4k context. Surprisingly, when exceeding 4k, up to 24k, LLaDA still retrieves information from the nearest 4k window, demonstrating a local perception likea sliding window. This behavior remarkably differs from auto-regressive LLM extrapolation. Similarphenomena are observed in LLaDA-1.5 and Dream-v0, as illustrated in Appendix B.2.
Different from auto-regressive LLMs, diffusion LLMs are influenced by sampling steps and strategies.
For simplicity, we compare the impact of sampling steps on retrieval depth in NIAH. As shown in Figure 3, using the same input-output settings from previous experiments, we evaluate the LLaDA-SII-OpenMOSS 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 54.25 020406080100 Score Average Depth Score (a) LLaDA-8B-Base 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 51.71 020406080100 Score Average Depth Score (b) LLaMA3-8B-Base 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 52.41 020406080100 Score Average Depth Score (c) LLaDA-8B-Instruct 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 46.60 020406080100 Score Average Depth Score (d) LLaMA3-8B-Instruct Figure 2: Results of Needle-In-A-Haystack tests (Gkamradt, 2023) on LLaDA-8B Series (Nie et al., 2025) and LLaMA3-8B Series (Meta, 2024b) under direct extrapolation.
2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 45.59 020406080100 Score Average Depth Score (a) LLaDA-8B-Base with s= 1 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 47.15 020406080100 Score Average Depth Score (b) LLaDA-8B-Base with s= 4 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 52.75 020406080100 Score Average Depth Score (c) LLaDA-8B-Base with s= 8 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 53.60 020406080100 Score Average Depth Score (d) LLaDA-8B-Base with s= 16 Figure 3: NIAH Results of LLaDA-8B-Base (Nie et al., 2025) with different sampling steps s.
8B-Base with sampling step s= 1,4,8,16. Results show that at 1 or 4 steps, LLaDA-8B-Base fails toretrieve information beyond 8k length, and increasing sto 8 or 16 can achieve retrieval depths of 25% at 16k and almost 10% at 24k context length. Similar results are observed on LLaDA-8B-Instruct and LLaDA-1.5 in Appendix B.2, demonstrating that the long-context performance of diffusion LLMs isinfluenced by sampling steps, but remains constrained by the maximum supported context length.
SII-OpenMOSS -8192 -4096 0 4096 8192 Relative Position1 -1 -1 -1 [1 - Ttrain, Ttrain - 1]cossin (a) LLaDA with Ttrain=4k 0 4096 8192 12288 16384 Relative Position1 -1 -1 -1 [0, Ttrain - 1]cossin (b) LLaMA3 with Ttrain=8k Figure 4: Comparison of trained position embedding interval between LLaDA-8B and LlaMA3-8B.
The area within the dashed line represents trained relative position, while that beyond represents therelative position in length extrapolation, with unlearned position embedding values colored in gray.
3 M ECHANISTIC ANALYSIS According to the preliminary knowledge in Appendix A, we attribute this phenomenon to diffusion LLMs being trained with richer positional information compared to auto-regressive LLMs. Critically, the bidirectional attention mechanisms in diffusion LLMs expose them to relative position rage of [1−Ttrain, Ttrain−1]during training, contrasting with the [0, Ttrain−1]range typical of auto-regressivemodels. This difference is evident in the RoPE mechanism. As visualized in Figure 4, for LLaDA (Ttrain=4k) and LLaMA ( Ttrain=8k), we observe how the positional embeddings (sine/cosinecomponents) behave within and beyond their maximum trained relative positions.
•High Frequencies : Both models perceive complete sinusoidal periods within their maximumtrained relative distance, yielding comparable positional information encoding.
•Moderate Frequencies : LLaMA3’s auto-regressive attention observes relative positions [0,8191] when trained on 8192-token sequences. In contrast, LLaDA’s bidirectional attentionobserves symmetric relative positions [−4095,4095] despite its shorter 4096-token traininglength. This symmetric coverage provides a key advantage by fully capturing a completeperiod of both the cosine and sine, enhancing its tolerance of direct length extrapolation.
•Low Frequencies : Both models exhibit limited extrapolation capability beyond their pretrained context windows. However, as visualized in Figure 4, the out-of-distribution (OOD) regions differ remarkably: LLaMA3 struggles to capture all negative position embeddings (gray region), representing half of the potential embedding space, while LLaDA significantlyreduces the unlearned OOD spaces, resulting in enhanced robustness in length extrapolation.
This results in a relatively flattened perplexity growth curve, similar to auto-regressive RoPE-based LLMs with a smaller base (Liu et al., 2023b; Men et al., 2024), as detailed in Appendix A. However, since the cosine function in RoPE, which primarily captures relative distances, is even, negativerelative positions do not increase the LLM’s maximum perceivable distance in the pre-training stage.
Thus, diffusion LLM can only retrieve key information from limited relative positions within thetraining length, leading to the observed decay pattern in the NIAH evaluation.
We validate this interpretation with the t-SNE visualization (Van der Maaten & Hinton, 2008; Zandiehet al., 2024) of QK states from the final layer of LLaMA3-8B-Base (Meta, 2024a) and LLaDA-8BBase (Nie et al., 2025), as shown in Figure 5. As shown in Figure 5a, for auto-regressive LLMs suchas LLaMA3-8B-Base, the QK states within and beyond the maximum supported context length, 8k, present two different distribution clusters, and the manifold for QK states with RoPE also shows adifferent trend when position embedding becomes OOD. Comparatively, regarding the clustering SII-OpenMOSS (a) LLaDA-8B-Base (b) LLaMA3-8B-Base Figure 5: Visualization of the QK states from the final layer of LLaMA3-8B-Base (Meta, 2024a) and LLaDA-8B-Base (Nie et al., 2025) for sample from the GovReport subsets in LongBench (Bai et al., 2023). The visualization uses a 2D t-SNE projection (Van der Maaten & Hinton, 2008), with eachtoken represented as a point in the image and the position index shown via color changing.
feature for diffusion LLMs such as LLaDA-8B-Base, there is no distribution shift between QK stateswithin and beyond 4k, and a uniform manifold for QK states with RoPE. This demonstrates thatdiffusion LLM is more robust for the OOD position embeddings in length extrapolation. Therefore, unlike traditional auto-regressive LLMs that exhibit catastrophic performance degradation whenexceeding their maximum supported context length, diffusion LLMs maintain stable outputs anddemonstrate local perception in extended context.
4 C ONTEXT EXTENSION FORDIFFUSION LLM S Since the reason for the surprising phenomenon has been clarified, we now move on to the extrapolation methods for diffusion LLMs. Since the retrievable depth of diffusion LLMs remains constrainedby the range of cosine values encountered during pre-training, we transfer the NTK-based extrapolation (bloc97, 2023b) and its scaling laws (Liu et al., 2023b) to diffusion LLMs, thus proposingthe length extrapolation method for diffusion LLMs, LongLLaDA. As detailed in Appendix A. Thescaling factor λin training-free NTK scaling (bloc97, 2023b) for RoPE-based auto-regressive LLMsis decided by the extrapolation context length tand critical dimension dextracalculated by rotary baseβ0and pretrained context length Ttrain, as shown in Equation 1.
λ= 10−4·t 2πd/d extra , d extra= 2d 2logβ0Ttrain 2π . (1) Similarly, in LongLLaDA, based on Nie et al. (2025), the pretrained rotary base β0= 500000 , andthe pre-training context length Ttrainis 4k. This yields a critical dimension dextra= 64 . Accordingly, the required scaling factor λfor extrapolation to 8k, 16k, 24k, and 32k is calculated as 4, 14, 31, and 55, respectively. The extrapolation results are illustrated in Figure 6 and Figure 7.
When λ= 4,14, LongLLaDA can effectively extrapolate diffusion LLMs to the correspondingcontext lengths, achieving near 100% recall across all depths within these ranges. As the contextlength increases beyond the extrapolation limit, the retrievable depth proportionally expands whilemaintaining the local-perception effect. The average depth score curves exhibit a right shift acrossdifferent context lengths. When λ= 31 , a lost-in-the-middle phenomenon (Liu et al., 2023a) similarto auto-regressive models emerges in intermediate depths, indicating that LongLLaDA approaches itspractical extrapolation limit (bloc97, 2023b). When λ= 55 , further extrapolation is unachievable.
We also validate the effectiveness of LongLLaDA in LLaDA-1.5 (Zhu et al., 2025) and Dream-v0 (Yeet al., 2025) in Appendix B.2. Consequently, for RoPE-based diffusion LLMs, NTK extrapolationand its scaling law remain applicable during inference.
SII-OpenMOSS 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 71.33 020406080100 Score Average Depth Score (a) LLaDA-8B-Base with λ= 4 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 82.33 020406080100 Score Average Depth Score (b) LLaDA-8B-Base with λ= 14 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 82.33 020406080100 Score Average Depth Score (c) LLaDA-8B-Base with λ= 31 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 75.43 020406080100 Score Average Depth Score (d) LLaDA-8B-Base with λ= 55 Figure 6: NIAH Results of LLaDA-8B-Base (Nie et al., 2025) with different RoPE scaling factor.
2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 61.72 020406080100 Score Average Depth Score (a) LLaDA-8B-Instruct with λ= 4 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 67.97 020406080100 Score Average Depth Score (b) LLaDA-8B-Instruct with λ= 14 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 67.19 020406080100 Score Average Depth Score (c) LLaDA-8B-Instruct with λ= 31 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 61.69 020406080100 Score Average Depth Score (d) LLaDA-8B-Base with λ= 55 Figure 7: NIAH Results of LLaDA-8B-Instruct (Nie et al., 2025) with different RoPE scaling factor.
5 T ASK-DRIVEN LONG -CONTEXT CAPABILITY ANALYSIS Regarding the downstream long-context performance of diffusion LLMs and their difference fromtraditional auto-regressive LLMs, apart from the NIAH retrieval evaluation, we conduct comparativeanalyses across more benchmarks using LLaDA and LLaMA as examples. We first evaluate LLaDA8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025), and LLaMA3-8B (Meta, 2024a), including pretrained models and those employing NTK-based extrapolation during inference, with LongBench (Baiet al., 2023), in 4k and 8k context length, with the exceeding part being truncated from the middle.
For the summary tasks, the output length is 512, while for the others, the output length is 64. We still SII-OpenMOSS 4k 8k SD MD Sum ICL Syn Code Avg SD MD Sum ICL Syn Code Avg LLaDA-8B-Base 15.1 18.4 32.0 42.0 54.7 59.6 34.1 13.9 13.1 30.8 40.7 56.0 57.4 32.4 + NTK λ= 4 14.7 19.1 31.5 40.9 52.4 63.0 34.0 15.2 18.6 31.0 41.4 53.8 59.2 33.6 LLaDA-8B-Instruct 25.1 19.4 30.6 36.4 62.8 62.7 37.2 22.7 14.0 33.4 33.2 66.7 66.4 36.8 + NTK λ= 4 22.1 19.8 33.0 38.0 63.3 65.0 37.8 23.4 19.8 35.3 39.8 72.9 67.3 40.6 LLaDA-1.5 24.4 19.4 31.6 33.5 63.6 66.7 37.6 22.6 14.5 33.4 33.0 67.6 67.6 37.1 + NTK λ= 4 21.8 19.7 33.1 35.3 63.4 67.3 37.8 23.0 20.6 34.9 39.3 72.9 67.9 40.7 LLaMA3-8B-Base 17.2 18.7 25.0 41.7 47.6 66.5 33.6 18.2 18.3 26.1 44.5 49.6 69.4 35.1 LLaMA3-8B-Instruct 31.9 26.1 33.6 39.6 46.6 55.9 37.0 37.5 28.3 34.7 40.7 62.8 56.1 41.9 Table 1: Results of LLaDA-8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025) and LLaMA38B (Meta, 2024b) on LongBench (Bai et al., 2023) under 4k and 8k context length. Gray cellsindicate that the evaluation context length exceeds the context length supported by the evaluated LLM.
SD, MD, Sum, and Syn stand for Single-Doc QA, Multi-Doc QA, Summarization, and Synthetictasks, while Avg is the average score of all subtasks weighted by the evaluation data number.
4k 8k 16k NIAH AGG QA Avg NIAH AGG QA Avg NIAH AGG QA Avg LLaDA-8B-Base 99.7 65.2 82.5 89.1 53.8 45.2 41.0 49.8 22.0 1.9 36.0 19.5 + NTK λ= 4 99.5 82.3 80.5 92.6 96.4 61.0 73.0 84.7 51.8 17.1 53.5 44.1 + NTK λ= 14 99.8 83.5 77.0 92.5 99.3 68.9 64.0 86.8 85.4 48.1 54.0 72.0 + NTK λ= 31 100.0 83.8 77.0 92.7 97.8 75.2 62.5 87.1 97.2 51.8 41.0 78.0 LLaDA-8B-Instruct 99.3 57.8 90.5 88.4 52.3 44.3 48.0 49.8 18.9 12.0 47.0 21.6 + NTK λ= 4 99.8 65.5 89.5 90.3 95.9 56.6 89.0 85.8 41.6 31.0 73.0 44.0 + NTK λ= 14 100.0 76.4 89.0 92.9 97.3 66.2 89.5 88.9 67.1 53.8 84.5 66.7 + NTK λ= 31 100.0 77.7 86.5 92.8 98.8 73.5 88.5 91.3 88.0 62.2 82.0 81.1 LLaDA-1.5 98.7 66.0 90.0 89.8 53.9 45.1 48.5 51.0 19.0 14.2 46.0 22.1 + NTK λ= 4 99.8 73.9 91.0 92.5 96.3 59.3 88.0 86.5 43.3 32.2 73.0 45.3 + NTK λ= 14 100.0 79.8 88.5 93.6 99.9 67.8 89.0 90.8 67.4 51.6 84.0 66.3 + NTK λ= 31 100.0 81.6 87.5 93.8 98.9 75.1 86.5 91.5 85.8 58.2 81.5 78.7 LLaMA3-8B-Base 99.8 98.1 67.5 94.4 99.6 93.5 63.0 92.5 0.0 0.0 0.0 0.0 + NTK λ= 4 99.9 98.7 65.0 94.2 99.8 94.1 59.0 92.2 97.0 86.6 54.5 88.1 + NTK λ= 13 99.5 98.6 66.0 94.1 99.1 94.0 59.0 91.8 93.8 90.3 56.0 87.2 LLaMA3-8B-Instruct 99.6 97.2 68.5 94.3 98.2 92.6 54.0 90.1 0.0 0.0 0.0 0.0 + NTK λ= 4 99.8 96.9 72.0 94.9 99.6 93.5 65.0 92.8 95.0 89.5 63.0 88.8 + NTK λ= 13 99.5 96.7 68.0 94.0 99.3 92.4 63.5 92.2 95.3 78.6 62.5 86.4 Table 2: Results of LLaDA-8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025) and LLaMA38B (Meta, 2024b) on RULER (Hsieh et al., 2024) under 4k, 8k and 16k context length.
keep the sampling steps the same as the output length, and the block size to 64 for diffusion LLMs.
The results are shown in Table 1. Still, LLaDA can give a stable output and get a decent performancebeyond the maximum supported context length. Moreover, we find that in all task domains besidessynthetic tasks, the difference between LLaDA Series and LLaMA3 Series is relatively limitedcompared with the difference within LLaMA3 Series. Only in the synthetic domain do LLaDA Series outperform LLaMA3 Series consistently. This inspires us to conduct an in-depth discussion ofdiffusion LLMs on the performance of the synthesis tasks compared with auto-regressive LLMs.
We further the discussion with RULER benchmark (Hsieh et al., 2024), we compare LLaDA-8B (Nieet al., 2025), LLaDA-1.5 (Zhu et al., 2025), and LLaMA3-8B (Meta, 2024a), at context lengths of 4k, 8k, and 16k. We set the block size and sampling steps to 64 for diffusion LLMs. The resultsare shown in Table 2. First, consistent with the NIAH results, auto-regressive LLMs fail to producevalid outputs beyond their effective context length, while diffusion LLMs maintain measurableperformance. Regarding task types, diffusion LLMs achieve comparable results to auto-regressive SII-OpenMOSS LLMs on NIAH tasks, including Single-Key, Multi-Key, Multi-Query, and Multi-Value variants.
However, diffusion LLMs show significantly inferior performance in aggregation tasks, including Variable Tracing and Frequent or Common Word Extraction, where auto-regressive LLMs typicallyperform well. Surprisingly, on QA tasks, including SQuAD and Hotpot, that challenge auto-regressive LLMs (Hsieh et al., 2024), diffusion LLMs demonstrate superior capability. These observations revealthe distinctive characteristics of diffusion LLMs in long-context tasks, that current diffusion LLMs, like LLaDA, demonstrate comparable performance to the auto-regressive LLMs, like LLaMA3, inmost task types, but underperform in aggregation tasks , and outperform in QA tasks consistently.
6 R ELATED WORK Large Language Diffusion Models Recently, Large Language Diffusion Models, or diffusion LLMs, have become a widely discussed topic in NLP research. After the theoretical simplification (Sahoo et al., 2024; Ou et al., 2024) and fine-tuning verification (Gong et al., 2024), researchersscale the size of diffusion LLMs to billions of parameters (Nie et al., 2024; 2025; Ye et al., 2025) anddemonstrate that diffusion LLMs can achieve comparable results with more promising performancein the reversal curse (Berglund et al., 2023). These immediately attract the attention of many moreresearchers. Significant research efforts have focused on adapting diffusion LLMs for multimodality, such as MMaDA (Yang et al., 2025), LLaDA-V (You et al., 2025), and LaViDa (Li et al., 2025), applying them to reasoning tasks, such as d1 (Zhao et al., 2025), DCoLTHuang et al. (2025), and LLaDA-1.5 (Zhu et al., 2025), and optimizing their efficiency (Ma et al., 2025; Hu et al., 2025; Wuet al., 2025), including dKV-Cache (Ma et al., 2025), Dimple (Yu et al., 2025), dLLM-Cache (Liuet al.), FreeCache (Hu et al., 2025), Fast-dLLM (Wu et al., 2025), and so on. However, there is stillno discussion on the long-context capability of diffusion LLMs.
Length Extrapolation in LLM Length extrapolation, or length generalization, or context extension, is an important issue for LLMs (Press et al., 2022). The mainstream extrapolation research mainlyfocuses on adjusting position embedding, especially the widely used RoPE (Su et al., 2021). Forexample, Linear PI (Chen et al., 2023) first achieves LLMs’ length extrapolation by scaling positionindices to the pre-training range with little fine-tuning. The NTK method (bloc97, 2023b;a; Penget al., 2023) then scales the rotary base in RoPE (Su et al., 2021) to achieve plug-and-play lengthextrapolation. Subsequently, amplifying the rotary base and training on longer lengths has becomethe dominant approach for length extrapolation (Rozi `ere et al., 2023; Xiong et al., 2023; Liu et al., 2023b; Ding et al., 2024). In addition, ReRoPE (Su, 2023), ReAttention (Liu et al., 2024b), and DCA (An et al., 2024a;b) also achieve plug-and-play extrapolation by limiting the relative position.
In this paper, we still focus on the length extrapolation via NTK scaling (bloc97, 2023b; Liu et al., 2023b) in the inference stage, and try to reveal and explain the similarities and differences in lengthextrapolation between diffusion-based and auto-regressive LLM.
7 C ONCLUSION In this work, we provide the first systematic analysis of long-context capabilities in diffusion LLMs.
We demonstrate and analyze their characteristics for stable perplexity and local perception in directcontext extrapolation from the perspective of the RoPE dynamic. Then, we propose LongLLaDA, which extends the context length in NTK scaling effectively without further training, and validatethat the scaling laws still work for diffusion LLMs. Besides, we also show that diffusion LLMsmatch auto-regressive models on the average score of LongBench as well as the retrieval tasks, lag inaggregation tasks, but excel at QA in RULER evaluation. We hope our work can pave the foundationfor future long-context research in diffusion LLMs.
LIMITATION Although we have conducted extensive experiments on diffusion LLMs, our results mainly focus on LLaDA Series and the inference stage. We will carry out the fine-tuning extrapolation experimentsin the future to verify more conclusions in RoPE-based scaling theory for auto-regressive LLMs.
Besides, we will also add more analyses focused on sampling strategy specialized in diffusion LLMs.
SII-OpenMOSS REFERENCES Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.
Training-free long-context scaling of large language models. arXiv preprint arXiv:2402.17463 , 2024a.
Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of llms fall short? arXiv preprint arXiv:2410.18745 , 2024b.
Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. arXiv preprint Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long contextunderstanding. arXiv preprint arXiv:2308.14508 , 2023.
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on” a is b” fail to learn” b is a”. arXiv preprintbloc97. Dynamically scaled rope further increases performance of long context llama with zerofine-tuning, July 2023a. URL https://www.reddit.com/r/LocalLLaMA/comments/ 14mrgpr/dynamically_scaled_rope_further_increases/ .
bloc97. Ntk-aware scaled rope allows llama models to have extended (8k+) contextsize without any fine-tuning and minimal perplexity degradation., June 2023b. URLhttps://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/ .
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window oflarge language models via positional interpolation. CoRR , abs/2306.15595, 2023. doi: 10.48550/ ARXIV .2306.15595. URL https://doi.org/10.48550/arXiv.2306.15595 .
OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models.
https://github.com/open-compass/opencompass , 2023.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. doi: 10.48550/ARXIV .2307.08691. URL https://doi.org/10.
48550/arXiv.2307.08691 .
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783 , 2024.
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits oftransformers on compositionality. Advances in Neural Information Processing Systems , 36:70293– 70332, 2023.
Gkamradt. Needle in a haystack - pressure testing llms. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack , 2023.
Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation fromautoregressive models. arXiv preprint arXiv:2410.17891 , 2024.
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simpleon-the-fly length generalization for large language models. CoRR , abs/2308.16137, 2023. doi: 10.
48550/ARXIV .2308.16137. URL https://doi.org/10.48550/arXiv.2308.16137 .
SII-OpenMOSS Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXivpreprint arXiv:2404.06654 , 2024.
Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng. Can perplexity reflect largelanguage model’s ability in long text understanding? arXiv preprint arXiv:2405.06105 , 2024.
Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guideddiffusion. arXiv preprint arXiv:2505.21467 , 2025.
Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusionchain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446 , 2025.
Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng Zhang. Massive values in self-attention modules are the key to contextual knowledgeunderstanding. arXiv preprint arXiv:2502.01563 , 2025.
Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval andreasoning in 1 million context window? arXiv preprint arXiv:2407.11963 , 2024.
Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: A large diffusion languagemodel for multimodal understanding. arXiv preprint arXiv:2505.16839 , 2025.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws ofrope-based extrapolation. CoRR , abs/2310.05209, 2023b. doi: 10.48550/ARXIV .2310.05209.
URLhttps://doi.org/10.48550/arXiv.2310.05209 .
Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Mianqiu Huang, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Beyond homogeneous attention: Memory-efficient llms via fourier-approximated kv cache. arXiv preprint Xiaoran Liu, Ruixiao Li, Qipeng Guo, Zhigeng Liu, Yuerong Song, Kai Lv, Hang Yan, Linlin Li, Qun Liu, and Xipeng Qiu. Reattention: Training-free infinite context with finite attention scope.
arXiv preprint arXiv:2407.15176 , 2024b.
Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, et al. Thus spake long-context large language model. arXiv preprint Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching.
Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusionlanguage models. arXiv preprint arXiv:2505.15781 , 2025.
Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen. Base of rope bounds context length. arXiv preprint arXiv:2405.14591 , 2024.
AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI. , 2024a.
AI Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI. , 2024b.
Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514 , 2024.
SII-OpenMOSS Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992 , 2025.
Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li.
Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXivpreprint arXiv:2406.03736 , 2024.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context windowextension of large language models. CoRR , abs/2309.00071, 2023. doi: 10.48550/ARXIV .2309.
00071. URL https://doi.org/10.48550/arXiv.2309.00071 .
Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URLhttps://openreview.net/forum?id=R8sQPpGCv0 .
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR , abs/2308.12950, 2023. doi: 10.
48550/ARXIV .2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950 .
Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and V olodymyr Kuleshov. Simple and effective masked diffusion languagemodels. Advances in Neural Information Processing Systems , 37:130136–130184, 2024.
Jianlin Su. Rerope: Rectified rotary position embeddings, July 2023. URL https://github.
com/bojone/rerope .
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer withrotary position embedding. CoRR , abs/2104.09864, 2021. URL https://arxiv.org/abs/ 2104.09864 .
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shiduo Zhang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, Yu-Gang Jiang, and Xipeng Qiu. Moss: An open conversational large languagemodel. Machine Intelligence Research , 2024. ISSN 2731-5398. doi: 10.1007/s11633-024-1502-8.
URLhttps://github.com/OpenMOSS/MOSS .
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machinelearning research , 9(11), 2008.
Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. arXiv preprint Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Videorope: What makes for goodvideo rotary position embedding? arXiv preprint arXiv:2502.05173 , 2025.
Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cacheand parallel decoding. arXiv preprint arXiv:2505.22618 , 2025.
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling offoundation models. arXiv preprint arXiv:2309.16039 , 2023.
SII-OpenMOSS An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2technical report. arXiv preprint arXiv:2407.10671 , 2024.
Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada:
Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809 , 2025.
Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream .
Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large languagemodel with parallel decoding. arXiv preprint arXiv:2505.16990 , 2025.
Amir Zandieh, Insu Han, Vahab Mirrokni, and Amin Karbasi. Subgen: Token generation in sublineartime and memory. arXiv preprint arXiv:2402.06082 , 2024.
Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusionlarge language models via reinforcement learning. arXiv preprint arXiv:2504.12216 , 2025.
Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization forlarge language diffusion models. arXiv preprint arXiv:2505.19223 , 2025.
A P RELIMINARY : ROPE E XTRAPOLATION IN AUTO-REGRESSIVE LLM Rotary Position Embedding (RoPE) (Su et al., 2021) employs trigonometric functions to encodeabsolute positions in Q state qt=hq(0) t,· · ·, q(d−1) tiand K state ks=hk(0) s,· · ·, k(d−1) si . Byleveraging the properties of rotation matrices, RoPE encodes relative position in the attention matrix A, as shown in Equantion 2 and demonstrates superior performance, thus being widely adopted bymany auto-regressive LLMs (Sun et al., 2024; Dubey et al., 2024; Yang et al., 2024).
At,s= (qtRt) (ksRs)⊤=qtRt−sk⊤ s =d/2−1Xn=0" q(2n) tq(2n+1) t#⊤ cosθn(t−s)−sinθn(t−s) sinθn(t−s) cos θn(t−s)" k(2n) sk(2n+1) s# =d/2−1Xn=0 q(2n) tk(2n) s+q(2n+1) t k(2n+1) s cosθn(t−s) − q(2n) tk(2n+1) s −q(2n+1) t k(2n) s sinθn(t−s). (2) However, RoPE still faces the length extrapolation issue (Press et al., 2022). When RoPE-based autoregressive LLMs are tested beyond the pre-trained context length, the perplexity rises significantly, and downstream performance drops sharply. The underlying causes and corresponding solutions canbe attributed to two key properties of trigonometric functions: periodicity andmonotonicity .
SII-OpenMOSS Rule of Periodicity According to the design of RoPE, different dimensions of qt,ksuse differentrotary angles θn, with rotary base β0= 10000 by default and the periods Tnforsin(θnt)andcos(θnt) increasing from low to high dimensions as shown in Equation 3.
θn=β−2n/d 0 , T n= 2π·β2n/d 0, n = 0,· · ·, d/2−1. (3) For lower dimensions, Tnis very short, compared with the pre-trained context length Ttrain, while forhigher ones, Tnbecomes significantly longer, exceeding Ttrain. Consequently, there exists a criticaldimension ,dextra, as shown in Equation 4, within which sin(θnt)orcos(θnt)complete at least onefull period within the pretrained length, whereas those beyond do not.
dextra= 2d 2logβ0Ttrain 2π . (4) Therefore, dimensions beyond dextrawill encounter OOD position embedding when processing longerinputs and larger position indices in inference, leading to extrapolation issues (Liu et al., 2023b).
To enable LLM to handle unseen position indices, NTK methods (bloc97, 2023b; Xiong et al., 2023) scale the rotary base by a factor λ, reducing the rotary angle to achieve position interpolation. However, since different dimensions undergo different degrees of interpolation, the position embedding at thecritical dimension will first become OOD. Thus, based on the scaled period of the critical dimension, the extrapolation upper bound Textrafor NTK methods can be derived, as shown in Equation 5.
Textra= 2π·(λ·β0)dextra/d. (5) Based on Equation 5, for an input length t, the rotary base scaling factor λshould be set as shownin Equation 6 to ensure no OOD position embeddings occur. Notably, this adjustment coefficientexhibits a sup-linear, power-law increase with inference length (Liu et al., 2023b; 2025).
λt=β−1 0·t 2πd/d extra . (6) It should be noted that while such interpolation could theoretically avoid extrapolation issues, it canonly achieve 2 ×to 6×long-context extension during inference, as longer inputs lead to increasedattention entropy, limiting further extrapolation (bloc97, 2023b; Han et al., 2023; Wang et al., 2024).
Rule of Monotonicity Since the pre-trained position information of dimensions beyond the criticaldimension limits the extrapolation capability of RoPE-based auto-regressive LLMs, if the rotary baseis reduced, and each dimension can cover at least half or even a full period, the perplexity curve ofauto-regressive LLMs will be flattened (Liu et al., 2023b). However, this does not imply real lengthextrapolation. Subsequent studies (Men et al., 2024; Hu et al., 2024) find that such LLMs can onlyperceive local information in downstream evaluations and fail to retrieve long-context dependencies.
Exposing LLM to periodic position information leads to downstream degeneration, manifesting asliding-window effect, which reveals another aspect of RoPE-based extrapolation, the impact ofmonotonicity. Although higher dimensions do not observe complete position information, theyprovide a relatively complete monotonic interval, reflecting partial ordering in long-context scenarios.
These dimensions exhibit larger activation values in long-context tasks (Jin et al., 2025), are moresensitive to modeling long-context dependencies (Liu et al., 2024a), and are better suited for capturingsequential relationships (Wei et al., 2025). Thus, solely optimizing for periodicity at the cost of losingmonotonicity across all dimensions is wrong (Men et al., 2024; Liu et al., 2025).
B M ORE EXPERIMENT RESULTS B.1 S ETUP Since we have clarified the RoPE-based extrapolation in auto-regressive LLM, we now turn ourfocus to that in diffusion LLM and try to answer the three questions raised in Section 1. We conductexperiments on the existing diffusion LLM series, including LLaDA-8B (Nie et al., 2025), LLaDA1.5 (Zhu et al., 2025), and Dream-v0 (Ye et al., 2025). By default, we set the number of sampling SII-OpenMOSS 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 51.49 020406080100 Score Average Depth Score (a) Pretrained LLaDA-1.5 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 61.71 020406080100 Score Average Depth Score (b) LLaDA-1.5 with λ= 4 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 69.56 020406080100 Score Average Depth Score (c) LLaDA-1.5 with λ= 14 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 68.51 020406080100 Score Average Depth Score (d) LLaDA-1.5 with λ= 31 Figure 8: NIAH Results of LLaDA-1.5 (Zhu et al., 2025) with different λ.
steps in diffusion LLM to 32 and keep the sampling strategy in the official code of LLaDA1and Dream2. We use OpenCompass (Contributors, 2023) for validation. All experiments are performedwith a fixed random seed of 2025, FP16 precision, and accelerated with FlashAttention2 (Dao, 2023).
B.2 M ORE RESULTS OF NEEDLE -IN-A-H AYSTACK We report the NIAH results of LLaDA-8B-Instruct (Nie et al., 2025) and LLaDA (Zhu et al., 2025) with different sampling steps sin Figure 11 and Figure 12 respectively, similar to Figure 3. In Figure 8, we report the NIAH results of pre-trained and NTK-scaled LLaDA-1.5 (Zhu et al., 2025).
LLaDA-1.5 still supports 4k context length and has a local perception in direct length extrapolation.
We use the same scaling factors as LLaDA and achieve similar length extrapolation effects consistentwith the prediction of the scaling law of RoPE-based length extrapolation (Liu et al., 2023b).
In Figure 9 and Figure 10, we also report the NIAH performance of Dream-v0-7B-Base and Dream-v0-7B-Instruct. We find that both diffusion LLMs still have a local perception in direct extrapolation, but only support context length around 2k. We recalculate the scaling factor λfor Dream-v0 in 4k, 8k, and 16k, respectively, based on Equation 1. Though the long-context capability of Dream-v0 Series ispoor, we still find the NTK scaling works effectively in a 4k context length. Besides, since Dream-v0 Series is trained not from scratch but initialized from auto-regressive LLM, Qwen2.5-7B (Ye et al., 2025), we do not compare them with auto-regressive LLMs in task characteristics.
1https://github.com/ML-GSAI/LLaDA 2https://github.com/HKUNLP/Dream SII-OpenMOSS 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 40.73 020406080100 Score Average Depth Score (a) Pretrained Dream-v0-7B-Base 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 45.30 020406080100 Score Average Depth Score (b) Dream-v0-7B-Base with λ= 5 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 44.07 020406080100 Score Average Depth Score (c) Dream-v0-7B-Base with λ= 25 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 34.52 020406080100 Score Average Depth Score (d) Dream-v0-7B-Base with λ= 126 Figure 9: NIAH Results of Dream-v0-7B-Base (Ye et al., 2025) with different λ.
2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 44.66 020406080100 Score Average Depth Score (a) Pretrained Dream-v0-7B-Instruct 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 45.29 020406080100 Score Average Depth Score (b) Dream-v0-7B-Instruct with λ= 5 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 44.97 020406080100 Score Average Depth Score (c) Dream-v0-7B-Instruct with λ= 25 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 36.75 020406080100 Score Average Depth Score (d) Dream-v0-7B-Instruct with λ= 126 Figure 10: NIAH Results of Dream-v0-7B-Instruct (Ye et al., 2025) with different λ.
SII-OpenMOSS 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 24.57 020406080100 Score Average Depth Score (a) LLaDA-8B-Instruct with s= 1 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 37.59 020406080100 Score Average Depth Score (b) LLaDA-8B-Instruct with s= 4 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 50.28 020406080100 Score Average Depth Score (c) LLaDA-8B-Instruct with s= 8 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 51.27 020406080100 Score Average Depth Score (d) LLaDA-8B-Instruct with s= 16 Figure 11: NIAH Results of LLaDA-8B-Instruct (Nie et al., 2025) with different sampling steps.
2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 22.57 020406080100 Score Average Depth Score (a) LLaDA-1.5 with s= 1 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 36.83 020406080100 Score Average Depth Score (b) LLaDA-1.5 with s= 4 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 49.52 020406080100 Score Average Depth Score (c) LLaDA-1.5 with s= 8 2000 4000 8000 16000 24000 32000 T oken Limit0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0Depth PercentOverall Score: 50.49 020406080100 Score Average Depth Score (d) LLaDA-1.5 with s= 16 Figure 12: NIAH Results of LLaDA-1.5 (Zhu et al., 2025) with different sampling steps.
