---
title: 'Bridging Offline and Online Reinforcement Learning for LLMs'
source: https://arxiv.org/html/2506.21495v1
author:
  - arxiv.org
published: ''
fetched: '2025-06-27T11:25:13.102148+00:00'
tags:
  - codex
  - arxiv
image: 
---

## è¦ç´„

æœ¬è«–æ–‡ã§ã¯ã€äº‹å‰åé›†ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’DPOã¨ã€ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚’éšæ™‚åˆ©ç”¨ã™ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ã‚’çµ„ã¿åˆã‚ã›ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’æ¤œè¨¼ã™ã‚‹ã€‚NuminaMathã‚„WildChatã¨ã„ã£ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å˜ç‹¬ã‚ˆã‚Šã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã¾ãŸã¯ã‚»ãƒŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãŒè‘—ã—ã„ç²¾åº¦å‘ä¸Šã‚’ç¤ºã—ã€ç‰¹ã«éæ¤œè¨¼å‹ã‚¿ã‚¹ã‚¯ã§ã‚‚æœ‰ç”¨ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºã‹ã‚ãŸã€‚å¿œç­”é•·ã®åã‚Šã‚„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä½ä¸‹ãªã©ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç‰¹æœ‰ã®å‰¯ä½œç”¨ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€åŒæœŸé–“éš”ã®èª¿æ•´ã‚„æå¤±é–¢æ•°ã®æ‹¡å¼µã‚’è©¦ã¿ã‚‹ã€‚ã•ã‚‰ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®å ±é…¬ä¿¡å·ã‚’åŒæ™‚ã«æ‰±ã†ã“ã¨ã§ã€åŒæ–¹ã®ã‚¿ã‚¹ã‚¯ã§å®‰å®šã—ã¦é«˜ã„æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹ã¨å ±å‘Šã—ã¦ãŠã‚Šã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¸ã®ç§»è¡ŒãŒä¸å¯æ¬ ã§ã‚ã‚‹ã¨çµè«–ä»˜ã‘ã¦ã„ã‚‹ã€‚

## æœ¬æ–‡

### 4.1 Main Results

**Verifiable math**
Â [TableÂ 1](https://arxiv.org/html/2506.21495v1#S3.T1 "Table 1 â€£ 3.2 Verifiable math problems â€£ 3 Experimental Setup â€£ Bridging Offline and Online Reinforcement Learning for LLMs") shows math evaluation results for the different training regimes on the NuminaMath training set. The offline DPO training improves performance across all benchmarks compared to the seed model. However, we see substantial gains when training in online, or semi-online regimes.
We observe several important trends. First, online and semi-online trained models (sâ‰¥1)ğ‘ 1(s\geq 1)( italic\_s â‰¥ 1 ) all outperform the offline DPO model (s=âˆ)ğ‘ (s=\infty)( italic\_s = âˆ ) by a wide margin. This highlights the limitation of offline training and the importance of training on responses generated by an updated model.
Second, we notice the effectiveness of operating in a semi-online setting with (s>1)ğ‘ 1(s>1)( italic\_s > 1 ) for DPO, which performs very similarly to completely online DPO (s=1)ğ‘ 1(s=1)( italic\_s = 1 ).
This is an important finding indicating that pure online training might not be necessary.
We find that online DPO marginally outperforms GRPO. Lastly, we experiment with different numbers of responses in GRPO and report results in Appendix [TableÂ 5](https://arxiv.org/html/2506.21495v1#A3.T5 "Table 5 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs"), where scaling it beyond 8888 did not boost performance further.

Table 2: **Non-Verifiable Task Evaluations.
We show winrate with standard error for length-controlled AlpacaEval, and ArenaHard scores with 95% confidence intervals. Similar to verifiable tasks, both semi-online and online DPO show the best performance, closely followed by GRPO. We show results using two judges: GPT-4-1106 and GPT-4o. While GPT-4o gives overall lower winrates, we see general relative agreement between the two judges.**

**Non-verifiable instruction following**
[TableÂ 2](https://arxiv.org/html/2506.21495v1#S4.T2 "Table 2 â€£ 4.1 Main Results â€£ 4 Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs") compares the performance of different models training on WildChat prompts with the Athene-RM-8B reward model. We show AlpacaEval-2.0 Length-Controlled (LC) winrates and ArenaHard scores.
We observe improvements over the baseline seed model in all training regimes: offline, semi-online, and online. However, again semi-online and online methods significantly outperform the offline DPO results. For example, averaged across both judges, Online DPO results in a 56.6% increase in AlpacaEval LC winrate and 45.6% increase in ArenaHard score compared to the commonly used offline DPO.

Similar to the verifiable task setting, online DPO results in slightly higher performance compared to GRPO. Hence both settings emphasize the importance of online and semi-online training methods compared to offline. For semi-online DPO, we test smaller semi-online synchronization step sizes s={5,10}ğ‘ 510s=\{5,10\}italic\_s = { 5 , 10 } because 32 steps is already a full data epoch, and we find s=100ğ‘ 100s=100italic\_s = 100 to be too unstable with our non-verifiable hyperparameters. We find similar performance between semi-online and online, reiterating the effectiveness of sync step sizes that we observed in the verifiable task. While it is possible that there is some reward hacking with the Athene-RM-8B reward model via response length (seeÂ [SectionÂ 4.2](https://arxiv.org/html/2506.21495v1#S4.SS2 "4.2 Additional Experiments and Observations â€£ 4 Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs")), our results demonstrate robust performance on two commonly used instruction following benchmarks that are highly correlated with human preferences and control for common reward hacking such as length and style.

**Combining verifiable and non-verifiable**
Finally, we analyze the effectiveness of training a model with both verifiable and non-verifiable tasks in the training set.
Given the strong performance results in the individual verifiable and non-verifiable tasks, and due to computational resource constraints, we only consider online DPO training in this setting.
[TableÂ 3](https://arxiv.org/html/2506.21495v1#S4.T3 "Table 3 â€£ 4.1 Main Results â€£ 4 Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs") shows the results of the combined dataset models compared to training on individual verifiable or non-verifiable tasks.
First, we see that the â€œcrossâ€ task performance, training on only verifiable and testing on non-verifiable or vice versa, results in either a decrease in performance or marginal improvement compared to the seed Llama-3.1-8B-Instruct model, i.e. there is no transfer.
However, we observe significant improvements on non-verifiable tasks when starting from either a WildChat or NuminaMath checkpoint and finetuning on the opposite training set. Notably, even when starting from a checkpoint trained on 1k WildChat and finetuning on 100k NuminaMath samples, we still see gains on non-verifiable evals. We hypothesize that since AlpacaEval and ArenaHard contain â€œverifiableâ€ prompts such as math and code, it is critical to incorporate some verifiable signal during training as opposed to only using an LLM-based reward signal. When starting from the NuminaMath checkpoint and finetuning on WildChat, we see a significant decrease in math performance as the model starts to optimize for the LLM-based reward.
Lastly, we see performance gains across both verifiable and non-verifiable tasks when starting from the seed model and training on both reward signals. Performance is comparable to training on each task individually, with slight improvements in the non-verifiable evaluations, demonstrating that not only is it possible to combine rewards during training, it can also help improve performance in certain tasks.

Table 3: **Combined Verifiable + Non-Verifiable Evaluations. We first show the â€œcrossâ€ task evaluations, when training a model on either the NuminaMath (NM) task only or the WildChat (WC) task only, where we see poor cross task transfer. We then show three separate models trained on both task rewards either starting from a checkpoint trained on the opposite task, or training on both at once.
We observe better results with the combined task models across all four datasets than any individual-task model. We show accuracy with standard error for MATH500 and AMC23, winrate with standard error for length-controlled AlpacaEval, and ArenaHard scores with 95% confidence intervals.**

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  |  |  | **Verifiable** | | **Non-verifiable (GPT-4o Judge)** | |
| **Seed** | **Training** | **Dataset** | **MATH500** | **AMC23** | **AlpacaEval LC** | **ArenaHard** |
| Llama-3.1-70B-Instr. | - | - | 67.2 (1.4) | 46.6 (4.3) | 43.5 (1.6) | 56.4 (-2.3, 2.4) |
| Llama-3.1-8B-Instr. | - | - | 47.4 (1.6) | 23.7 (5.2) | 32.0 (1.6) | 27.8 (-2.1, 1.8) |
| Llama-3.1-8B-Instr. | Online DPO | NM only | **58.7** (1.2) | **32.9** (5.2) | 36.2 (1.6) | 34.9 (-2.2, 2.2) |
| Llama-3.1-8B-Instr. | Online DPO | WC only | 35.0 (1.6) | 15.0 (4.3) | 62.8 (1.5) | 50.4 (-1.7, 1.9) |
| WC-Checkpoint | Online DPO | NM only | 54.7 (1.2) | 30.4 (5.5) | 71.9 (1.5) | **62.3** (-2.1, 2.4) |
| NM-Checkpoint | Online DPO | WC only | 33.3 (1.6) | 13.2 (4.9) | **78.8** (1.6) | 61.2 (-2.0, 2.3) |
| Llama-3.1-8B-Instr. | Online DPO | NM+WC | 57.3 (1.4) | 31.7 (6.0) | 65.6 (1.3) | 57.1 (-2.3, 3.0) |

![Refer to caption](https://arxiv.org/html/2506.21495v1/x2.png)

Figure 2: Without syncing the reference model, response lengths of online DPO collapse when trained on verifiable tasks (left). This length collapse is also correlated with lower validation reward (right).

![Refer to caption](https://arxiv.org/html/2506.21495v1/x3.png)

Figure 3: **Logit entropy collapse in iterative and online training on verifiable tasks. Despite stable average length of rollouts during training (right), the average entropy of the next token distribution (left) decreases significantly during the training in all training regimes except the offline one.**

### 4.2 Additional Experiments and Observations

**Response length**
Although past work has found that both offline and online post-training methods tend to encourage longer answers (Park etÂ al., [2024a](https://arxiv.org/html/2506.21495v1#bib.bib30); Singhal etÂ al., [2024](https://arxiv.org/html/2506.21495v1#bib.bib40); Guo etÂ al., [2025](https://arxiv.org/html/2506.21495v1#bib.bib10)), we encounter both length increase and decrease in our training.
In the verifiable task, for example, we observe that disabling reference model sync and increasing training speed lead to greater risk of response length collapse and performance degradation ([FigureÂ 2](https://arxiv.org/html/2506.21495v1#S4.F2 "Figure 2 â€£ 4.1 Main Results â€£ 4 Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs")). We hypothesize that the bimodal distribution of response lengths (one peak with very short responses, and one with very long responses) is a major contributor to this collapse ([FigureÂ 9](https://arxiv.org/html/2506.21495v1#A3.F9 "Figure 9 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs")).

On the other hand, we observe tendencies towards response length increase in the non-verifiable reward experiments. Since we are using an off-the-shelf LLM reward model, the model tends to hack its length bias to maximize rewards (Singhal etÂ al., [2024](https://arxiv.org/html/2506.21495v1#bib.bib40)). Therefore, the response lengths generally increase over time in the online or semi-online settings ([FigureÂ 7](https://arxiv.org/html/2506.21495v1#A3.F7 "Figure 7 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs"), right). There are several methods to mitigate this: creating or finetuning a reward model for less length bias, incorporating a length penalty in the loss (Wu etÂ al., [2024b](https://arxiv.org/html/2506.21495v1#bib.bib45); Park etÂ al., [2024b](https://arxiv.org/html/2506.21495v1#bib.bib31)), or selecting checkpoints by normalizing for length. For simplicity across all experiments, we choose the last option and find that this selection method generalizes well.

**Entropy collapse and regularization**
We measure the entropy of the next token distribution averaged over all tokens sampled in rollouts in both DPO and GPRO experiments. [FigureÂ 3](https://arxiv.org/html/2506.21495v1#S4.F3 "Figure 3 â€£ 4.1 Main Results â€£ 4 Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs") shows substantial entropy collapse regardless of algorithm in the verifiable task, except for offline DPO.
It is possible that offline DPO training is also reducing entropy, but it is not detected here as the measurement is on the rollouts that are not generated from the current model.
Non-verifiable tasks, however, exhibit less collapse as training continues ([FigureÂ 7](https://arxiv.org/html/2506.21495v1#A3.F7 "Figure 7 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs"), left). This may be due to both the task properties (*i.e.*, gradual improvements, non-binary rewards) and the use of a model-based reward.

We experiment with entropy regularization in the verifiable task to mitigate the entropy collapse in DPO. The average negative entropy of next token distribution is added to the training objective with a configurable coefficient. Empirical results in multiple levels of scale reveal that maintaining stable entropy throughout online training is a non-trivial task, demonstrated in [FigureÂ 6](https://arxiv.org/html/2506.21495v1#A3.F6 "Figure 6 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs"), and requires further investigation which we leave for future work.

**Experiments with the loss**
Prior work reports benefits of adding an NLL loss over the chosen response ycsubscriptğ‘¦ğ‘y\_{c}italic\_y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT to iterative DPO training (Pang etÂ al., [2024](https://arxiv.org/html/2506.21495v1#bib.bib29); Hong etÂ al., [2024](https://arxiv.org/html/2506.21495v1#bib.bib13)). We experiment with adding the NLL term to our online and semi-online DPO configurations in the verifiable task. We did not observe any benefits after adding the extra term ([FigureÂ 10](https://arxiv.org/html/2506.21495v1#A3.F10 "Figure 10 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs")). We explain this observation with the fact that chosen log probabilities do not substantially decrease during training in our case, which was one of the motivations for adding the NLL term in previous works.

While GRPO trains on all generated responses, DPO only utilizes a pair of responses.
In an attempt at improving utilization of the set of generated responses in DPO training, we propose to pair each of the correct responses ğ’´csubscriptğ’´ğ‘\mathcal{Y}\_{c}caligraphic\_Y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT with each the incorrect responses ğ’´rsubscriptğ’´ğ‘Ÿ\mathcal{Y}\_{r}caligraphic\_Y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT in the verifiable task, thus making a group of preference pairs. We then average DPO losses off all pairs to compute the GroupDPO loss:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’GroupDPOâ¢(x,ğ’´c,ğ’´r)=1|ğ’´c|â‹…|ğ’´r|â¢âˆ‘ycâˆˆğ’´câˆ‘yrâˆˆğ’´râ„’DPOâ¢(x,yc,yr)subscriptâ„’GroupDPOğ‘¥subscriptğ’´ğ‘subscriptğ’´ğ‘Ÿ1â‹…subscriptğ’´ğ‘subscriptğ’´ğ‘Ÿsubscriptsubscriptğ‘¦ğ‘subscriptğ’´ğ‘subscriptsubscriptğ‘¦ğ‘Ÿsubscriptğ’´ğ‘Ÿsubscriptâ„’DPOğ‘¥subscriptğ‘¦ğ‘subscriptğ‘¦ğ‘Ÿ\displaystyle\mathcal{L}\_{\text{GroupDPO}}(x,\mathcal{Y}\_{c},\mathcal{Y}\_{r})=% \frac{1}{|\mathcal{Y}\_{c}|\cdot|\mathcal{Y}\_{r}|}\sum\_{y\_{c}\in\mathcal{Y}\_{c}% }\sum\_{y\_{r}\in\mathcal{Y}\_{r}}\mathcal{L}\_{\text{DPO}}(x,y\_{c},y\_{r})caligraphic\_L start\_POSTSUBSCRIPT GroupDPO end\_POSTSUBSCRIPT ( italic\_x , caligraphic\_Y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT , caligraphic\_Y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT ) = divide start\_ARG 1 end\_ARG start\_ARG | caligraphic\_Y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT | â‹… | caligraphic\_Y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT | end\_ARG âˆ‘ start\_POSTSUBSCRIPT italic\_y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT âˆˆ caligraphic\_Y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT âˆ‘ start\_POSTSUBSCRIPT italic\_y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT âˆˆ caligraphic\_Y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT caligraphic\_L start\_POSTSUBSCRIPT DPO end\_POSTSUBSCRIPT ( italic\_x , italic\_y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT , italic\_y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT ) |  | (4) |

We experimented with GroupDPO in the verifiable task setup, as presented inÂ [FigureÂ 8](https://arxiv.org/html/2506.21495v1#A3.F8 "Figure 8 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs"), and did not observe substantial changes in the performance compared with the model using a single preference pair chosen randomly.

Both group DPO and GRPO can learn from all responses in an online manner, but using very different loss functions.
This begs the question whether these two losses can be combined.
We implemented this loss as â„’combined=â„’GroupDPOâ¢(x,ğ’´c,ğ’´r)+Î±â¢â„’GRPOâ¢(x,ğ’´câˆªğ’´r)subscriptâ„’combinedsubscriptâ„’GroupDPOğ‘¥subscriptğ’´ğ‘subscriptğ’´ğ‘Ÿğ›¼subscriptâ„’GRPOğ‘¥subscriptğ’´ğ‘subscriptğ’´ğ‘Ÿ\mathcal{L}\_{\text{combined}}=\mathcal{L}\_{\text{GroupDPO}}(x,\mathcal{Y}\_{c},%
\mathcal{Y}\_{r})+\alpha\mathcal{L}\_{\text{GRPO}}(x,\mathcal{Y}\_{c}\cup\mathcal%
{Y}\_{r})caligraphic\_L start\_POSTSUBSCRIPT combined end\_POSTSUBSCRIPT = caligraphic\_L start\_POSTSUBSCRIPT GroupDPO end\_POSTSUBSCRIPT ( italic\_x , caligraphic\_Y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT , caligraphic\_Y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT ) + italic\_Î± caligraphic\_L start\_POSTSUBSCRIPT GRPO end\_POSTSUBSCRIPT ( italic\_x , caligraphic\_Y start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT âˆª caligraphic\_Y start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT ) and train on the verifiable task. We compare the results against semi-DPO and online group DPO inÂ [FigureÂ 8](https://arxiv.org/html/2506.21495v1#A3.F8 "Figure 8 â€£ Appendix C Additional Experimental Results â€£ Bridging Offline and Online Reinforcement Learning for LLMs") and find no substantial difference in reward or entropy.
All relevant hyper-parameters used in additional experiments are provided in Appendix [SectionÂ B.2](https://arxiv.org/html/2506.21495v1#A2.SS2 "B.2 Additional experiments hyperparameters â€£ Appendix B Training Details â€£ Bridging Offline and Online Reinforcement Learning for LLMs").
