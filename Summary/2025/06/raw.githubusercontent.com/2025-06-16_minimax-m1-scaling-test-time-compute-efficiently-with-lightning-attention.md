---
title: 'MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention'
source: https://raw.githubusercontent.com/MiniMax-AI/MiniMax-M1/main/MiniMax_M1_tech_report.pdf
author:
  - Aili Chen
  - Aonian Li
  - Bangwei Gong
  - Binyang Jiang
  - Bo Fei
  - MiniMax AI
published: '2025-06-16T10:05:37Z'
fetched: '2025-06-16T16:28:23.069879+00:00'
tags:
  - codex
  - ai
  - llm
  - mixture-of-experts
  - reinforcement-learning
  - research
image: 
---

## 要約

MiniMax社が公開したMiniMax-M1は世界初のオープンウェイト大型推論モデルで、Mixture-of-ExpertsとLightning Attentionを組み合わせ、1Mトークンの長いコンテキストを効率的に処理できる。新アルゴリズムCISPOによりREINFORCEのIS重みをクリップしつつ高速なRL訓練を実現し、512 GPUで3週間の低コスト学習に成功。DeepSeek-R1の25%の計算量で100Kトークン生成を達成し、複雑なソフトウェア開発やツール利用、長文タスクで優れた性能を示した。さらに公開ベンチマークでも既存のオープンモデルを上回る結果を報告しており、推論プロセスの拡張と効率化の両立を証明した。

## 本文

MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention MiniMax1 We introduce MiniMax-M1, the world’s first open-weight, large-scale hybrid-attention reasoning model.
MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightningattention mechanism. The model is developed based on our previous MiniMax-Text-01 model (MiniMaxet al., 2025), which contains a total of 456 billion parameters with 45.9 billion parameters activatedper token. The M1 model natively supports a context length of 1 million tokens, 8x the context sizeof DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficientscaling of test-time compute – For example, compared to DeepSeek R1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1 particularly suitable forcomplex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained usinglarge-scale reinforcement learning (RL) on diverse problems ranging from traditional mathematicalreasoning to sandbox-based, real-world software engineering environments. In addition to the inherentefficiency advantage of lightning attention for RL training, we propose CISPO, a novel RL algorithm tofurther enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMaxM1’s full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. WereleasetwoversionsofMiniMax-M1modelswith40Kand80Kthinkingbudgetsrespectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standardbenchmarks show that our models are comparable or superior to strong open-weight models such asthe original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. Through efficient scaling of test-time compute, MiniMax-M1serves as a strong foundation for next-generation language model agents to reason and tackle real-worldchallenges. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.
AIME 2024 LiveCodeBench SWE-bench Verified TAU-bench MRCR (4-needle)020406080100Accuracy (%)86.0 65.0 56.062.873.4Closed-weight Models OpenAI o3 Gemini-2.5 ProClaude 4 Opus Seed-Thinking-v1.5Open-weight Models DeepSeek-R1 DeepSeek-R1-0528Qwen3-235B-A22B MiniMax-M1 0 32K 64K 96K 128K Generation Length01234567FLOPs (×10¹ ) DeepSeek R1 Qwen3-235B-A22B MiniMax-M1 Figure 1|Left: Benchmark performance comparison of leading commercial and open-weight modelsacrosscompetition-levelmathematics,coding,softwareengineering,agentictooluse,andlong-contextunderstanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1. Right: Theoreticalinference FLOPs scaling with generation length (# tokens).
1Please send correspondence to model@minimax.io.
©2025 MiniMax. All rights reserved MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention 1. Introduction Large reasoning models (LRMs), such as OpenAI o1 (OpenAI, 2024a) and DeepSeek-R1 (DeepSeek-AIet al., 2025), have demonstrated remarkable success by extending the length of reasoning throughlarge-scale reinforcement learning (RL). In recent months, both the open-source community andcommercial organizations have followed this trend, achieving significant advances on complex taskssuch as Olympiad mathematics competitions and competitive programming (Anthropic, 2025; Google DeepMind, 2025; Hu et al., 2025; Kimi Team, 2025; Seed et al., 2025; Yu et al., 2025; Zeng et al., 2025). The success of LRMs has been primarily attributed to a new scaling dimension of test-timecompute—As more FLOPs are dedicated to extended reasoning processes during generation, modelperformanceshowsconsistentimprovement, particularlyforcomplexreal-worldapplications(Jimenezet al., 2024; OpenAI, 2025).
However, continuously extending the reasoning process is challenging within the traditional transformer architecture (Vaswani et al., 2017), due to the inherent quadratic computational complexityof the softmax attention mechanism. While previous works have proposed various techniques tomitigate this issue—such as sparse attention (Beltagy et al., 2020; Lu et al., 2025; Yuan et al., 2025; Zaheer et al., 2020), linear attention (Arora et al., 2024; Choromanski et al., 2021; Du et al., 2025; He et al., 2024; Katharopoulos et al., 2020; Peng et al., 2024b, 2021; Qin et al., 2021, 2022a,b, 2024a,c; Shen et al., 2024; Sun et al., 2025, 2023; Zhang et al., 2024), linear attention with deltadecay (Peng et al., 2025; Yang et al., 2024a,b), state space models (Dao and Gu, 2024; Gloriosoet al., 2024; Gu and Dao, 2024; Gu et al., 2020, 2022, 2023; Gupta et al., 2022; Jamba Team, 2024; Ren et al., 2024), and linear RNNs (Behrouz et al., 2024; Chou et al., 2024; Chung and Ç, 2014; Hochreiter and Schmidhuber, 1997; Martin and Cundy, 2018; Peng et al., 2023, 2024a; Qin et al., 2023, 2024d; Siems et al., 2025; Sun et al., 2024; von Oswald et al., 2025)—these approaches havenot been fully validated in large-scale reasoning models, and nearly all competitive LRMs to date stillrely on traditional attention designs. An exception is the Hunyuan-T1 model (Tencent AI Lab, 2025) that employs the Mamba architecture (Dao and Gu, 2024; Gu and Dao, 2024). However, this model isnot open-sourced and few details are disclosed. In this work, we aim to build and open-source a largereasoning model that can efficiently scale up test-time compute and compete with the state-of-the-artreasoning models.
We introduce MiniMax-M1, a reasoning model with a hybrid Mixture-of-Experts (MoE) architecture and Lightning Attention (Qin et al., 2024b), an I/O-aware implementation of a linear attentionvariant (Qin et al., 2022a). MiniMax-M1 is developed based on our previous MiniMax-Text-01 (MiniMax et al., 2025) model, and comprises 456 billion parameters in total, with 45.9 billion activationsand 32 experts. In our attention design, a transformer block with softmax attention follows everyseven transnormer blocks (Qin et al., 2022a) with lightning attention. This design theoreticallyenables efficient scaling of reasoning lengths to hundreds of thousands of tokens, as illustrated in Figure 1 (Right). For example, compared to DeepSeek R1, M1 consumes less than 50% of the FLOPsat a generation length of 64K tokens, and approximately 25% of the FLOPs at a length of 100K tokens.
This substantial reduction in computational cost makes M1 significantly more efficient during bothinference and large-scale RL training. Furthermore, owing to its lightning attention mechanism and inline with MiniMax-Text-01, our M1 model natively supports a context length of up to 1 million tokens – eight times the context size of DeepSeek R1 and an order of magnitude greater than all open-weight LRMs available to date. These features make M1 particularly well-suited for addressing complex, real-world tasks that require processing long inputs and generating extended thinking. A comparisonof the maximum input and output lengths of M1 and other leading models is demonstrated in Table 1.
To develop our M1 model, we first continue pretraining MiniMax-Text-01 on 7.5T tokens from acarefully curated, reasoning-intensive corpus. Subsequently, we perform supervised fine-tuning (SFT) MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Table 1|The maximum supported input length and output length (# tokens) of different reasoning models. For Claude-4 we refer to the Claude-4-Opus model. “DS-R1” represents the latest DeepSeek-R1-0528 model.
o3 Gemini 2.5 Pro Claude 4 DS-R1 Qwen3-235B MiniMax-M1-80k Max Input 200K 1M 200K 128K 128K 1M Max Output 100K 64K 32K 64K 32K 80Kto inject certain chain-of-thought (CoT) (Wei et al., 2022) patterns, establishing a strong foundationfor reinforcement learning, the core stage of M1 development. Notably, our RL scaling with M1 ismade efficient through innovations from two key perspectives: (1) We propose a novel RL algorithm, CISPO, which abandons the trust region constraint and instead clips the importance sampling weightsto stabilize training. This approach always leverages all tokens for gradient computations, achievingenhanced efficiency compared to GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025) empirically – For example, on a controlled study based on Qwen2.5-32B models (Qwen et al., 2025), CISPOachieves a 2x speedup compared to DAPO; (2) Although the hybrid-attention design in M1 naturallyallows for efficient RL scaling, unique challenges arise when scaling RL with this architecture. Forinstance, we find a precision mismatch between the training and inference kernels of our architecture, which prevents reward growth during RL training. We develop targeted solutions to address thesechallenges and successfully scale up RL with this hybrid architecture. In the end, our efficient RLframework enables us to complete a full RL run of MiniMax-M1 within 3 weeks using 512 H800 GPUs—equivalent to a rental cost of approximately $0.53M USD.
In addition to methodological innovations, we curate a diverse set of problems and environmentsfor RL training. Our data encompasses both verifiable and non-verifiable problems. For verifiableproblemsthataretypicallyconsideredcriticalforreasoninglearning,wenotonlyincludemathematicalreasoning and competitive programming problems as commonly used in related works, but alsoleverage our previous data synthesis framework SynLogic (Liu et al., 2025a) to generate diverselogical reasoning problems spanning 41 distinct tasks. Furthermore, we construct sandboxes forcomplexsoftwareengineering(SE)environmentsderivedfromSWE-bench(Jimenezetal.,2024),andconduct RL on real-world SE problems with execution-based rewards to improve M1’s performance inchallenging SE scenarios. Our unverifiable problems span a broad range of domains such as questionanswering and creative writing, where we use generative reward models to provide the feedback.
We train two versions of MiniMax-M1 models with 40K and 80K tokens of maximum generationlength respectively, which leads to two models MiniMax-M1-40k and MiniMax-M1-80k. MiniMax-M180k outperforms MiniMax-M1-40k on complex mathematical and coding tasks, further demonstratingthe benefits of scaling test-time compute. As shown in Figure 1 (Left), MiniMax-M1 surpasses previousleading open-weight models such as the original DeepSeek-R1 and Qwen-235B overall, with particularadvantages in complex software engineering, tool-using, and long-context tasks. Compared to thelatest DeepSeek-R1-0528 model, MiniMax-M1 lags in mathematical and coding competitions butachieves comparable or superior performance in more realistic tool-using and long-context scenarios.
Notably, MiniMax-M1 outperforms Gemini 2.5 Pro on the agentic tool use benchmark TAU-Bench (Yaoetal.,2025), andsurpassesOpenAIo3andClaude4Opusonlong-contextunderstandingbenchmarks.
With efficient test-time scaling, we contend that MiniMax-M1 establishes a strong foundation fornext-generation language model agents to address real-world challenges.
To facilitate collaboration and advancement in the field, we have made our models publiclyavailableatGitHubandHuggingFace. Theyarenowsupportedbyboththe vLLMandTransformersframeworks, with detailed deployment guides available at vLLM and Transformers respectively. This MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attentionenables easy integration of MiniMax-M1 into modern inference pipelines. We also provide commercialstandard API at minimax.io.
2. Preparation for Scalable RL: Continual Pretraining and SFT In this work, we focus on scaling up reinforcement learning to enhance reasoning capabilities of Minimax-Text-01. To facilitate scalable RL training, we first carry out continual pretraining of our basemodel to strengthen its intrinsic reasoning abilities. Subsequently, we perform a cold-start supervisedfine-tuning (SFT) stage to inject specific reasoning patterns to the model, thereby providing a strongerfoundation for the subsequent RL phase.
2.1. Continual Pre-Training: Foundation for RL Scaling To enhance the reasoning and long context capabilities of the foundation model while ensuringdiversity, wecontinuetrainingtheMiniMax-Text-01modelwithadditional7.5Ttokenswithoptimizeddata quality and mixture.
TrainingData. WerefineourpretrainingWebandPDFparsingmechanismsandenhanceourheuristiccleaning rules to ensure a high recall rate for mathematical and code-related data. We prioritize theextraction of natural Question-Answer (QA) pairs from a diverse range of sources, including webpages, forums, and textbooks, while strictly avoiding the use of synthetic data. Additionally, we conductsemantic deduplication on the QA data to maintain its diversity and uniqueness. Furthermore, weincrease the proportion of STEM (Science, Technology, Engineering, and Mathematics), code, book, and reasoning-related data to 70%. This significantly enhances the foundation model’s ability tohandle complex tasks without compromising its other general capabilities.
Training Recipe. We decrease the coefficient of the MoE auxiliary loss and adjust the parallel trainingstrategy to support a larger training micro batch size, which mitigates the detrimental effects of theauxiliary loss on overall model performance. Based on MiniMax-Text-01, we continue training with aconstant learning rate of 8e-5 for 2.5T tokens, followed by a decay schedule over 5T tokens down to 8e-6.
Long Context Extension. For a hybrid-lightning architecture model with higher convergence complexity, we have observed that excessively aggressive extensions of the training length can lead to asudden gradient explosion that may occur during the training process. This makes the optimizationprocess extremely challenging. We attribute this to the parameter optimization of the earlier layersnot keeping up with the changes in the later layers – For lightning attention, the earlier and laterlayers have different decay rates, which makes the earlier layers focus more on local information. Wealleviate this issue by adapting a smoother extension of context length across four stages, startingfrom a 32K context window length and ultimately extending the training context to 1M tokens.
2.2. Supervised Fine-Tuning: Focused Alignment for Efficient RL After continual pretraining, we conduct Supervised Fine-Tuning (SFT) to instill desired behaviors likereflection-based Chain-of-Thought (CoT) reasoning using high-quality examples, creating a strongstarting point for more efficient and stable RL in the next stage. Specifically, we curate data sampleswith long CoT responses. These data samples cover diverse domains such as math, coding, STEM, writing, QA, and multi-turn chat. Math and coding samples account for around 60% of all the data.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention 3. Efficient RL Scaling: Algorithms and Lightning Attention As shown in Figure 1 (Right), the M1 architecture demonstrates a clear efficiency advantage duringinference. This naturally facilitates efficient RL scaling where increasingly longer responses aregenerated. However, as pioneers in scaling up RL with this hybrid architecture, we encounter uniquechallenges during the process, and the RL procedure can become unstable or even fail due to variousissues. To address these difficulties, we develop targeted solutions that enable us to successfully scaleup RL training for M1. In addition, we propose a new RL algorithm that achieves greater RL efficiencycompared to existing methods. These dual contributions yield an efficient and scalable RL frameworkfor training M1, where the complete training cycle requires 3 weeks on 512 H800 GPUs—equivalentto a rental cost of approximately $0.53M USD. In this section, we first provide general context on RL and present our novel RL algorithm, and then describe the specific challenges we face with thehybrid architecture, along with the solutions we devise to overcome them.
3.1. Efficient RL Scaling with CISPO Background. For questions 𝑞from a datasetD, we denote 𝜋as the policy model parameterized by𝜃, and𝑜as the response generated by the policy. PPO (Schulman et al., 2017) adopts the followingobjective to optimize the policy to maximize the expected return, and a clipping operation is appliedto stabilize training:
JPPO(𝜃)=𝔼𝑞∼D,𝑜𝑖∼𝜋𝜃old(·|𝑞) " |𝑜𝑖||𝑜𝑖|∑︁ 𝑡=1min 𝑟𝑖,𝑡(𝜃)ˆ𝐴𝑖,𝑡,clip  𝑟𝑖,𝑡(𝜃),1−𝜖,1+𝜖ˆ𝐴𝑖,𝑡 −𝛽𝐷𝐾𝐿(𝜋𝜃||𝜋ref)# ,(1) where𝑟𝑖,𝑡(𝜃)=𝜋𝜃(𝑜𝑖,𝑡|𝑞,𝑜𝑖,<𝑡) 𝜋𝜃old(𝑜𝑖,𝑡|𝑞,𝑜𝑖,<𝑡)is the importance sampling (IS) weight, which is used to correct thedistribution during off-policy updates, because we use 𝜋𝜃oldto collect trajectories to update the policyvia multiple steps in a minibatch manner. While PPO requires a separate value model to compute theadvantage ˆ𝐴𝑖,𝑡, GRPO (Shao et al., 2024) eliminates the value model and defines the advantage asthe output reward relative to other responses in the group:
ˆ𝐴𝑖,𝑡=𝑅𝑖−mean({𝑅𝑗}𝐺𝑗=1) std({𝑅𝑗}𝐺𝑗=1), (2) where𝑅𝑖is the reward of the response, and 𝐺responses{𝑜𝑖}𝐺𝑖=1are sampled for each question. Thereward is either from rule-based verifiers such as in mathematical problem solving, or from a rewardmodel.
Issues of Token Clipping. In our initial experiments with the hybrid architecture under the zero-RLsetting, we observed that the GRPO algorithm adversely affected training performance and failed toeffectively promote the emergence of long CoT reasoning behaviors. Through a series of controlledablation studies, we ultimately identified the undesirable clipping operation in the original PPO/GRPOloss as the primary factor contributing to degraded learning performance. Specifically, we found thattokens associated with reflective behaviors (e.g., However ,Recheck ,Wait,Aha), which often serveas “forks” in reasoning paths, were typically rare and assigned low probabilities by our base model.
During policy updates, these tokens were likely to exhibit high 𝑟𝑖,𝑡values. As a result, these tokenswere clipped out after the first on-policy update, preventing them from contributing to subsequentoff-policy gradient updates. This issue was particularly pronounced in our hybrid-architecture modeland further hindered the scalability of reinforcement learning. These low-probability tokens, however, MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention 2x speedup Figure 2|Comparison of GRPO, DAPO, and our proposed CISPO on AIME 2024, based on Qwen2.5-32B-base. CISPO outperforms both GRPO and DAPO in terms of performance at the same number oftraining steps, and achieves comparable performance to DAPO using 50% of the training steps.
are often crucial for stabilizing entropy (Cui et al., 2025) and facilitating scalable RL (Wang et al., 2025). Although DAPO attempts to mitigate this issue by increasing the upper clipping bound (Yuet al., 2025), we found this approach to be less effective in our setup, which involved 16 rounds ofoff-policy updates per generation batch.
The CISPO Algorithm. In response, we propose a new algorithm that explicitly avoids droppingtokens, even those associated with large updates, while inherently maintaining entropy within areasonable range to ensure stable exploration. First, recall that the vanilla REINFORCE objective withcorrected distribution for offline updates is:
JREINFORCE(𝜃)=𝔼(𝑞,𝑎)∼D,𝑜𝑖∼𝜋𝜃old(·|𝑞) " |𝑜𝑖||𝑜𝑖|∑︁ 𝑡=1sg(𝑟𝑖,𝑡(𝜃))ˆ𝐴𝑖,𝑡log𝜋𝜃(𝑜𝑖,𝑡|𝑞,𝑜𝑖,<𝑡)# ,(3) where sg(·)denotes the stop-gradient operation. Rather than clipping the token updates as in PPO/GRPO, we instead clip the importance sampling weight in Eq. 3 to stabilize training. We termour approach CISPO ( ClippedIS-weightPolicyOptimization). Adopting the group relative advantagefrom GRPO and the token-level loss (Liu et al., 2025b; Yu et al., 2025), CISPO optimizes the followingobjective:
JCISPO(𝜃)=𝔼(𝑞,𝑎)∼D,{𝑜𝑖}𝐺𝑖=1∼𝜋𝜃old(·|𝑞) " 1Í𝐺𝑖=1|𝑜𝑖|𝐺∑︁ 𝑖=1|𝑜𝑖|∑︁ 𝑡=1sg(ˆ𝑟𝑖,𝑡(𝜃))ˆ𝐴𝑖,𝑡log𝜋𝜃(𝑜𝑖,𝑡|𝑞,𝑜𝑖,<𝑡)# ,(4) where ˆ𝑟𝑖,𝑡(𝜃)is the clipped IS weight:
ˆ𝑟𝑖,𝑡(𝜃)=clip 𝑟𝑖,𝑡(𝜃),1−𝜖𝐼𝑆𝑙𝑜𝑤,1+𝜖𝐼𝑆ℎ𝑖𝑔ℎ . (5) We note that without weight clipping, JCISPOreduces to the standard policy gradient objective. Inour experiments, we did not impose a lower bound on the IS weight by setting 𝜖𝐼𝑆𝑙𝑜𝑤to a large value; MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attentioninstead, we only tuned 𝜖𝐼𝑆ℎ𝑖𝑔ℎ. Although the gradient of Eq. 4 is slightly biased due to weight clipping, this approach preserves gradient contributions from all tokens, especially in long responses. CISPOproves effective in our experiments, helping reduce variance and stabilizing RL training. In addition, we utilize the dynamic sampling and length penalty techniques from Yu et al. (2025). There is no KLpenalty term in CISPO similar to other recent works (Hu et al., 2025; Yu et al., 2025).
A General Formulation. While we adopt CISPO in our experiments, here we further present aunified formulation by introducing a token-wise mask into the CISPO objective. This allows forhyperparameter tuning to control whether, and under what conditions, gradients from specific tokensshould be dropped:
Junify(𝜃)=𝔼(𝑞,𝑎)∼D,{𝑜𝑖}𝐺𝑖=1∼𝜋𝜃old(·|𝑞) " 1Í𝐺𝑖=1|𝑜𝑖|𝐺∑︁ 𝑖=1|𝑜𝑖|∑︁ 𝑡=1sg(ˆ𝑟𝑖,𝑡(𝜃))ˆ𝐴𝑖,𝑡log𝜋𝜃(𝑜𝑖,𝑡|𝑞,𝑜𝑖,<𝑡)𝑀𝑖,𝑡# .(6) The mask 𝑀𝑖,𝑡is equivalent to the mask implicitly defined in the PPO trust region:
𝑀𝑖,𝑡=  0ifˆ𝐴𝑖,𝑡>0and𝑟𝑖,𝑡(𝜃)>1+𝜖high, 0ifˆ𝐴𝑖,𝑡<0and𝑟𝑖,𝑡(𝜃)<1−𝜖low, 1otherwise.(7) This unified loss formulation can flexibly represent different clipping strategies under a commonframework.
EmpiricalValidationofCISPO. To validate the effectiveness ofCISPO, weempiricallycompare it with DAPO and GRPO in a zero-RL training setting. Specifically, we apply different RL algorithms to trainthe Qwen2.5-32B-base model on the mathematical reasoning dataset from Yu et al. (2025), and reportperformance on the AIME 2024 benchmark. As shown in Figure 2, CISPO significantly outperformsboth DAPO and GRPO with the same number of training steps. Notably, CISPO demonstrates superiortraining efficiency compared to other approaches; for example, it matches DAPO’s performance withonly 50% of the training steps.
3.2. Efficient RL Scaling with Lightning Attention – Challenges and Recipes AsshowninFigure1(Right), weemphasizethatourhybridattentioninherentlyenablesmoreefficient RL scaling compared to traditional attention designs, since rollout computation and latency are oftenthe primary bottlenecks in RL training. However, as pioneers in conducting large-scale RL experimentswith this novel architecture, we encountered unique challenges and developed targeted solutions, aswe describe below.
Computational Precision Mismatch in Generation and Training. RL training is highly sensitiveto computational precision. During our RL training, we observed a significant discrepancy in theprobabilities of rolled-out tokens between training-mode and inference-mode, as shown in Figure 3 (Left). This discrepancy arose from a precision mismatch between the training and inference kernels.
The issue was detrimental and prevented reward growth in our experiments. Interestingly, this issuedid not appear in smaller, dense models with softmax attention. Through layer-by-layer analysis, weidentified high-magnitude activations in the LM head at the output layer as the primary source oferror. To address this, we increased the precision of the LM output head to FP32, thereby realigningthe two theoretically identical probabilities, as demonstrated in Figure 3 (Right). This adjustmentimproved the correlation between training and inference probabilities from approximately 0.9x to MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention 0.0 0.2 0.4 0.6 0.8 1.0 Inference Probability0.00.20.40.60.81.0Training Probability Correlation: 0.987319MiniMax-M1 Before Fix Perfect Precision (y=x) 0.00.10.20.30.40.5 Absolute Probability Difference 0.0 0.2 0.4 0.6 0.8 1.0 Inference Probability0.00.20.40.60.81.0Training Probability Correlation: 0.997135MiniMax-M1 After Fix Perfect Precision (y=x) 0.00.10.20.30.40.5 Absolute Probability Difference Figure 3|Probability of tokens in training-mode code vs. probability of tokens in inference-modecode. Each point in the figures represents an individual token. The Pearson correlation coefficient isindicated in the figures. Theoretically, the two probabilities should be identical, and all the tokensshould be exactly on the diagonal line. Left:Correlation of the M1 model before our fix; Right:
Correlation of the M1 model after applying our fix of using FP32 precision for the LM output head.
0.99x. Notably, this correlation metric remained stable throughout training, enabling successfulreward increase.
Optimizer Hyperparameter Sensitivity. We employ the AdamW (Loshchilov and Hutter, 2019) optimizer, and inappropriate configurations of 𝛽1,𝛽2, and𝜖can lead to non-convergence duringtraining. (Molybog et al., 2023). For instance, using the default configuration from VeRL (Shenget al., 2024), where betas = (0.9, 0.999) and eps = 1e-8, can result in such issues. We have observedthat the gradient magnitudes in MiniMax-M1 training span a wide range, from 1e-18 to 1e-5, withthe majority of the gradients being smaller than 1e-14. Furthermore, the correlation between thegradients of adjacent iterations is weak. Based on this, we set 𝛽1=0.9,𝛽2=0.95, and eps=1e-15.
Early Truncation via Repetition Detection. During RL training, we found that complex promptscould induce pathologically long and repetitive responses, whose large gradients threatened modelstability. Our goal was to preemptively terminate these generation loops rather than penalize thealready repetitive text. As simple string-matching is ineffective against varied repetition patterns, wedeveloped a heuristic based on token probabilities. We observed that once a model enters a repetitivecycle, the probability for each token soars. Consequently, we implemented an early truncation rule:
generation is halted if 3,000 consecutive tokens each have a probability above 0.99. This methodsuccessfully prevents model instability and improves generation throughput by eliminating thesepathological, long-tail cases.
4. Scaling Reinforcement Learning with Diverse Data In this section, we describe the data and reward we adopted for our RL stage. We incorporate a diverseset of environments in our RL training pipeline, including tasks that can be verified by rules andgeneral tasks that need to be verified through reward models. All these environments are integratedinto the RL stage using a carefully designed curriculum.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention 4.1. Reasoning-Intensive Tasks with Rule-based Verification Below, we introduce our data that can be verified by deterministic rules. For all the following tasks, we employ rule-based final correctness as the correctness reward, complemented by a format reward.
Mathematical Reasoning. Our initial mathematical dataset comprises hundreds of thousands ofhigh-quality, competition-level problems, meticulously curated and organized from public sourcesand official mathematics competitions. These problems span a wide range of difficulty levels, eachpaired with a standard reference solution. Our data cleaning pipeline begins with the removal ofincomplete samples and those exhibiting formatting or typographical errors. We subsequently applyembedding-based deduplication across the RL data sources and enforce a strict separation from the SFT dataset to avoid any overlap, as leakage from the SFT phase into the RL stage hinders explorationand undermines training effectiveness. Additionally, we employ both n-gram and embedding-basedmethodstoeliminatepotentialcontaminationfromcommonlyusedmathematicalbenchmarktestsets, therebyensuringtheintegrityandfairnessofourevaluations. Wefilteroutsamplescontainingmultiplesub-problems, proof-based questions, and binary questions (e.g., true/false) that are susceptible torandom guessing. Multiple-choice questions are reformulated into open-ended formats to betteralign with our reinforcement learning framework. Next, we employ our internal model to extractthe final answers from the reference solution, retaining only those samples whose extracted answerscan be correctly parsed by our rule-based answer checker. Finally, we use a strong reasoning modelto compute the pass@10 for each question and retain only those samples with a pass rate strictlybetween 0 and 0.9, resulting in a curated dataset of nearly 50K high-quality mathematical samplesfor our RL training.
Logical Reasoning. For logical reasoning data, we carefully select 41 logical reasoning tasks requiringnon-trivialreasoningabilitysuchascipherandSudoku,thenweimplementadatasynthesisframeworkto synthesize all the data. Concretely, we utilize our SynLogic framework (Liu et al., 2025a) toimplement the data synthesis pipeline featuring task-specific data generators and rule-based taskspecific verifiers, enabling automatic logical data generation. We meticulously configure the difficultyparameters during generation, ensuring the appropriate learning challenge of the generated data.
Specifically, to prevent inclusion of overly difficult instances, we establish an upper difficulty boundbased on the solvability limits of current strong reasoning models, requiring their pass@10 ratesgreater than zero. Similarly, we set a lower difficulty bound using the lowest difficulty parameters forwhich the MiniMax-Text-01 model achieves pass rates between 0 and 0.5. This approach ensures thedata maintains a balance between difficulty and learnability. In addition, as the model capabilitiesimproveduringtraining, weincreasethedifficultyofthedatainthelaterstages. Usingthisframework, we synthesize approximately 53K logical reasoning samples for RL training.
Competitive Programming. For the competitive programming problems, we collect publicly availableproblems from online judge platforms and popular coding websites. For problems lacking test cases, we develop an LLM-based workflow and use the MiniMax-Text-01 model to generate comprehensivetest suites. Similar to our approach with mathematical reasoning datasets, we filter problems basedon quality and difficulty using pass rates from model sampling, retaining moderately challenging andhigh-quality algorithmic problems. Through this process, we generate 30K competitive programmingdata samples for RL training.
Software Engineering. For the software engineering domain, inspired by SWE-bench (Jimenezet al., 2024), we construct verifiable reinforcement learning environments by leveraging real-worlddata from public GitHub repositories. Our dataset primarily comprises issues and pull requests (PRs) that encapsulate common software development challenges, including bug localization, code repair, and test case synthesis. To facilitate effective reinforcement learning, we develop a sophisticatedcontainerized sandbox environment that simulates a realistic software development workflow. This MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attentionenvironment enables the actual execution of code, providing direct and verifiable feedback on thecorrectness and efficacy of an agent’s proposed interventions. The pass/fail status of pre-defined ornewly generated test cases serves as the primary reward signal for our RL framework. A successfulexecution that passes all relevant test cases yields a positive reward, while compilation errors, runtimefailures, or test case regressions result in a zero or negative reward, thus providing a clear signal forpolicy optimization. Through this process, we curate several thousand high-quality data samples.
Each sample includes a problem description (e.g., bug report from an issue), the initial faulty code, and a set of associated test cases. This setup allows our RL agent to learn to accurately pinpoint bugs, propose correct code fixes, and even synthesize new, effective test cases, with performance directlyverifiable through the execution within our sandboxed environment.
4.2. General Domain Tasks with Model-based Feedbacks In this section, we further extend the RL scope to a wider array of general domain tasks. As thesetasks cannot be easily verified by rules, we utilize reward models to provide the feedback.
4.2.1. Data and Reward Models Our general RL dataset consists of a total of 25K complex samples. These can be broadly categorizedinto two types: samples with ground-truth answers that are verifiable but difficult to validate usingrules, and samples without ground-truth answers.
Tasks with Ground Truth. This category primarily includes STEM and other factual problems whereanswers are objective but may have multiple valid expressions. Such diversity often renders rulebased answer checkers inaccurate. Our data cleaning process is similar to that used in mathematicalreasoning, while we use our Generative Reward Model (GenRM) as a verifier, instead of relying onrule-based checkers. To evaluate consistency between ground-truth answers and model responses, we adopt a five-grade reward scale to evaluate the two components. First, we construct a humanannotated reward model benchmark, which covers a range of objective tasks across diverse knowledgeand task domains, especially the pairs of model response–ground truth that rule-based checkersfail to judge accurately. Second, we evaluate the GenRM’s effectiveness by comparing the Best-of-N (BoN) responses selected by GenRM against the pass@N metrics across several benchmarks. GenRMperformance is assessed using its accuracy on the human-annotated benchmark and the performancegapbetweenBoNandpass@N.Thesemetricsguideexperimentstooptimizeboththedatadistributionand the prompt design used during the GenRM training.
TaskswithoutGroundTruth. Thiscategoryencompassesawiderrangeoftasks,includinginstructionfollowing, creative writing, etc. Prompts are sampled from a large pool based on our internal taggingsystem, ensuring a balanced training distribution across fine-grained domains. Even though thesequeries are typically open-ended and do not have a ground-truth answer, we seek to pair a referenceanswer for each query, which serves as a reference for reward model judgment. To this end, wefirst generate responses by various internal and external models, and then these reference answerswill undergo our internal quality evaluation. During RL training, we adopt a pairwise comparisonframework to evaluate model responses. Each comparison yields a score of -1, 0, or 1, indicatingwhetherthe model’s outputis worsethan, similarto, orbetter thana referenceanswer. Forinstructionfollowing tasks with constraints particularly, we utilize both the rule-based reward to assess whetherthe response satisfies the constraint, and model-based reward to evaluate response’s quality. As withthe ground-truth setting, we first build a human-annotated benchmark, incorporating multiple blindpreference judgments from reliable annotators. We then refine our scoring criteria and preferenceprompt to optimize accuracy as well as potential biases, which would be mentioned in §4.2.2 below.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention To minimize the potential biases, training data are also optimized by several methods, such asmultiple-blind consistent judgment, position-switched consistent judgment, etc. Once an optimal GenRM is trained, a Swiss Round scoring system is performed across the training dataset to determinethe most suitable reference answer for RL training.
4.2.2. Addressing Bias of Generative Reward Models for Long CoT Effective general RL for complex CoT reasoning tasks is critically dependent on accurate and unbiasedreward models. Assessing such CoT responses turns out to be challenging, and we found that GenRMspreferred longer outputs over potentially superior concise alternatives, irrespective of actual reasoningquality. This length bias is a significant issue as it may substantially misguide RL policy optimization, incentivizing verbosity without substance and inducing reward hacking. Our initial efforts to improve GenRM fidelity include standard offline strategies: (1) Diversifying training data with a wide rangeof response lengths, sources, and quality tiers; (2) Incorporating adversarial examples to exposevulnerabilities; and (3) Refining model architectures. However, empirical analysis revealed that purelyoffline evaluation and preemptive mitigation of length bias in GenRMs frequently failed to preventlength bias during RL training.
Consequently, ourcorestrategyincorporatescontinuousonlinemonitoringoflengthbiasduringRLtraining. Specific metrics are established to detect whether the RL policy disproportionately extendsoutput lengths to maximize GenRMs rewards without gains in task success or reasoning depth. Upondetecting such detrimental length-seeking behavior, indicative of exploiting GenRMs length bias, immediate GenRMs recalibration is triggered. This iterative adjustment is vital to preempt rewardhacking related to output length, ensuring the policy prioritized substantive capability enhancementover superficial text inflation. Complementing this adaptive approach, RL-side techniques includingreward shaping, value clipping, and normalization are systematically employed. These mechanismsdesensitize reward signals to extreme values from superficial characteristics (e.g., length), therebydirecting policy optimization toward substantive quality and correctness of its long CoT reasoning.
4.3. Curriculum of Incorporating Diverse Data Given that our RL data spans a wide spectrum of categories, a core challenge is training a singlepolicy capable of excelling on both reasoning-intensive tasks and general domain tasks. To addressthis, our approach entails a carefully managed curriculum and dynamic weighting strategy forreasoning and general-domain tasks during the RL training process with CISPO: we start with onlythe reasoning-intensive tasks with rule-based reward, and then gradually mix in the general domaintasks. This ensures that the model continues to refine its verifiable skills (e.g., in math and code) while progressively enhancing its performance on a diverse spectrum of general tasks, from complexinstruction following to open-ended CoT reasoning. This mixed RL training encourages the modelto learn context-dependent application of its reasoning abilities—applying rigorous, step-by-stepdeduction for verifiable problems and more flexible, adaptive generation for general queries—allwithin a unified policy framework. It prevents catastrophic forgetting of specialized skills whilefostering broader generalization.
5. Extending RL Scaling to Longer Thinking Our first RL training is performed with an output length limit of 40K tokens. Given that the hybridarchitecture of M1 natively supports near-linear scaling for longer sequences, as demonstrated in Figure 1 (Right), we further extend the generation length during RL training to 80K tokens. Thisresults in a new model, which we refer to as MiniMax-M1-80k.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Data.To efficiently train our RL model for an 80K output length, we utilize our previously trained 40K model to guide the data filtering process. First, we evaluate the pass rates on the curated datasetdescribed in §4 and remove samples that are easily solved. We then adjust the data distribution tofavor more challenging examples, such as difficult mathematical and coding problems. Additionally, we downsample synthetic reasoning data after observing that it destabilizes long-context RL training.
Specifically, outputs generated from this data type often become repetitive and homogenous, andcontinued exposure to these patterns proves detrimental to the model’s overall performance.
Length Scaling Strategy. To gradually increase the output length, we employ a staged windowexpansion RL strategy. We begin with an output length of 40K and incrementally expand it to 48K, 56K, 64K, 72K, and ultimately 80K. This staged approach ensures training stability at each step. Thetransition to a subsequent length is determined by a set of empirical indicators. These include theconvergence of perplexity on the generated sequences and whether the 99th percentile of the outputlengths is approaching the current context window limit. These signals offer valuable insights intothe model’s readiness for scaling, which allows us to maintain robust training throughout the process.
Addressing Training Instability During Scaling. During the scaling process, we encountered acritical issue in the later stages of training at each length window. Specifically, the model exhibitedsusceptibility to pattern collapse, where the latter portions of generated sequences degraded intoincoherent or garbled text. This phenomenon consistently coincided with increased perplexity, indicating compromised generation quality and stability. We identify the root cause: during outputlength extension, negative samples increase in length substantially faster than positive samples, frequently reaching the context window limit earlier. Consequently, disproportionately large negativegradients accumulate in the latter segments of generation sequences. This imbalance originates fromthe inherently unequal nature of GRPO’s advantage normalization and the token-level loss we adopt.
To address this, we implement three key solutions: (1) Detecting repetitive patterns (consecutivehigh-probability tokens) with early stopping to prevent excessive context window consumption byrepetitive responses; (2) Adopting combined sample-level loss and token-level normalization toalleviate negative-positive sample imbalance and mitigate adverse effects; (3) Decreasing both thegradient clipping threshold and 𝜖𝐼𝑆ℎ𝑖𝑔ℎto further stabilize generation.
6. Evaluations 6.1. Core Benchmarks We conduct a comprehensive evaluation of MiniMax-M1 across several key domains: mathematics, generalcoding,softwareengineering,reasoning&knowledge,longcontext,agentictooluse,factuality, and general assistant ability. We evaluate all tasks using temperature 1.0 and top-p 0.95 sampling.
•Mathematics: To evaluate mathematical reasoning capabilities, we utilize several competitionlevel math benchmarks, including MATH-500 (Hendrycks et al., 2021), AIME 2024, AIME 2025.
For AIME evaluation, we sample 32 times and compute the average passrate as the final score.
•General Coding: We assess general programming proficiency using LiveCodeBench (Jain et al., 2025) and FullStackBench (Liu et al., 2024), which evaluate code generation across diverseprogramming tasks. For both benchmarks, we report scores as the average passrate of 16samples.
•Reasoning & Knowledge: We assess domain knowledge and reasoning capabilities through GPQA-Diamond (Rein et al., 2024), MMLU-Pro (Wang et al., 2024), and the challenging HLEbenchmark (Phan et al., 2025). For GPQA-Diamond, we sample 32 times and report the averagepassrate. For HLE evaluation, we assess the model without external tools. Additionally, we MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Table 2|Performance of MiniMax-M1 on core benchmarks.
TasksLeading Close-Weights Models Open-Weights Models Our Models OpenAI-o3Gemini 2.5 Pro (06-05)Claude 4 OpusSeedThinkingv1.5DeepSeekR1DeepSeek-R1-0528Qwen3-235B-A22BMiniMax-M1-40kMiniMax-M1-80k Extended Thinking100K 64K 64K 32K 32K 64K 32K 40K 80K Mathematics AIME 2024 91.6 92.0 76.0 86.7 79.8 91.4 85.7 83.3 86.0 AIME 2025 88.9 88.0 75.5 74.0 70.0 87.5 81.5 74.6 76.9 MATH-500 98.1 98.8 98.2 96.7 97.3 98.0 96.2 96.0 96.8 General Coding LiveCodeBench (24/8∼25/5)75.8 77.1 56.6 67.5 55.9 73.1 65.9 62.3 65.0 FullStackBench 69.3 – 70.3 69.9 70.1 69.4 62.9 67.6 68.3 Reasoning & Knowledge GPQA Diamond 83.3 86.4 79.6 77.3 71.5 81.0 71.1 69.2 70.0 HLE (no tools) 20.3 21.6 10.7 8.2 8.6∗17.7∗7.6∗7.2∗8.4∗ ZebraLogic 95.8 91.6 95.1 84.4 78.7 95.1 80.3 80.1 86.8 MMLU-Pro 85.0 86.0 85.0 87.0 84.0 85.0 83.0 80.6 81.1 Software Engineering SWE-bench Verified 69.1 67.2 72.5 47.0 49.2 57.6 34.4 55.6 56.0 Long Context OpenAI-MRCR (128k) 56.5 76.8 48.9 54.3 35.8 51.5 27.7 76.1 73.4 OpenAI-MRCR (1M) – 58.8 – – – – – 58.6 56.2 LongBench-v2 58.8 65.0 55.6 52.5 58.3 52.1 50.1 61.0 61.5 Agentic Tool Use TAU-bench ( airline) 52.0 50.0 59.6 44.0 – 53.5 34.7 60.0 62.0 TAU-bench (retail) 73.9 67.0 81.4 55.7 – 63.9 58.6 67.8 63.5 Factuality SimpleQA 49.4 54.0 – 12.9 30.1 27.8 11.0 17.9 18.5 General Assistant MultiChallenge 56.5 51.8 45.8 43.0 40.7 45.0 40.0 44.7 44.7 * conducted on the text-only HLE subset.
measure logical reasoning ability using ZebraLogic (Lin et al., 2025).
•Software Engineering: We evaluate software engineering capabilities using SWE-bench Verified (Jimenez et al., 2024), which measures the ability to resolve real-world GitHub issues.
We report results derived from the Agentless scaffold (Xia et al., 2024). Departing from theoriginal pipeline, our methodology employs a two-stage localization process (without anyembedding-based retrieval mechanisms): initial coarse-grained file localization followed byfine-grained localization to specific files and code elements.
•Long Context: We evaluate long context understanding using OpenAI-MRCR (OpenAI, 2024b), which tests retrieval and disambiguation of multiple similar items within extended contexts, and LongBench-v2 (Bai et al., 2024), a challenging benchmark with 503 multiple-choice questions MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attentionacross contexts ranging from 8k to 2M words.
•Agentic Tool Use: We assess tool use capabilities through TAU-bench (Yao et al., 2025), whichemulates dynamic conversations where agents must utilize API tools while adhering to domainspecific policy guidelines. We evaluate TAU-bench with GPT-4.1 as user model, a general systemprompt2and without any custom tools. The maximum number of interaction steps is 40.
•Factuality: TomeasurefactualityofLLMs,weutilizeSimpleQA(Weietal.,2024),anadversariallycollected benchmark of fact-seeking questions with single, indisputable answers.
•General Assistant: We evaluate general assistant capabilities using MultiChallenge (Sirdeshmukh et al., 2025), which assesses LLMs on conducting realistic multi-turn conversations withhuman users. We report our scores judged by GPT-4o.
Results on Math, Coding, and other General Tasks. Table 2 presents our model’s performancecompared to state-of-the-art large reasoning models. In mathematical reasoning, the MiniMax-M1models demonstrate strong performance across multiple benchmarks, achieving results comparable to the close-weight model Seed-Thinking-v1.5 (Seed et al., 2025). Notably, MiniMax-M1-80kachieves 86.0% on AIME 2024, placing it second among open-weight models and trailing only thelatest DeepSeek-R1-0528 model. For general coding, MiniMax-M1-80k matches Qwen3-235B on LiveCodeBench while outperforming it on FullStackBench, demonstrating robust capabilities amongleading open-weight models. On reasoning & knowledge benchmarks, MiniMax-M1-80k similarlytrails DeepSeek-R1-0528 but achieves competitive performance against other top open-weight models.
On the factuality benchmark SimpleQA, Minimax-M1 models underperform DeepSeek-R1 whileoutperforming all other open-weight models and Seed-Thinking-v1.5. On MultiChallenge, both MiniMax models perform comparably to DeepSeek-R1-0528 and Claude 4 Optus, with inferior resultsonly to o3 and Gemini-2.5-Pro.
Highlights in Complex Scenarios: Software Engineering, Long Context, and Tool use. Benefitingfrom our execution-based, software engineering environments during RL, MiniMax-M1-40k and MiniMax-M1-80k achieve strong scores of 55.6% and 56.0% on SWE-bench verified respectively.
These results are slightly inferior to DeepSeek-R1-0528’s 57.6% and significantly surpass other openweights models. Leveraging its 1M context window, the M1 models significantly outperform all otheropen-weight models in long-context understanding. They even surpass OpenAI o3 and Claude 4 Opus, ranking second globally and trailing only Gemini 2.5 Pro by a small margin. In agentic tool-usescenarios (TAU-bench), MiniMax-M1-40k surpasses all open-weight models and even Gemini-2.5 Pro. Moreover, MiniMax-M1-80k consistently outperforms MiniMax-M1-40k across most benchmarks, confirming the benefits of scaling test-time compute.
6.2. Effect of RL Scaling To investigate the effect of RL scaling, we track performance and response length throughout training.
Figure 4 presents three representative examples from AIME 2024, AIME 2025, and LiveCodeBenchv5, respectively. We observe consistent improvements in both model performance and response lengthduring training. Notably, average response lengths on AIME and LiveCodeBench exceed 20,000tokens, with AIME 2024 accuracy showing substantial gains from 68% to 80%. Crucially, the strongcorrelation between accuracy gains and increased response length in these visualizations underscoresthe importance of extending RL scaling to facilitate more extensive reasoning processes.
2"In each round, you need to carefully examine the tools provided to you to determine if any can be used. You mustadhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found withinthese policies." MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention 0 1000 2000 3000 4000 Training Steps687072757880828588Accuracy (%) AIME2024 Performance T oken Length 120001400016000180002000022000 Token Length 0 1000 2000 3000 4000 Training Steps50556065707580Accuracy (%) AIME2025 Performance T oken Length 1400016000180002000022000240002600028000 Token Length 0 1000 2000 3000 4000 Training Steps5558606265687072Accuracy (%) LiveCodeBench v5 Performance T oken Length 1400016000180002000022000 Token Length Figure 4|Accuracy and generation length versus RL training steps for MiniMax-M1.
7. Conclusion and Future work In this work, we introduce and release MiniMax-M1, the world’s first open-weight, large-scalereasoning model featuring a lightning attention mechanism. This efficient attention design enables MiniMax-M1tonativelysupportinputsofupto1Mtokensandgenerationlengthsof80Ktokens—bothsignificantly exceeding capabilities of other open-weight models. These capabilities render MiniMaxM1 uniquely suited for complex, realistic scenarios requiring long context and extended reasoning, properties empirically validated by its strong performance on software engineering, agentic tool use, and long-context understanding benchmarks. Beyond the inherent efficiency advantages of lightningattention for RL training, this work contributes a novel RL algorithm, CISPO, to accelerate training.
Combining architectural advantages with CISPO, we efficiently trained MiniMax-M1, with complete RL training completed in three weeks using 512 H800 GPUs. Across comprehensive evaluations, MiniMax-M1 ranks among the world’s best open-weight models alongside DeepSeek-R1 and Qwen3235B.
Looking forward, as test-time compute continuously scales to power increasingly complex scenarios, we foresee significant potential for such efficient architectures in addressing real-worldchallenges. These include automating company workflows (Xu et al., 2025) and conducting scientificresearch (OpenAI, 2025; Si et al., 2024). Real-world applications particularly demand LRMs thatfunction as agents interacting with environments, tools, computers, or other agents—requiring reasoning across dozens to hundreds of turns while integrating long-context information from diversesources. We envision MiniMax-M1 serving as a strong foundation for such applications with uniqueadvantages, and we are fully dedicated to further evolving MiniMax-M1 toward this goal.
References Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/ claude-3-7-sonnet , 2025. Blog post, February 24, 2025.
Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Ré. Simple linear attention language models balance the recall-throughputtradeoff. arXiv preprint arXiv:2402.18668 , 2024.
Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench. arXiv preprint arXiv:2412.15204 , 2024.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXivpreprint arXiv:2501.00663 , 2024.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXivpreprint arXiv:2004.05150 , 2020.
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with Performers. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id= Ua6zuk0WRH .
Yuhong Chou, Man Yao, Kexin Wang, Yuqi Pan, Rui-Jie Zhu, Jibin Wu, Yiran Zhong, Yu Qiao, Bo Xu, and Guoqi Li. Metala: Unified optimal linear approximation to softmax attention map. Advances in Neural Information Processing Systems , 37:71034–71067, 2024.
Junyoung Chung and Ç. Empirical evaluation of gated recurrent neural networks on sequencemodeling. arXiv preprint arXiv:1412.3555 , 2014.
Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning languagemodels. arXiv preprint arXiv:2505.22617 , 2025.
Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms throughstructured state space duality. arXiv preprint arXiv:2405.21060 , 2024.
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. Deepseek-r1: Incentivizing reasoning capability in llms viareinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.
Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling withmixture-of-memories. arXiv preprint arXiv:2502.13685 , 2025.
Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: A compact 7b SSM. arXiv preprint arXiv:2405.16712 , 2024.
Google DeepMind. Gemini pro. https://deepmind.google/models/gemini/pro/ , 2025. Webpage, accessed 2025.
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling , 2024. URL https://openreview.net/forum?id= tEYskw1VY2 .
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory withoptimal polynomial projections. Advances in neural information processing systems , 33:1474–1487, 2020.
Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structuredstate spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL https://openreview.net/forum?id= uYLFoz1vlAC .
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO:
State space models with generalized orthogonal basis projections. In International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=klK17OQ3KB .
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structuredstate spaces. In NeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html .
Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, and Weiyao Lin. Rodimus*: Breaking theaccuracy-efficiency trade-off with efficient attentions. arXiv preprint arXiv:2410.06577 , 2024.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXivpreprint arXiv:2103.03874 , 2021.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):
1735–1780, 1997.
Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model.
arXiv preprint arXiv:2503.24290 , 2025.
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination freeevaluation of large language models for code. In The Thirteenth International Conference on Learning Representations , 2025.
Jamba Team. Jamba-1.5: Hybrid T. arXiv preprint arXiv:2408.12570 , 2024.
Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id= VTF8yNQM66 .
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:
Fast autoregressive transformers with linear attention. In International Conference on Machine Learning , pages 5156–5165. PMLR, 2020.
Kimi Team. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 , 2025.
Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, and Junxian He.
Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond.
arXiv preprint arXiv:2505.19641 , 2025a.
Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Ge Zhang, Wenhao Huang, Kai Shen, and Liang Xiang.
Fullstack bench: Evaluating llms as full stack coders. arXiv preprint arXiv:2412.00535 , 2024.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783 , 2025b.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conferenceon Learning Representations , 2019.
Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXivpreprint arXiv:2502.13189 , 2025.
Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018. URL https://openreview.
net/forum?id=HyUNwulC- .
MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightningattention. arXiv preprint arXiv:2501.08313 , 2025.
Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, Binh Tang, Diana Liskovich, Puxin Xu, Yuchen Zhang, Melanie Kambadur, Stephen Roller, and Susan Zhang. A theory on adam instability inlarge-scale machine learning. arXiv preprint arXiv:2304.09871 , 2023.
OpenAI. Introducing openai o1. https://openai.com/o1/ , 2024a. Web page, accessed 2024.
OpenAI. Openai mrcr dataset. https://huggingface.co/datasets/openai/mrcr , 2024b.
Accessed: 2025-06-15.
OpenAI. Introducing deep research, 2025. URL https://openai.com/index/ introducing-deep-research/ .
Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnnsfor the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2023.
Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, and Przemysł Kazienko. Eagle and finch: Rwkv withmatrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892 , 2024a.
Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, and Przemysł Kazienko. Eagle and finch: Rwkv withmatrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892 , 2024b.
BoPeng, Ruichong Zhang, DanielGoldstein, EricAlcaide, XingjianDu, HaowenHou, JiajuLin, Jiaxing Liu, Janna Lu, William Merrill, et al. Rwkv-7. arXiv preprint arXiv:2503.14456 , 2025.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Randomfeature attention. In International Conference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=QtTKTdVrFBB .
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity’s last exam. arXiv preprint Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In Proceedings of the International Conference on Learning Representations (ICLR) , 2021.
Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong.
The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 7025–7041, 2022a.
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosFormer: Rethinking softmax in attention. In International Conference on Learning Representations , 2022b. URL https://openreview.net/forum?id=Bl8CQrx2Up4 .
Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequencemodeling. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 33202–33221, 2023.
Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, and Yiran Zhong. You only scanonce: Efficient multi-dimension sequential modeling with lightnet. arXiv preprint arXiv:2405.21022 , 2024a.
Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2:
A free lunch for handling unlimited sequence lengths in large language models. arXiv preprint Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constant speed: Efficient language modeling with lightning attention. In International conferenceon machine learning , pages 41517–41535. PMLR, 2024c.
Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. HGRN2.
arXiv preprint arXiv:2404.07904 , 2024d.
Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115 , 2025.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling , 2024.
Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simplehybrid state space models for efficient unlimited context language modeling. arXiv preprint John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policyoptimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning modelswith reinforcement learning. arXiv preprint arXiv:2504.13914 , 2025.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. DeepSeekMath. arXiv preprint arXiv:2402.03300 , 2024.
Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linearcomplexity language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 16377–16426, 2024.
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint ChengleiSi,DiyiYang,andTatsunoriHashimoto. Canllmsgeneratenovelresearchideas? alarge-scalehuman study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109 , 2024.
Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, and Riccardo Grazzi.
Deltaproduct: Improving state-tracking in linear rnns via householder products. arXiv preprint Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: A realistic multi-turnconversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399 , 2025.
Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, and Yu Cheng. Linear-moe: Linear sequence modelingmeets mixture-of-experts. arXiv preprint arXiv:2503.05447 , 2025.
Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hiddenstates. arXiv preprint arXiv:2407.04620 , 2024.
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint Tencent AI Lab. Hunyuan-t1: Reasoning efficiency redefined. https://llm.hunyuan.tencent.
com/#/Blog/hy-t1/ , 2025. Accessed: 2025-06-15.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processingsystems, 30, 2017.
Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, et al. Mesanet:
Sequence modeling by locally optimal test-time training. arXiv preprint arXiv:2506.05233 , 2025.
Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effectivereinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-tasklanguageunderstandingbenchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2024.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neuralinformation processing systems , 35:24824–24837, 2022.
Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXivpreprint arXiv:2411.04368 , 2024.
Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifyingllm-based software engineering agents. arXiv preprint arXiv:2407.01489 , 2024.
Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig.
Theagentcompany: Benchmarking llm agents on consequential real world tasks. arXiv preprint Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attentiontransformers with hardware-efficient training. arXiv preprint arXiv:2312.06635 , 2024a.
Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformerswith the delta rule over sequence length. arXiv preprint arXiv:2406.06484 , 2024b.
Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik R Narasimhan. 𝜏-bench: A benchmark fortool-agent-user interaction in real-world domains. In The Thirteenth International Conference on Learning Representations , 2025.
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llmreinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025.
Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and nativelytrainable sparse attention. arXiv preprint arXiv:2502.11089 , 2025.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers forlonger sequences. Advances in neural information processing systems , 33:17283–17297, 2020.
Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo:
Investigating and taming zero reinforcement learning for open base models in the wild. arXivpreprint arXiv:2503.18892 , 2025.
Yu Zhang, Songlin Yang, Rui-Jie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, et al. Gated slot attention for efficient linear-time sequence modeling.
Advances in Neural Information Processing Systems , 37:116870–116898, 2024.
MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention A. Contributors The contributors to the report are listed in alphabetical order as follows:
Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun
